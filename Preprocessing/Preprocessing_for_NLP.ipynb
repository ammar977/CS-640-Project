{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "npERY2Tqwb0N",
    "outputId": "48cbfb5f-59f7-476d-ad1d-8453ff8f396a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (1.2.4)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages/numpy-1.19.5-py3.8-linux-x86_64.egg (from pandas) (1.19.5)\n",
      "Requirement already satisfied: pytz>=2017.3 in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.1; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/share/pkg.7/python3/3.8.10/install/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages/numpy-1.19.5-py3.8-linux-x86_64.egg (1.19.5)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.1; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/share/pkg.7/python3/3.8.10/install/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sklearn in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from sklearn) (0.24.2)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages/scipy-1.6.2-py3.8-linux-x86_64.egg (from scikit-learn->sklearn) (1.6.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from scikit-learn->sklearn) (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages/numpy-1.19.5-py3.8-linux-x86_64.egg (from scikit-learn->sklearn) (1.19.5)\n",
      "Requirement already satisfied: joblib>=0.11 in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.0.1)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.1; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/share/pkg.7/python3/3.8.10/install/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /usr4/cs640g/jenajj/.local/lib/python3.8/site-packages (4.13.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from transformers) (4.60.0)\n",
      "Requirement already satisfied: sacremoses in /usr4/cs640g/jenajj/.local/lib/python3.8/site-packages (from transformers) (0.0.46)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr4/cs640g/jenajj/.local/lib/python3.8/site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages/numpy-1.19.5-py3.8-linux-x86_64.egg (from transformers) (1.19.5)\n",
      "Requirement already satisfied: filelock in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr4/cs640g/jenajj/.local/lib/python3.8/site-packages (from transformers) (0.2.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from transformers) (2021.4.4)\n",
      "Requirement already satisfied: requests in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from requests->transformers) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr4/cs640g/jenajj/.local/lib/python3.8/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: click in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: six in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from sacremoses->transformers) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.1; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/share/pkg.7/python3/3.8.10/install/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: demoji in /usr4/cs640g/jenajj/.local/lib/python3.8/site-packages (1.1.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.1; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/share/pkg.7/python3/3.8.10/install/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: googletrans==3.1.0a0 in /usr4/cs640g/jenajj/.local/lib/python3.8/site-packages (3.1.0a0)\n",
      "Requirement already satisfied: httpx==0.13.3 in /usr4/cs640g/jenajj/.local/lib/python3.8/site-packages (from googletrans==3.1.0a0) (0.13.3)\n",
      "Requirement already satisfied: httpcore==0.9.* in /usr4/cs640g/jenajj/.local/lib/python3.8/site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (0.9.1)\n",
      "Requirement already satisfied: certifi in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2020.12.5)\n",
      "Requirement already satisfied: chardet==3.* in /usr4/cs640g/jenajj/.local/lib/python3.8/site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (3.0.4)\n",
      "Requirement already satisfied: idna==2.* in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2.10)\n",
      "Requirement already satisfied: sniffio in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.2.0)\n",
      "Requirement already satisfied: rfc3986<2,>=1.3 in /usr4/cs640g/jenajj/.local/lib/python3.8/site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.5.0)\n",
      "Requirement already satisfied: hstspreload in /usr4/cs640g/jenajj/.local/lib/python3.8/site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2021.12.1)\n",
      "Requirement already satisfied: h11<0.10,>=0.8 in /usr4/cs640g/jenajj/.local/lib/python3.8/site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (0.9.0)\n",
      "Requirement already satisfied: h2==3.* in /usr4/cs640g/jenajj/.local/lib/python3.8/site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (3.2.0)\n",
      "Requirement already satisfied: hpack<4,>=3.0 in /usr4/cs640g/jenajj/.local/lib/python3.8/site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (3.0.0)\n",
      "Requirement already satisfied: hyperframe<6,>=5.2.0 in /usr4/cs640g/jenajj/.local/lib/python3.8/site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (5.2.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.1; however, version 21.3.1 is available.\r\n",
      "You should consider upgrading via the '/share/pkg.7/python3/3.8.10/install/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install sklearn\n",
    "!pip install transformers\n",
    "!pip install demoji\n",
    "!pip install googletrans==3.1.0a0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.4\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# BERT imports\n",
    "import torch\n",
    "# from urllib3.util.ssl_ import ssl\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "import pandas as pd\n",
    "print(pd.__version__)\n",
    "import io\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "from googletrans import Translator\n",
    "import demoji\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hus9RCdSgr-j"
   },
   "source": [
    "## Define Preprocess Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " black small square \n",
      "▪ The sky is blue and I like bananas\n",
      "Hello.▪\n"
     ]
    }
   ],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join( c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "def trans_text(df, col):\n",
    "    translator = Translator()\n",
    "    translations = {}\n",
    "    # translations[t] = translator.translate(t,dest='en').text\n",
    "    cnt = 0\n",
    "    df_en = df.copy()\n",
    "    translations = {}\n",
    "    # unique elements of the column\n",
    "    unique_elements = df_en[col].unique()\n",
    "    for element in unique_elements:\n",
    "        # add translation to the dictionary\n",
    "        detected_eq = translator.detect(element)\n",
    "        if detected_eq.lang!='en' and detected_eq.confidence> 0.5:\n",
    "            print(element)\n",
    "            translations[element] = translator.translate(element).text\n",
    "            cnt+=1\n",
    "    return translations, int(cnt)\n",
    "\n",
    "print(demoji.replace_with_desc('▪', \" \"))\n",
    "translator = Translator()\n",
    "try:\n",
    "    trans = translator.translate('안녕하세요.▪',dest='en')\n",
    "    translation = translator.translate(\"▪Der Himmel ist blau und ich mag Bananen\", dest='en')\n",
    "except BaseException as err:\n",
    "    print('Error returned')\n",
    "    print(f\"Unexpected {err} =, {type(err)}\")\n",
    "print(translation.text)\n",
    "print(trans.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "eFB066u2lnsg"
   },
   "outputs": [],
   "source": [
    "\n",
    "regexMap={r\"<[\\w'/'\\s]*>\": \"\",r\"[\\'\\\"\\-]+\": \"\",r\"@[\\w]+\":\"\",r\"#[\\w]+\":\"\"}\n",
    "\n",
    "def preprocess(datainput):\n",
    "    t=datainput\n",
    "    t = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', t, flags=re.MULTILINE)\n",
    "    for regx in regexMap.keys():\n",
    "        t = re.sub(regx, regexMap[regx], t)\n",
    "    return t\n",
    "\n",
    "\n",
    "def remove_emoji(datainput):\n",
    "    t=datainput\n",
    "    t = demoji.replace_with_desc(t, \" \")\n",
    "    return t\n",
    "\n",
    "\n",
    "def lower_strip(datainput):\n",
    "    t=datainput\n",
    "    t = unicodeToAscii(t.strip())\n",
    "    # Lowercase and trim\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KFGvfxwafgVp"
   },
   "source": [
    "## Load Twitter Dataset Files:  Training & Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "dxi-TOsRwb0T"
   },
   "outputs": [],
   "source": [
    "# Access the training & test data: \n",
    "twitter_train=pd.read_csv('./training.1600000.processed.noemoticon.csv',encoding='latin1',header=None)\n",
    "twitter_test=pd.read_csv('./testdata.manual.2009.06.14.csv',encoding='latin1',header=None)\n",
    "\n",
    "# Prep the training & test data: \n",
    "# Select the subset of data fields & name the columns\n",
    "twitter_train=twitter_train[[5,0]]\n",
    "twitter_train.columns=['tweet','sentiment', ]\n",
    "twitter_test=twitter_test[[5,0]]\n",
    "twitter_test.columns=['tweet','sentiment',]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downselect the Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "TkQgroOf3LKf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600000\n",
      "359\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The Red Shore and ent will make this Tuesday a little more enjoyable...but that's after class  Hope I make it...\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Down SELECT THE DATA - we have neutral sentiment data but not enough for a viable model\n",
    "\n",
    "# #Switch out half of the no-emotion dataset in the test file for other data in the train file\n",
    "# a = twitter_train[(twitter_train['sentiment']==0)][0:34]\n",
    "# b = twitter_train[(twitter_train['sentiment']==4)][60:95]\n",
    "# c = twitter_test[(twitter_test['sentiment']==2)][0:34]\n",
    "# d = twitter_test[(twitter_test['sentiment']==2)][60:95]\n",
    "# ai = twitter_train.index[(twitter_train['sentiment']==0)][0:34]\n",
    "# bi = twitter_train.index[(twitter_train['sentiment']==4)][60:95]\n",
    "# ci = twitter_test.index[(twitter_test['sentiment']==2)][0:34]\n",
    "# di = twitter_test.index[(twitter_test['sentiment']==2)][60:95]\n",
    "# twitter_train = twitter_train.drop(ai)\n",
    "# twitter_train = twitter_train.drop(bi)\n",
    "# twitter_train = twitter_train.append(c)\n",
    "# twitter_train = twitter_train.append(d)\n",
    "# twitter_test = twitter_test.drop(ci)\n",
    "# twitter_test = twitter_test.drop(di)\n",
    "# twitter_test = twitter_test.append(a)\n",
    "# twitter_test = twitter_test.append(b)\n",
    "\n",
    "# Try dropping the neutral sentiments from test per instructor's notes\n",
    "ai = twitter_test.index[(twitter_test['sentiment']==2)]\n",
    "twitter_test = twitter_test.drop(ai)\n",
    "\n",
    "datasets=[twitter_train,twitter_test]\n",
    "for i in datasets:\n",
    "  print(len(i))\n",
    "\n",
    "twitter_train['tweet'][6800]\n",
    "# print(twitter_test['tweet'][200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "rRjLrhFvebhT",
    "outputId": "e643170b-b7c3-4ab5-e4e2-fdeca0c79a97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet  sentiment\n",
      "0     Awww, thats a bummer.  You shoulda got Davi...          0\n",
      "1  is upset that he cant update his Facebook by t...          0\n",
      "2   I dived many times for the ball. Managed to s...          0\n",
      "3    my whole body feels itchy and like its on fire           0\n",
      "4   no, its not behaving at all. im mad. why am i...          0\n",
      "                                               tweet  sentiment\n",
      "0   I loooooooovvvvvveee my Kindle2. Not that the...          4\n",
      "1  Reading my kindle2...  Love it... Lee childs i...          4\n",
      "2  Ok, first assesment of the  ...it fucking rock...          4\n",
      "3   Youll love your Kindle2. Ive had mine for a f...          4\n",
      "4    Fair enough. But i have the Kindle2 and I th...          4\n"
     ]
    }
   ],
   "source": [
    "# Prep the training & test data: \n",
    "# remove usernames and URL links\n",
    "datasets=[twitter_train,twitter_test]\n",
    "for i in datasets:\n",
    "    i['tweet']=i['tweet'].apply(preprocess)\n",
    "    print(i.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets=[twitter_train,twitter_test]\n",
    "for i in datasets:\n",
    "    i['tweet']=i['tweet'].apply(remove_emoji)\n",
    "    i['tweet']=i['tweet'].apply(lower_strip)\n",
    "#     i.to_csv('./lower_'+i+'.csv', header=None, index=False,encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Red Shore and ent will make this Tuesday a little more enjoyable...but thats after class  Hope I make it...'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_train['tweet'][6800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_train.to_csv('./cased_train.csv', header=None, index=False,encoding=\"utf-8\")\n",
    "twitter_test.to_csv('./cased_test.csv', header=None, index=False,encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_train=pd.read_csv('./cased_train.csv',encoding=\"utf-8\",header=None)\n",
    "twitter_test=pd.read_csv('./cased_test.csv',encoding=\"utf-8\",header=None)\n",
    "twitter_train=twitter_train[[0,1]]\n",
    "twitter_train.columns=['tweet','sentiment', ]\n",
    "twitter_test=twitter_test[[0,1]]\n",
    "twitter_test.columns=['tweet','sentiment',]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Translate takes too LONG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google translate the data\n",
    "\n",
    "# final_non_english_total=0\n",
    "# processed_rows=0\n",
    "# batch_size = 80\n",
    "# for i in range(0,len(twitter_train),batch_size):\n",
    "# # for i in range(0,batch_size,batch_size):\n",
    "#   translations, cnt = trans_text(twitter_train.iloc[i:i+batch_size,[0]],'tweet')\n",
    "#   if cnt >0:\n",
    "#       # modify all the terms of the data frame by using the translations dictionary\n",
    "#       twitter_train.replace(translations, inplace = True)\n",
    "\n",
    "#   # note the outcome\n",
    "#   final_non_english_total+=cnt\n",
    "#   processed_rows+=batch_size\n",
    "#   if final_non_english_total % 10000 == 0:\n",
    "#     print(f' processed so far {processed_rows}')\n",
    "\n",
    "# # twitter_train['tweet'] = final_dataframe['tweet']\n",
    "# print(f' total changes: {final_non_english_total}')\n",
    "# print(f' processed : {processed_rows}')\n",
    "# twitter_train.to_csv('/content/drive/My Drive/Colab Notebooks/trainingandtestdata/translated_train.csv', header=None, index=False,encoding='latin1')\n",
    "\n",
    "\n",
    "# old buggy version\n",
    "# xx=twitter_test[0:len(twitter_test)].copy()\n",
    "# xx['tweet']=twitter_test['tweet'].apply(translate_text)\n",
    "# twitter_train.replace(xx, inplace = True)\n",
    "# print(twitter_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "zza5hueEwb0Y",
    "outputId": "76bef36e-b9fe-4e17-e942-17c1fe2f7964"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'woke up and was having an accident  &quot;Its pushing, its pushing!&quot; he was crying because he couldnt stop from wetting his pants.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_train['tweet'][400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "tgEqQmTuwb0Z",
    "outputId": "ab586730-0c43-46c9-93fa-61197215dfff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting ready to test out some burger receipes this weekend. Bobby Flay has some great receipes to try. Thanks Bobby.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(twitter_test['tweet'][350])\n",
    "len(twitter_test[(twitter_test['sentiment']==2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "history_visible": true,
   "name": "Sentiment_Analysis_HW5.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "586ad1ed5c97141e2437e681efbf1ec0adcd17d830cf5af2ca3d2819e743e158"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
