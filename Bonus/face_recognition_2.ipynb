{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edf2dabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d4408a",
   "metadata": {},
   "source": [
    "# Load data files\n",
    "\n",
    "Assumes you have unzipped the images.zip file into the folder \"Data Instagram\" which also contains the Excel sheet with the data labels.\n",
    "\n",
    "## Get data labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4adbbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ig_post_data = pd.read_excel(\"../Data Instagram/Labeled_instagram_posts_related_to_covid.xlsx\",\n",
    "                             usecols=\"A, N\", true_values=[1], false_values=[2, 3, 99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d734b3",
   "metadata": {},
   "source": [
    "## Get Image Sets\n",
    "\n",
    "### Load Instagram image data\n",
    "\n",
    "Find each corresponding image for each label we just loaded. Because the labels don't correspond to the image file names, first we strip off the leading information for all of the images' file names, then search directly for the remainder. This should be equal to imagename.jpg. You should only have to run rename_files() once. This way the images can be found quickly. The image resizing is done inside of get_labeled_images()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c552c9a7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def rename_files():\n",
    "    # Rename all image files in \"Data Instagram\", removing the leading integer and underscore\n",
    "    for filename in os.listdir(\"../Data Instagram\"):\n",
    "        if filename.endswith(\".jpg\"):\n",
    "            new_name = re.sub(r\"\\d*_\", \"\", filename)\n",
    "            \n",
    "            if not os.path.exists(\"../Data Instagram/\" + new_name):\n",
    "                os.rename(\"../Data Instagram/\" + filename, \"../Data Instagram/\" + new_name)\n",
    "\n",
    "rename_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "986d9062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labeled_images(image_data):\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    # Get image file name and label\n",
    "    for post in image_data.itertuples():\n",
    "        _, imagename, label = post\n",
    "        \n",
    "        # Change label to binary\n",
    "        if label != 1:\n",
    "            label = 0\n",
    "        \n",
    "        # Load and resize image\n",
    "        file_name = imagename + \".jpg\"\n",
    "        image_file_path = \"../Data Instagram/\" + file_name\n",
    "        if os.path.exists(image_file_path):\n",
    "            picture = tf.io.read_file(image_file_path)\n",
    "            picture = tf.image.decode_jpeg(picture, channels=3)\n",
    "            picture = tf.image.resize_with_pad(picture, 480, 480)\n",
    "            images.append(picture)\n",
    "            labels.append(label)\n",
    "    \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "350f95c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ig_images, ig_labels = get_labeled_images(ig_post_data)  # Will store the images we learn with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3116120e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "\n",
    "def move_labeled_images(image_data):\n",
    "    if not os.path.exists(\"../Data Instagram/Matching/\"):\n",
    "        os.mkdir(\"../Data Instagram/Matching\")\n",
    "    \n",
    "    # Get image file name and label\n",
    "    for post in image_data.itertuples():\n",
    "        _, imagename, label = post\n",
    "        \n",
    "        # Change label to binary\n",
    "        if label != 1:\n",
    "            label = 0\n",
    "        \n",
    "        # Load and resize image\n",
    "        file_name = imagename + \".jpg\"\n",
    "        image_file_path = \"../Data Instagram/\" + file_name\n",
    "        if os.path.exists(image_file_path):\n",
    "            picture = imageio.imread(image_file_path)\n",
    "            out_file_path = \"../Data Instagram/Matching/\" + file_name\n",
    "            imageio.imwrite(out_file_path, picture)\n",
    "            \n",
    "move_labeled_images(ig_post_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef84dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sum_l = 0\n",
    "for label in ig_labels:\n",
    "    sum_l = sum_l + label\n",
    "print(sum_l)\n",
    "print(len(ig_labels))\n",
    "print(ig_labels[5:5])\n",
    "test_list = ig_labels[5:5] + ig_labels[:3]\n",
    "print(test_list)\n",
    "\n",
    "#print(type(ig_images[0]))\n",
    "#i = 0\n",
    "#j = 0\n",
    "#while i < 9:\n",
    "#    lbl = ig_labels[j]\n",
    "#    if lbl == 1:\n",
    "#        ax = plt.subplot(3, 3, i + 1)\n",
    "#        plt.imshow(ig_images[j])\n",
    "#        print(ig_images[j].shape)\n",
    "#        plt.title(int(lbl))\n",
    "#        plt.axis(\"off\")\n",
    "#        i += 1\n",
    "#    j += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8084d203",
   "metadata": {},
   "source": [
    "### Split data\n",
    "\n",
    "Split the data into training, validation, and test data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69855b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_split = int(len(ig_images) / 10) # 10% for validation, 10% for test\n",
    "train_images = ig_images[:-validation_split]\n",
    "train_labels = ig_labels[:-validation_split]\n",
    "pos_cls_wgt = (len(train_labels) - sum(train_labels)) / sum(train_labels)\n",
    "neg_cls_wgt = (len(train_labels) - sum(train_labels)) / len(train_labels)\n",
    "cls_wgts_dic = {0: neg_cls_wgt, 1: pos_cls_wgt}\n",
    "\n",
    "# Split data\n",
    "training_ds = tf.data.Dataset.from_tensor_slices((train_images[:-validation_split],\n",
    "                                                 train_labels[:-validation_split]))\n",
    "validation_ds = tf.data.Dataset.from_tensor_slices((train_images[-validation_split:],\n",
    "                                                   train_labels[-validation_split:]))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((ig_images[-validation_split:],\n",
    "                                             ig_labels[-validation_split:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356c2498",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pos_cls_wgt, neg_cls_wgt)\n",
    "print(training_ds.element_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2211cf22",
   "metadata": {},
   "source": [
    "### Apply ResNet preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ed5edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_ds = training_ds.map(lambda d, l:\n",
    "                              (tf.keras.applications.resnet_v2.preprocess_input(d), l))\n",
    "validation_ds = validation_ds.map(lambda d, l:\n",
    "                                 (tf.keras.applications.resnet_v2.preprocess_input(d), l))\n",
    "test_ds = test_ds.map(lambda d, l:\n",
    "                      (tf.keras.applications.resnet_v2.preprocess_input(d), l))\n",
    "\n",
    "batch_size = 32\n",
    "training_ds = training_ds.cache().batch(batch_size).prefetch(buffer_size=10)\n",
    "validation_ds = validation_ds.cache().batch(batch_size).prefetch(buffer_size=10)\n",
    "test_ds = test_ds.cache().batch(batch_size).prefetch(buffer_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e3a914",
   "metadata": {},
   "source": [
    "# Training on Instagram Images (Proof of Concept)\n",
    "\n",
    "## Instantiate pre-trained ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a848af",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = keras.applications.ResNet50V2(include_top=False,\n",
    "                                     weights=\"imagenet\",\n",
    "                                     input_shape=(480, 480, 3))\n",
    "base.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283825dc",
   "metadata": {},
   "source": [
    "## Create classification layer for 'East Asia' classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da554d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(480, 480, 3))\n",
    "\n",
    "a = base(inputs, training=False)\n",
    "a = keras.layers.GlobalAveragePooling2D()(a)\n",
    "\n",
    "outputs = keras.layers.Dense(1, activation=\"sigmoid\")(a)\n",
    "\n",
    "classifier = keras.Model(inputs, outputs)\n",
    "classifier.compile(optimizer=keras.optimizers.Adam(),\n",
    "              loss=keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "              metrics=[keras.metrics.BinaryAccuracy(),\n",
    "                       keras.metrics.Recall(),\n",
    "                       keras.metrics.Precision(),\n",
    "                       keras.metrics.FalseNegatives(),\n",
    "                       keras.metrics.FalsePositives()])\n",
    "\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54667da",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403b3590",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(training_ds, epochs=1, class_weight=cls_wgts_dic, validation_data=validation_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92980c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c14c95",
   "metadata": {},
   "source": [
    "# 5-Fold Cross-Validation on Instagram Images\n",
    "\n",
    "## Set up\n",
    "\n",
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8f6ce36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    # Instantiate pre-trained ResNet\n",
    "    base = keras.applications.ResNet50V2(include_top=False,\n",
    "                                     weights=\"imagenet\",\n",
    "                                     input_shape=(480, 480, 3))\n",
    "    base.trainable = False\n",
    "    \n",
    "    # Create classifier\n",
    "    inputs = keras.Input(shape=(480, 480, 3))\n",
    "    a = base(inputs, training=False)\n",
    "    a = keras.layers.GlobalAveragePooling2D()(a)\n",
    "    outputs = keras.layers.Dense(1, activation=\"sigmoid\")(a)\n",
    "    classifier = keras.Model(inputs, outputs)\n",
    "    \n",
    "    # Compile model\n",
    "    classifier.compile(optimizer=keras.optimizers.Adam(),\n",
    "                       loss=keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "                       metrics=[keras.metrics.BinaryAccuracy(),\n",
    "                                keras.metrics.Recall(),\n",
    "                                keras.metrics.Precision()])\n",
    "    \n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4122cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(cv_round):\n",
    "    vs = int(len(ig_images) / 5) # 20% for validation, test set already withheld\n",
    "    valid_start = (cv_round) * vs\n",
    "    valid_end = (cv_round+1) * vs if cv_round != 4 else (len(ig_images)-1)\n",
    "\n",
    "    # Split data\n",
    "    training_ds = tf.data.Dataset.from_tensor_slices((ig_images[:valid_start] + ig_images[valid_end:],\n",
    "                                                      ig_labels[:valid_start] + ig_labels[valid_end:]))\n",
    "    validation_ds = tf.data.Dataset.from_tensor_slices((ig_images[valid_start:valid_end],\n",
    "                                                        ig_labels[valid_start:valid_end]))\n",
    "    \n",
    "    return training_ds, validation_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccd627a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(tds, vds):\n",
    "    # ResNet preprocessing\n",
    "    training_ds = tds.map(lambda d, l:\n",
    "                                  (tf.keras.applications.resnet_v2.preprocess_input(d), l))\n",
    "    validation_ds = vds.map(lambda d, l:\n",
    "                                      (tf.keras.applications.resnet_v2.preprocess_input(d), l))\n",
    "    \n",
    "    # Batch setup\n",
    "    batch_size = 32\n",
    "    training_ds = training_ds.cache().batch(batch_size).prefetch(buffer_size=10)\n",
    "    validation_ds = validation_ds.cache().batch(batch_size).prefetch(buffer_size=10)\n",
    "    \n",
    "    return training_ds, validation_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3baa9a",
   "metadata": {},
   "source": [
    "### One-time set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39d31934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Withhold test set\n",
    "#test_split = int(len(ig_images) / 10) # take 10%\n",
    "#test_ds = tf.data.Dataset.from_tensor_slices((ig_images[-validation_split:],\n",
    "#                                             ig_labels[-validation_split:]))\n",
    "#ig_images = ig_images[:-validation_split]\n",
    "#ig_labels = ig_labels[:-validation_split]\n",
    "\n",
    "# Prepare test set\n",
    "#test_ds = test_ds.map(lambda d, l:\n",
    "#                      (tf.keras.applications.resnet_v2.preprocess_input(d), l))\n",
    "#test_ds = test_ds.cache().batch(32).prefetch(buffer_size=10)\n",
    "\n",
    "# Create weight dictionary to offset imbalanced data\n",
    "pos_cls_wgt = (len(ig_labels) - sum(ig_labels)) / sum(ig_labels)\n",
    "neg_cls_wgt = (len(ig_labels) - sum(ig_labels)) / len(ig_labels)\n",
    "cls_wgts_dic = {0: neg_cls_wgt, 1: pos_cls_wgt}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2690367f",
   "metadata": {},
   "source": [
    "## Cross-Validation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8feaf6c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "82/82 [==============================] - 746s 9s/step - loss: 0.9436 - binary_accuracy: 0.6201 - recall: 0.7526 - precision: 0.2447 - val_loss: 0.5197 - val_binary_accuracy: 0.7612 - val_recall: 0.8876 - val_precision: 0.3527\n",
      "Epoch 2/5\n",
      "82/82 [==============================] - 735s 9s/step - loss: 0.7388 - binary_accuracy: 0.7606 - recall: 0.8151 - precision: 0.3623 - val_loss: 0.4692 - val_binary_accuracy: 0.8012 - val_recall: 0.8652 - val_precision: 0.3969\n",
      "Epoch 3/5\n",
      "82/82 [==============================] - 723s 9s/step - loss: 0.6690 - binary_accuracy: 0.7902 - recall: 0.8490 - precision: 0.4010 - val_loss: 0.4439 - val_binary_accuracy: 0.8197 - val_recall: 0.8652 - val_precision: 0.4231\n",
      "Epoch 4/5\n",
      "82/82 [==============================] - 725s 9s/step - loss: 0.6244 - binary_accuracy: 0.8122 - recall: 0.8802 - precision: 0.4333 - val_loss: 0.4283 - val_binary_accuracy: 0.8213 - val_recall: 0.8539 - val_precision: 0.4246\n",
      "Epoch 5/5\n",
      "82/82 [==============================] - 722s 9s/step - loss: 0.5906 - binary_accuracy: 0.8279 - recall: 0.8880 - precision: 0.4577 - val_loss: 0.4179 - val_binary_accuracy: 0.8243 - val_recall: 0.8427 - val_precision: 0.4286\n",
      "Epoch 1/5\n",
      "82/82 [==============================] - 736s 9s/step - loss: 0.8714 - binary_accuracy: 0.6574 - recall_1: 0.7866 - precision_1: 0.2749 - val_loss: 0.4673 - val_binary_accuracy: 0.7843 - val_recall_1: 0.7857 - val_precision_1: 0.3511\n",
      "Epoch 2/5\n",
      "82/82 [==============================] - 724s 9s/step - loss: 0.6876 - binary_accuracy: 0.7794 - recall_1: 0.8432 - precision_1: 0.3905 - val_loss: 0.4403 - val_binary_accuracy: 0.7997 - val_recall_1: 0.7857 - val_precision_1: 0.3708\n",
      "Epoch 3/5\n",
      "82/82 [==============================] - 723s 9s/step - loss: 0.6265 - binary_accuracy: 0.8099 - recall_1: 0.8586 - precision_1: 0.4321 - val_loss: 0.4323 - val_binary_accuracy: 0.7935 - val_recall_1: 0.7738 - val_precision_1: 0.3611\n",
      "Epoch 4/5\n",
      "82/82 [==============================] - 722s 9s/step - loss: 0.5871 - binary_accuracy: 0.8229 - recall_1: 0.8792 - precision_1: 0.4530 - val_loss: 0.4293 - val_binary_accuracy: 0.7982 - val_recall_1: 0.7738 - val_precision_1: 0.3672\n",
      "Epoch 5/5\n",
      "82/82 [==============================] - 729s 9s/step - loss: 0.5574 - binary_accuracy: 0.8353 - recall_1: 0.9049 - precision_1: 0.4738 - val_loss: 0.4285 - val_binary_accuracy: 0.7997 - val_recall_1: 0.7738 - val_precision_1: 0.3693\n",
      "Epoch 1/5\n",
      "82/82 [==============================] - 762s 9s/step - loss: 0.8768 - binary_accuracy: 0.6559 - recall_2: 0.7763 - precision_2: 0.2672 - val_loss: 0.4920 - val_binary_accuracy: 0.7750 - val_recall_2: 0.7634 - val_precision_2: 0.3641\n",
      "Epoch 2/5\n",
      "82/82 [==============================] - 950s 12s/step - loss: 0.6821 - binary_accuracy: 0.7760 - recall_2: 0.8263 - precision_2: 0.3783 - val_loss: 0.4543 - val_binary_accuracy: 0.8028 - val_recall_2: 0.7419 - val_precision_2: 0.3988\n",
      "Epoch 3/5\n",
      "82/82 [==============================] - 747s 9s/step - loss: 0.6224 - binary_accuracy: 0.8068 - recall_2: 0.8711 - precision_2: 0.4222 - val_loss: 0.4345 - val_binary_accuracy: 0.8166 - val_recall_2: 0.7527 - val_precision_2: 0.4217\n",
      "Epoch 4/5\n",
      "82/82 [==============================] - 743s 9s/step - loss: 0.5824 - binary_accuracy: 0.8260 - recall_2: 0.8895 - precision_2: 0.4519 - val_loss: 0.4241 - val_binary_accuracy: 0.8305 - val_recall_2: 0.7419 - val_precision_2: 0.4452\n",
      "Epoch 5/5\n",
      "82/82 [==============================] - 808s 10s/step - loss: 0.5521 - binary_accuracy: 0.8383 - recall_2: 0.9079 - precision_2: 0.4726 - val_loss: 0.4182 - val_binary_accuracy: 0.8290 - val_recall_2: 0.7312 - val_precision_2: 0.4416\n",
      "Epoch 1/5\n",
      "82/82 [==============================] - 761s 9s/step - loss: 0.8428 - binary_accuracy: 0.6890 - recall_3: 0.8450 - precision_3: 0.3042 - val_loss: 0.4246 - val_binary_accuracy: 0.8320 - val_recall_3: 0.7093 - val_precision_3: 0.4207\n",
      "Epoch 2/5\n",
      "82/82 [==============================] - 763s 9s/step - loss: 0.6780 - binary_accuracy: 0.7833 - recall_3: 0.8605 - precision_3: 0.3955 - val_loss: 0.3925 - val_binary_accuracy: 0.8459 - val_recall_3: 0.7209 - val_precision_3: 0.4493\n",
      "Epoch 3/5\n",
      "82/82 [==============================] - 765s 9s/step - loss: 0.6200 - binary_accuracy: 0.8060 - recall_3: 0.8760 - precision_3: 0.4264 - val_loss: 0.3819 - val_binary_accuracy: 0.8521 - val_recall_3: 0.7442 - val_precision_3: 0.4638\n",
      "Epoch 4/5\n",
      "82/82 [==============================] - 749s 9s/step - loss: 0.5815 - binary_accuracy: 0.8206 - recall_3: 0.8837 - precision_3: 0.4482 - val_loss: 0.3784 - val_binary_accuracy: 0.8521 - val_recall_3: 0.7674 - val_precision_3: 0.4648\n",
      "Epoch 5/5\n",
      "82/82 [==============================] - 753s 9s/step - loss: 0.5526 - binary_accuracy: 0.8329 - recall_3: 0.9044 - precision_3: 0.4685 - val_loss: 0.3774 - val_binary_accuracy: 0.8567 - val_recall_3: 0.7674 - val_precision_3: 0.4748\n",
      "Epoch 1/5\n",
      "82/82 [==============================] - 767s 9s/step - loss: 0.8309 - binary_accuracy: 0.7012 - recall_4: 0.7756 - precision_4: 0.2814 - val_loss: 0.5429 - val_binary_accuracy: 0.7338 - val_recall_4: 0.7934 - val_precision_4: 0.3934\n",
      "Epoch 2/5\n",
      "82/82 [==============================] - 5815s 72s/step - loss: 0.6541 - binary_accuracy: 0.8028 - recall_4: 0.8267 - precision_4: 0.3922 - val_loss: 0.5073 - val_binary_accuracy: 0.7492 - val_recall_4: 0.7934 - val_precision_4: 0.4103\n",
      "Epoch 3/5\n",
      "82/82 [==============================] - 734s 9s/step - loss: 0.5956 - binary_accuracy: 0.8190 - recall_4: 0.8523 - precision_4: 0.4178 - val_loss: 0.4841 - val_binary_accuracy: 0.7815 - val_recall_4: 0.7603 - val_precision_4: 0.4488\n",
      "Epoch 4/5\n",
      "82/82 [==============================] - 728s 9s/step - loss: 0.5563 - binary_accuracy: 0.8317 - recall_4: 0.8665 - precision_4: 0.4388 - val_loss: 0.4693 - val_binary_accuracy: 0.7923 - val_recall_4: 0.7686 - val_precision_4: 0.4650\n",
      "Epoch 5/5\n",
      "82/82 [==============================] - 720s 9s/step - loss: 0.5261 - binary_accuracy: 0.8437 - recall_4: 0.8892 - precision_4: 0.4603 - val_loss: 0.4591 - val_binary_accuracy: 0.7923 - val_recall_4: 0.7603 - val_precision_4: 0.4646\n"
     ]
    }
   ],
   "source": [
    "model_results = []\n",
    "for i in range(0, 5):\n",
    "    # Create the ith data split\n",
    "    trainset, validset = split_data(i)\n",
    "    \n",
    "    # Prepare data\n",
    "    trainset, validset = preprocess_data(trainset, validset)\n",
    "    \n",
    "    # Get model\n",
    "    model = build_model()\n",
    "    \n",
    "    # Fit model\n",
    "    result = model.fit(trainset,\n",
    "                       epochs=5,\n",
    "                       class_weight=cls_wgts_dic,\n",
    "                       validation_data=validset)\n",
    "    model_results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b17561",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "040620ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4285714328289032\n"
     ]
    }
   ],
   "source": [
    "print(model_results[0].history['val_precision' + ''][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "113d0e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results on validation sets:\n",
      "Split 1\n",
      "Accuracy: 0.8243451714515686\n",
      "Precision: 0.4285714328289032\n",
      "Recall: 0.8426966071128845\n",
      "--------------------\n",
      "Split 2\n",
      "Accuracy: 0.7996918559074402\n",
      "Precision: 0.3693181872367859\n",
      "Recall: 0.773809552192688\n",
      "--------------------\n",
      "Split 3\n",
      "Accuracy: 0.8289676308631897\n",
      "Precision: 0.44155845046043396\n",
      "Recall: 0.7311828136444092\n",
      "--------------------\n",
      "Split 4\n",
      "Accuracy: 0.8567026257514954\n",
      "Precision: 0.4748201370239258\n",
      "Recall: 0.7674418687820435\n",
      "--------------------\n",
      "Split 5\n",
      "Accuracy: 0.7923076748847961\n",
      "Precision: 0.46464645862579346\n",
      "Recall: 0.7603305578231812\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "print(\"Results on validation sets:\")\n",
    "for i in range(5):\n",
    "    result = model_results[i]\n",
    "    suffix = ''\n",
    "    if i != 0:\n",
    "        suffix = '_' + str(i)\n",
    "    \n",
    "    print(\"Split \" + str(i+1))\n",
    "    print(\"Accuracy: {0}\\nPrecision: {1}\\nRecall: {2}\".format(\n",
    "        result.history['val_binary_accuracy'][-1],\n",
    "        result.history['val_precision' + suffix][-1],\n",
    "        result.history['val_recall' + suffix][-1]))\n",
    "    print(\"--------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
