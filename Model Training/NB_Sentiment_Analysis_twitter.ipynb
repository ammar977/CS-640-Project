{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "interpreter": {
      "hash": "586ad1ed5c97141e2437e681efbf1ec0adcd17d830cf5af2ca3d2819e743e158"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "name": "NB_Sentiment_Analysis_HW5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MwpnS0rQApYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npERY2Tqwb0N",
        "outputId": "05fffbc3-336a-48e7-ab86-976abd8ec015"
      },
      "source": [
        "# !pip install transformers\n",
        "!pip install pandas\n",
        "!pip install numpy\n",
        "!pip install sklearn\n",
        "!pip install transformers\n",
        "!pip install demoji\n",
        "!pip install googletrans==3.1.0a0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.19.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (1.0.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (3.0.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.19.5)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.4.1)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.13.0-py3-none-any.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 4.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 38.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 461 kB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 40.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 41.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.2.1 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.13.0\n",
            "Collecting demoji\n",
            "  Downloading demoji-1.1.0-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: demoji\n",
            "Successfully installed demoji-1.1.0\n",
            "Collecting googletrans==3.1.0a0\n",
            "  Downloading googletrans-3.1.0a0.tar.gz (19 kB)\n",
            "Collecting httpx==0.13.3\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 1.9 MB/s \n",
            "\u001b[?25hCollecting httpcore==0.9.*\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 902 kB/s \n",
            "\u001b[?25hCollecting sniffio\n",
            "  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2021.10.8)\n",
            "Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (3.0.4)\n",
            "Collecting hstspreload\n",
            "  Downloading hstspreload-2021.12.1-py3-none-any.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 9.8 MB/s \n",
            "\u001b[?25hCollecting rfc3986<2,>=1.3\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: idna==2.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2.10)\n",
            "Collecting h2==3.*\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[K     |████████████████████████████████| 65 kB 3.0 MB/s \n",
            "\u001b[?25hCollecting h11<0.10,>=0.8\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 2.0 MB/s \n",
            "\u001b[?25hCollecting hyperframe<6,>=5.2.0\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Collecting hpack<4,>=3.0\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-3.1.0a0-py3-none-any.whl size=16367 sha256=d0d58522a779e55c056c95ced4db0103547d628faa7f06e7f1387f0b2aa27478\n",
            "  Stored in directory: /root/.cache/pip/wheels/0c/be/fe/93a6a40ffe386e16089e44dad9018ebab9dc4cb9eb7eab65ae\n",
            "Successfully built googletrans\n",
            "Installing collected packages: hyperframe, hpack, sniffio, h2, h11, rfc3986, httpcore, hstspreload, httpx, googletrans\n",
            "Successfully installed googletrans-3.1.0a0 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2021.12.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 rfc3986-1.5.0 sniffio-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "print(pd.__version__)\n",
        "import re\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report\n",
        "import unicodedata\n",
        "from googletrans import Translator\n",
        "import demoji"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUV5_V5pZMxL",
        "outputId": "254d3e24-cd56-4a56-b03f-c43e61fdd836"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.1.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "moeFpyFySESO",
        "outputId": "c31095c1-3884-4931-9184-a8d37867944d"
      },
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive/', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFGvfxwafgVp"
      },
      "source": [
        "## Load Twitter Dataset Files:  Training & Test "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxi-TOsRwb0T"
      },
      "source": [
        "twitter_train=pd.read_csv('/content/drive/My Drive/Colab Notebooks/trainingandtestdata/training.1600000.processed.noemoticon.csv',encoding='latin1',header=None)\n",
        "twitter_test=pd.read_csv('/content/drive/My Drive/Colab Notebooks/trainingandtestdata/testdata.manual.2009.06.14.csv',encoding='latin1',header=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hus9RCdSgr-j"
      },
      "source": [
        "## Preprocess the Training and Test data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn a Unicode string to plain ASCII, thanks to\n",
        "# https://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join( c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "def trans_text(df, col):\n",
        "    translator = Translator()\n",
        "    translations = {}\n",
        "    # translations[t] = translator.translate(t,dest='en').text\n",
        "    cnt = 0\n",
        "    df_en = df.copy()\n",
        "    translations = {}\n",
        "    # unique elements of the column\n",
        "    unique_elements = df_en[col].unique()\n",
        "    for element in unique_elements:\n",
        "        # add translation to the dictionary\n",
        "        detected_eq = translator.detect(element)\n",
        "        if detected_eq.lang!='en' and detected_eq.confidence> 0.5:\n",
        "            print(element)\n",
        "            translations[element] = translator.translate(element).text\n",
        "            cnt+=1\n",
        "    return translations, int(cnt)\n",
        "    \n",
        "print(demoji.replace_with_desc('▪', \" \"))\n",
        "translator = Translator()\n",
        "try:\n",
        "    trans = translator.translate('안녕하세요.',dest='en')\n",
        "    translation = translator.translate(\"Der Himmel ist blau und ich mag Bananen\", dest='en')\n",
        "except BaseException as err:\n",
        "    print('Error returned')\n",
        "    print(f\"Unexpected {err} =, {type(err)}\")\n",
        "print(translation.text)\n",
        "print(trans.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9n8-dTgbZgBX",
        "outputId": "d5f319ff-9aff-4dfd-c3cb-9c6a061e5732"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " black small square \n",
            "The sky is blue and I like bananas\n",
            "Hello.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFB066u2lnsg"
      },
      "source": [
        "regexMap={r\"<[\\w'/'\\s]*>\": \"\",r\"[\\'\\\"\\-]+\": \"\",r\"@[\\w]+\":\"\",r\"#[\\w]+\":\"\"}\n",
        "\n",
        "def preprocess(datainput):\n",
        "    t=datainput\n",
        "    t = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', t, flags=re.MULTILINE)\n",
        "    for regx in regexMap.keys():\n",
        "        t = re.sub(regx, regexMap[regx], t)\n",
        "    return t\n",
        "\n",
        "def remove_emoji(datainput):\n",
        "    t=datainput\n",
        "    t = demoji.replace_with_desc(t, \" \")\n",
        "    return t\n",
        "\n",
        "def no_bracket(datainput):\n",
        "    t=datainput\n",
        "    t = t[2:-2]\n",
        "    return t\n",
        "\n",
        "\n",
        "def lower_text(datainput):\n",
        "    t=datainput\n",
        "    t = unicodeToAscii(t.lower().strip())\n",
        "    # Lowercase, trim, and remove non-letter characters\n",
        "    t = re.sub(r\"([.!?])\", r\" \\1\", t)\n",
        "    t = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", t)\n",
        "    return t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "rRjLrhFvebhT",
        "outputId": "16ccee45-5313-4d34-83ef-ab65c568df07"
      },
      "source": [
        "# Prep the training data: select the subset of data fields, name the columns, remove usernames and URL links\n",
        "twitter_train=twitter_train[[0,5]]\n",
        "twitter_train.columns=[\"sentiment\", \"tweet\"]\n",
        "twitter_train['tweet']=twitter_train['tweet'].apply(preprocess)\n",
        "twitter_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Awww, thats a bummer.  You shoulda got Davi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>is upset that he cant update his Facebook by t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>I dived many times for the ball. Managed to s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>my whole body feels itchy and like its on fire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>no, its not behaving at all. im mad. why am i...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sentiment                                              tweet\n",
              "0          0     Awww, thats a bummer.  You shoulda got Davi...\n",
              "1          0  is upset that he cant update his Facebook by t...\n",
              "2          0   I dived many times for the ball. Managed to s...\n",
              "3          0    my whole body feels itchy and like its on fire \n",
              "4          0   no, its not behaving at all. im mad. why am i..."
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# TRANSLATE TO ENGLISH\n",
        "df = trans_text(twitter_train,'tweet')\n",
        "twitter_train['tweet'] = df['tweet']\n",
        "print(df.head())\n",
        "print(twitter_train.head())"
      ],
      "metadata": {
        "id": "ZresLXynaf-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# /// twitter_train.to_csv('./Translated_Training.csv', header=None, index=False,encoding='latin1')\n",
        "twitter_train['tweet']=twitter_train['tweet'].apply(remove_emoji)\n",
        "twitter_train.head()"
      ],
      "metadata": {
        "id": "i-2yWfbCgwQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# twitter_train.to_csv('./Noemoji_Training.csv',header=None, index=False,encoding='latin1')\n",
        "twitter_train['tweet']=twitter_train['tweet'].apply(lower_text)\n",
        "twitter_train.to_csv('/content/drive/My Drive/Colab Notebooks/trainingandtestdata/Lower_Training.csv', header=None, index=False,encoding='latin1')\n",
        "twitter_train.head()"
      ],
      "metadata": {
        "id": "FwB8hTsUhQXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_EA_3xcwb0T"
      },
      "source": [
        "# Prep the test data\n",
        "\n",
        "twitter_test=twitter_test.loc[:,(0,5)]\n",
        "twitter_test.columns=[\"sentiment\", \"tweet\"]\n",
        "twitter_test[\"tweet\"]=twitter_test[\"tweet\"].apply(preprocess)\n",
        "twitter_test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "twitter_test['tweet']=twitter_test['tweet'].apply(remove_emoji)\n",
        "twitter_test.head()\n",
        "twitter_test['tweet']=twitter_test['tweet'].apply(lower_text)\n",
        "twitter_test.to_csv('/content/drive/My Drive/Colab Notebooks/trainingandtestdata/Lower_Test.csv', header=None, index=False,encoding='latin1')\n",
        "twitter_test.head()"
      ],
      "metadata": {
        "id": "jHv14O8NGNAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWi5rpjQwb0X"
      },
      "source": [
        "print(len(twitter_train))\n",
        "print(len(twitter_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkQgroOf3LKf"
      },
      "source": [
        "# #Switch out half of the no-emotion dataset in the test file for other data in the train file\n",
        "# a = twitter_train[(twitter_train['sentiment']==0)][0:34]\n",
        "# b = twitter_train[(twitter_train['sentiment']==4)][60:95]\n",
        "# c = twitter_test[(twitter_test['sentiment']==2)][0:34]\n",
        "# d = twitter_test[(twitter_test['sentiment']==2)][60:95]\n",
        "# ai = twitter_train.index[(twitter_train['sentiment']==0)][0:34]\n",
        "# bi = twitter_train.index[(twitter_train['sentiment']==4)][60:95]\n",
        "# ci = twitter_test.index[(twitter_test['sentiment']==2)][0:34]\n",
        "# di = twitter_test.index[(twitter_test['sentiment']==2)][60:95]\n",
        "# twitter_train = twitter_train.drop(ai)\n",
        "# twitter_train = twitter_train.drop(bi)\n",
        "# twitter_train = twitter_train.append(c)\n",
        "# twitter_train = twitter_train.append(d)\n",
        "# twitter_test = twitter_test.drop(ci)\n",
        "# twitter_test = twitter_test.drop(di)\n",
        "# twitter_test = twitter_test.append(a)\n",
        "# twitter_test = twitter_test.append(b)\n",
        "\n",
        "# Try dropping the neutral sentiments from test per instructor's notes\n",
        "ai = twitter_test.index[(twitter_test['sentiment']==2)]\n",
        "twitter_test = twitter_test.drop(ai)\n",
        "\n",
        "print(len(twitter_train))\n",
        "print(len(twitter_test))\n",
        "\n",
        "print(twitter_train['tweet'][6800])\n",
        "# print(twitter_test['tweet'][200])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBSuwyKpwb0Z"
      },
      "source": [
        "vectorizer = CountVectorizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VF973X4Awb0Z"
      },
      "source": [
        "# Fit Count Vectorizer on training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwZ2E5d_wb0Z"
      },
      "source": [
        "vectorizer.fit(twitter_train['tweet'].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lL0TrV4Awb0a"
      },
      "source": [
        "vectorizer.vocabulary_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBP538IAwb0a"
      },
      "source": [
        "## Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EE1TjZq5wb0a"
      },
      "source": [
        "stop_words_vectorizer=CountVectorizer(stop_words='english')\n",
        "stop_words_vectorizer.fit(twitter_train['tweet'].values)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AgG_c-Swb0a"
      },
      "source": [
        "stop_words_vectorizer.vocabulary_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNkZXyWqwb0b"
      },
      "source": [
        "x_input=stop_words_vectorizer.transform(twitter_train['tweet'].values)\n",
        "x_test_input=stop_words_vectorizer.transform(twitter_test['tweet'].values)\n",
        "y_input=twitter_train['sentiment'].values\n",
        "y_test=twitter_test['sentiment'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CD9dp9qwb0b"
      },
      "source": [
        "nb = MultinomialNB(alpha=1.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfpyhAdTwb0b"
      },
      "source": [
        "nb.fit(x_input,y_input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-ie627Mwb0b"
      },
      "source": [
        "y_pred_class = nb.predict(x_test_input)\n",
        "metrics.accuracy_score(y_test, y_pred_class)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGLisPEEwb0b"
      },
      "source": [
        "print(classification_report(y_true=y_test,y_pred=y_pred_class))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-LEGQMUwb0c"
      },
      "source": [
        "len(twitter_train[(twitter_train['sentiment']==0)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WErof8opl09"
      },
      "source": [
        "len(twitter_train[(twitter_train['sentiment']==2)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bD_m7d75plYL"
      },
      "source": [
        "len(twitter_train[(twitter_train['sentiment']==4)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1z8DdKxCtVp0"
      },
      "source": [
        "len(pd.unique(twitter_train['sentiment']))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9rJC_aDtkw5"
      },
      "source": [
        "len(pd.unique(twitter_test['sentiment']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tE0iZrfXuMgm"
      },
      "source": [
        "twitter_train.sentiment.unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFhUeoF9uYVE"
      },
      "source": [
        "twitter_test.sentiment.unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlKerBXPujav"
      },
      "source": [
        "len(twitter_test[(twitter_test['sentiment']==0)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dqrf4qekurfz"
      },
      "source": [
        "len(twitter_test[(twitter_test['sentiment']==2)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXxY1nGdusOk"
      },
      "source": [
        "len(twitter_test[(twitter_test['sentiment']==4)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mespBm0vauI"
      },
      "source": [
        "b=twitter_test.index[(twitter_test['sentiment']==2)]\n",
        "b[0:34]\n",
        "b[60:95]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zagnqHggxmC4"
      },
      "source": [
        "a=twitter_test.index[(twitter_test['sentiment']==0)]\n",
        "a[0:34]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "686VrWnaxoMD"
      },
      "source": [
        "c=twitter_test.index[(twitter_test['sentiment']==4)]\n",
        "c[60:95]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-tGEyy55Ha1e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}