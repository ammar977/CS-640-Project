{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "1b215d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ammarahmad/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import preprocessor \n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "import fasttext\n",
    "import csv\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import itertools\n",
    "import emoji\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ef1b81",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "ebb909e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total train samples :  6755\n",
      "fear       2142\n",
      "anger      1617\n",
      "joy        1537\n",
      "sadness    1459\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"../Data Twitter/Train/\"\n",
    "dfs = []\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('txt'):\n",
    "        path = os.path.join(folder_path,filename)\n",
    "        df = pd.read_csv(path,sep='\\t',header=None)\n",
    "        df = df.drop(columns=[df.columns[0],df.columns[3]])\n",
    "        dfs.append(df)\n",
    "\n",
    "df_train = pd.concat(dfs)\n",
    "df_train.columns = ['text','label']\n",
    "df_train['label_numeric'] = df_train['label'].astype('category').cat.codes\n",
    "\n",
    "\n",
    "folder_path = \"../Data Twitter/Test/\"\n",
    "dfs = []\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('txt'):\n",
    "        path = os.path.join(folder_path,filename)\n",
    "        df = pd.read_csv(path,sep='\\t',header=None)\n",
    "        df = df.drop(columns=[df.columns[0],df.columns[3]])\n",
    "        dfs.append(df)\n",
    "\n",
    "df_train_2 = pd.concat(dfs)\n",
    "df_train_2.columns = ['text','label']\n",
    "df_train_2['label_numeric'] = df_train_2['label'].astype('category').cat.codes\n",
    "\n",
    "df_train = df_train.append(df_train_2).reset_index()\n",
    "print('total train samples : ',len(df_train))\n",
    "print(df_train['label'].value_counts())\n",
    "\n",
    "class_mapping = list(df_train['label'].astype('category').cat.categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "95d9df0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total train samples :  694\n",
      "fear       220\n",
      "anger      168\n",
      "joy        158\n",
      "sadness    148\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"../Data Twitter/Dev/\"\n",
    "dfs = []\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('txt'):\n",
    "        path = os.path.join(folder_path,filename)\n",
    "        df = pd.read_csv(path,sep='\\t',header=None)\n",
    "        df = df.drop(columns=[df.columns[0],df.columns[3]])\n",
    "        dfs.append(df)\n",
    "\n",
    "df_dev = pd.concat(dfs)\n",
    "df_dev.columns = ['text','label']\n",
    "df_dev['label_numeric'] = df_dev['label'].astype('category').cat.codes\n",
    "\n",
    "folder_path = \"../Data Twitter/Dev/without intensity\"\n",
    "dfs = []\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('txt'):\n",
    "        path = os.path.join(folder_path,filename)\n",
    "        df = pd.read_csv(path,sep='\\t',header=None)\n",
    "        df = df.drop(columns=[df.columns[0],df.columns[3]])\n",
    "        dfs.append(df)\n",
    "\n",
    "df_dev_2 = pd.concat(dfs)\n",
    "df_dev_2.columns = ['text','label']\n",
    "df_dev_2['label_numeric'] = df_dev_2['label'].astype('category').cat.codes\n",
    "\n",
    "df_dev = df_dev.append(df_dev_2).reset_index()\n",
    "print('total train samples : ',len(df_dev))\n",
    "print(df_dev['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab864b92",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "fe292941",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor.set_options(preprocessor.OPT.URL,preprocessor.OPT.RESERVED)\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "def preprocess(text_str):    \n",
    "    text_str = preprocessor.tokenize(text_str)\n",
    "    text_str = ' '.join([word for word in text_str.split(' ') if word.lower() not in stop_words])\n",
    "    return text_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "35208f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def load_dict_smileys():\n",
    "    \n",
    "    return {\n",
    "        \":‑)\":\"smiley\",\n",
    "        \":-]\":\"smiley\",\n",
    "        \":-3\":\"smiley\",\n",
    "        \":->\":\"smiley\",\n",
    "        \"8-)\":\"smiley\",\n",
    "        \":-}\":\"smiley\",\n",
    "        \":)\":\"smiley\",\n",
    "        \":]\":\"smiley\",\n",
    "        \":3\":\"smiley\",\n",
    "        \":>\":\"smiley\",\n",
    "        \"8)\":\"smiley\",\n",
    "        \":}\":\"smiley\",\n",
    "        \":o)\":\"smiley\",\n",
    "        \":c)\":\"smiley\",\n",
    "        \":^)\":\"smiley\",\n",
    "        \"=]\":\"smiley\",\n",
    "        \"=)\":\"smiley\",\n",
    "        \":-))\":\"smiley\",\n",
    "        \":‑D\":\"smiley\",\n",
    "        \"8‑D\":\"smiley\",\n",
    "        \"x‑D\":\"smiley\",\n",
    "        \"X‑D\":\"smiley\",\n",
    "        \":D\":\"smiley\",\n",
    "        \"8D\":\"smiley\",\n",
    "        \"xD\":\"smiley\",\n",
    "        \"XD\":\"smiley\",\n",
    "        \":‑(\":\"sad\",\n",
    "        \":‑c\":\"sad\",\n",
    "        \":‑<\":\"sad\",\n",
    "        \":‑[\":\"sad\",\n",
    "        \":(\":\"sad\",\n",
    "        \":c\":\"sad\",\n",
    "        \":<\":\"sad\",\n",
    "        \":[\":\"sad\",\n",
    "        \":-||\":\"sad\",\n",
    "        \">:[\":\"sad\",\n",
    "        \":{\":\"sad\",\n",
    "        \":@\":\"sad\",\n",
    "        \">:(\":\"sad\",\n",
    "        \":'‑(\":\"sad\",\n",
    "        \":'(\":\"sad\",\n",
    "        \":‑P\":\"playful\",\n",
    "        \"X‑P\":\"playful\",\n",
    "        \"x‑p\":\"playful\",\n",
    "        \":‑p\":\"playful\",\n",
    "        \":‑Þ\":\"playful\",\n",
    "        \":‑þ\":\"playful\",\n",
    "        \":‑b\":\"playful\",\n",
    "        \":P\":\"playful\",\n",
    "        \"XP\":\"playful\",\n",
    "        \"xp\":\"playful\",\n",
    "        \":p\":\"playful\",\n",
    "        \":Þ\":\"playful\",\n",
    "        \":þ\":\"playful\",\n",
    "        \":b\":\"playful\",\n",
    "        \"<3\":\"love\"\n",
    "        }\n",
    "\n",
    "\n",
    "def load_dict_contractions_slangs():\n",
    "    \n",
    "    cont = {\n",
    "        \"ain't\":\"is not\",\n",
    "        \"amn't\":\"am not\",\n",
    "        \"aren't\":\"are not\",\n",
    "        \"can't\":\"cannot\",\n",
    "        \"'cause\":\"because\",\n",
    "        \"couldn't\":\"could not\",\n",
    "        \"couldn't've\":\"could not have\",\n",
    "        \"could've\":\"could have\",\n",
    "        \"daren't\":\"dare not\",\n",
    "        \"daresn't\":\"dare not\",\n",
    "        \"dasn't\":\"dare not\",\n",
    "        \"didn't\":\"did not\",\n",
    "        \"doesn't\":\"does not\",\n",
    "        \"don't\":\"do not\",\n",
    "        \"e'er\":\"ever\",\n",
    "        \"em\":\"them\",\n",
    "        \"everyone's\":\"everyone is\",\n",
    "        \"finna\":\"fixing to\",\n",
    "        \"gimme\":\"give me\",\n",
    "        \"gonna\":\"going to\",\n",
    "        \"gon't\":\"go not\",\n",
    "        \"gotta\":\"got to\",\n",
    "        \"hadn't\":\"had not\",\n",
    "        \"hasn't\":\"has not\",\n",
    "        \"haven't\":\"have not\",\n",
    "        \"he'd\":\"he would\",\n",
    "        \"he'll\":\"he will\",\n",
    "        \"he's\":\"he is\",\n",
    "        \"he've\":\"he have\",\n",
    "        \"how'd\":\"how would\",\n",
    "        \"how'll\":\"how will\",\n",
    "        \"how're\":\"how are\",\n",
    "        \"how's\":\"how is\",\n",
    "        \"i'd\":\"i would\",\n",
    "        \"i'll\":\"i will\",\n",
    "        \"i'm\":\"i am\",\n",
    "        \"i'm'a\":\"i am about to\",\n",
    "        \"i'm'o\":\"i am going to\",\n",
    "        \"isn't\":\"is not\",\n",
    "        \"it'd\":\"it would\",\n",
    "        \"it'll\":\"it will\",\n",
    "        \"it's\":\"it is\",\n",
    "        \"i've\":\"i have\",\n",
    "        \"kinda\":\"kind of\",\n",
    "        \"let's\":\"let us\",\n",
    "        \"mayn't\":\"may not\",\n",
    "        \"may've\":\"may have\",\n",
    "        \"mightn't\":\"might not\",\n",
    "        \"might've\":\"might have\",\n",
    "        \"mustn't\":\"must not\",\n",
    "        \"mustn't've\":\"must not have\",\n",
    "        \"must've\":\"must have\",\n",
    "        \"needn't\":\"need not\",\n",
    "        \"ne'er\":\"never\",\n",
    "        \"o'\":\"of\",\n",
    "        \"o'er\":\"over\",\n",
    "        \"ol'\":\"old\",\n",
    "        \"oughtn't\":\"ought not\",\n",
    "        \"shalln't\":\"shall not\",\n",
    "        \"shan't\":\"shall not\",\n",
    "        \"she'd\":\"she would\",\n",
    "        \"she'll\":\"she will\",\n",
    "        \"she's\":\"she is\",\n",
    "        \"shouldn't\":\"should not\",\n",
    "        \"shouldn't've\":\"should not have\",\n",
    "        \"should've\":\"should have\",\n",
    "        \"somebody's\":\"somebody is\",\n",
    "        \"someone's\":\"someone is\",\n",
    "        \"something's\":\"something is\",\n",
    "        \"that'd\":\"that would\",\n",
    "        \"that'll\":\"that will\",\n",
    "        \"that're\":\"that are\",\n",
    "        \"that's\":\"that is\",\n",
    "        \"there'd\":\"there would\",\n",
    "        \"there'll\":\"there will\",\n",
    "        \"there're\":\"there are\",\n",
    "        \"there's\":\"there is\",\n",
    "        \"these're\":\"these are\",\n",
    "        \"they'd\":\"they would\",\n",
    "        \"they'll\":\"they will\",\n",
    "        \"they're\":\"they are\",\n",
    "        \"they've\":\"they have\",\n",
    "        \"this's\":\"this is\",\n",
    "        \"those're\":\"those are\",\n",
    "        \"'tis\":\"it is\",\n",
    "        \"'twas\":\"it was\",\n",
    "        \"wanna\":\"want to\",\n",
    "        \"wasn't\":\"was not\",\n",
    "        \"we'd\":\"we would\",\n",
    "        \"we'd've\":\"we would have\",\n",
    "        \"we'll\":\"we will\",\n",
    "        \"we're\":\"we are\",\n",
    "        \"weren't\":\"were not\",\n",
    "        \"we've\":\"we have\",\n",
    "        \"what'd\":\"what did\",\n",
    "        \"what'll\":\"what will\",\n",
    "        \"what're\":\"what are\",\n",
    "        \"what's\":\"what is\",\n",
    "        \"what've\":\"what have\",\n",
    "        \"when's\":\"when is\",\n",
    "        \"where'd\":\"where did\",\n",
    "        \"where're\":\"where are\",\n",
    "        \"where's\":\"where is\",\n",
    "        \"where've\":\"where have\",\n",
    "        \"which's\":\"which is\",\n",
    "        \"who'd\":\"who would\",\n",
    "        \"who'd've\":\"who would have\",\n",
    "        \"who'll\":\"who will\",\n",
    "        \"who're\":\"who are\",\n",
    "        \"who's\":\"who is\",\n",
    "        \"who've\":\"who have\",\n",
    "        \"why'd\":\"why did\",\n",
    "        \"why're\":\"why are\",\n",
    "        \"why's\":\"why is\",\n",
    "        \"won't\":\"will not\",\n",
    "        \"wouldn't\":\"would not\",\n",
    "        \"would've\":\"would have\",\n",
    "        \"y'all\":\"you all\",\n",
    "        \"you'd\":\"you would\",\n",
    "        \"you'll\":\"you will\",\n",
    "        \"you're\":\"you are\",\n",
    "        \"you've\":\"you have\",\n",
    "        \"Whatcha\":\"What are you\",\n",
    "        \"luv\":\"love\",\n",
    "        \"sux\":\"sucks\",\n",
    "        \"rn\":\"right now\",\n",
    "        \"atm\": \"at the moment\",\n",
    "        \"idk\": \"i dont know\",\n",
    "        \"cuz\": \"because\",\n",
    "        \"bcuz\": \"because\",\n",
    "        \"ur\": \"you are\",\n",
    "        \"ly\": \"love you\",\n",
    "        \"lol\": \"laugh out loud\",\n",
    "        \"rofl\": \"rolling on floor laughing\",\n",
    "        \"lmao\": \"laughing my ass off\",\n",
    "        \"ok\": \"okay\",\n",
    "        \"ty\": \"thank you\",\n",
    "        \"fav\": \"favorite\",\n",
    "        \"omg\": \"oh my god\"\n",
    "        }\n",
    "    \n",
    "    to_ret = {}\n",
    "    for k,v in cont.items():\n",
    "        key = k.lower().replace(\"'\",'')\n",
    "        to_ret[key] = v.lower()\n",
    "        \n",
    "    return to_ret\n",
    "\n",
    "\n",
    "def strip_accents(text):\n",
    "    if 'ø' in text or  'Ø' in text:\n",
    "        #Do nothing when finding ø \n",
    "        return text   \n",
    "    text = text.encode('ascii', 'ignore')\n",
    "    text = text.decode(\"utf-8\")\n",
    "    return str(text)\n",
    "\n",
    "\n",
    "def tweet_cleaning_for_sentiment_analysis(tweet):    \n",
    "    \n",
    "    #Escaping HTML characters\n",
    "    tweet = BeautifulSoup(tweet).get_text()\n",
    "    #Special case not handled previously.\n",
    "    tweet = tweet.replace('\\x92',\"'\")\n",
    "    #Removal of account\n",
    "    tweet = ' '.join(re.sub(\"(@[A-Za-z0-9_]+)\", \" \", tweet).split())\n",
    "    # removal of hashtag\n",
    "    tweet = tweet.replace('#','')\n",
    "    #Removal of address\n",
    "    tweet = ' '.join(re.sub(\"(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
    "    #Lower case\n",
    "    tweet = tweet.lower()\n",
    "    # rempval of 'RT'\n",
    "    tweet = tweet.replace('rt','')\n",
    "    # Removal of Punctuation\n",
    "    tweet = ' '.join(re.sub(\"[\\.\\,\\!\\?\\:\\;\\-\\=]\", \" \", tweet).split())\n",
    "    # removal of stop words\n",
    "    tweet = ' '.join([word for word in tweet.split(' ') if word not in stop_words])\n",
    "    # CONTRACTIONS source: https://en.wikipedia.org/wiki/Contraction_%28grammar%29\n",
    "    CONTRACTIONS_SLANGS = load_dict_contractions_slangs()\n",
    "    tweet = tweet.replace(\"’\",\"'\").replace(\"'\",\"\")\n",
    "    words = tweet.split()\n",
    "    reformed = [CONTRACTIONS_SLANGS[word] if word in CONTRACTIONS_SLANGS else word for word in words]\n",
    "    tweet = \" \".join(reformed)\n",
    "\n",
    "    # Standardizing words - lemmatization\n",
    "    tweet = ' '.join([lemmatizer.lemmatize(word) for word in tweet.split(' ')]).lower()\n",
    "    \n",
    "    #Deal with smileys\n",
    "    #source: https://en.wikipedia.org/wiki/List_of_emoticons\n",
    "    SMILEY = load_dict_smileys()  \n",
    "    words = tweet.split()\n",
    "    reformed = [SMILEY[word] if word in SMILEY else word for word in words]\n",
    "    tweet = \" \".join(reformed)\n",
    "    # replace emojis with desctiption - remove accents - remove underscores\n",
    "    tweet = emoji.demojize(tweet).lower()\n",
    "    tweet = strip_accents(tweet).lower()\n",
    "    tweet = tweet.replace(\":\",\" \")\n",
    "    tweet = ' '.join(tweet.split()).replace('_',' ')\n",
    "    \n",
    "    # remove new line characters\n",
    "    tweet = tweet.strip().replace('\\\\n',' ')\n",
    "    \n",
    "    # only keep alphabets\n",
    "    tweet = ''.join([c for c in tweet if (c.isalpha() or c ==' ')]).strip()\n",
    "    \n",
    "    # rempove double spaces and triple spaces\n",
    "    tweet = tweet.replace('   ',' ').replace('  ',' ')\n",
    "    \n",
    "    # one word replacements\n",
    "    replacement_dict = {\n",
    "        'u': 'you',\n",
    "        'v': 'we',\n",
    "        'r': 'are',\n",
    "        'w': 'we',\n",
    "        'n': 'and',\n",
    "        'nd': 'and',\n",
    "        '&': 'and'\n",
    "    }\n",
    "    \n",
    "    tweet = ' '.join([replacement_dict[word] if word in replacement_dict else word for word in tweet.split(' ')])\n",
    "    \n",
    "    # removal of stop words\n",
    "    tweet = ' '.join([word for word in tweet.split(' ') if word not in stop_words])\n",
    "\n",
    "    return tweet\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "fd609c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['text'] = df_train['text'].apply(tweet_cleaning_for_sentiment_analysis)\n",
    "df_dev['text'] = df_dev['text'].apply(tweet_cleaning_for_sentiment_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74cd62d",
   "metadata": {},
   "source": [
    "# Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "4d1b4749",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_instance(txt,label):\n",
    "    cur_row = []\n",
    "    #Prefix the index-ed label with __label__\n",
    "    label = \"__label__\" + label  \n",
    "    cur_row.append(label)\n",
    "    cur_row.extend(nltk.word_tokenize(txt))\n",
    "    return cur_row\n",
    "\n",
    "\n",
    "def preprocess(input_text,labels, output_file, keep=1):\n",
    "    i=0\n",
    "    with open(output_file, 'w') as csvoutfile:\n",
    "        csv_writer = csv.writer(csvoutfile, delimiter=' ', lineterminator='\\n')\n",
    "        for i in range(len(labels)):\n",
    "            row_output = transform_instance(input_text[i],labels[i])\n",
    "            csv_writer.writerow(row_output )\n",
    "            i=i+1\n",
    "            if i%10000 ==0:\n",
    "                print(i)\n",
    "            \n",
    "\n",
    "            \n",
    "def prepare_data(lbl):\n",
    "    \n",
    "    df_train_0 = df_train[df_train['label'] == lbl]\n",
    "    df_train_rest = df_train[df_train['label'] != lbl]\n",
    "    df_train_rest = df_train_rest.sample(int(len(df_train_0)*1.5))\n",
    "                                         \n",
    "\n",
    "    df_train_sampled = df_train_0.append(df_train_rest)\n",
    "    df_train_sampled = df_train_sampled.sample(frac=1).reset_index()\n",
    "    \n",
    "    train_labels = df_train_sampled['label'].apply(lambda x: x if x == lbl else 'not-' + lbl ).tolist()\n",
    "    test_labels = df_dev['label'].apply(lambda x: x if x == lbl else 'not-' + lbl ).tolist()\n",
    "    \n",
    "    \n",
    "    # Preparing the training dataset   \n",
    "    preprocess(df_train_sampled['text'].tolist(),train_labels, 'tweets' + lbl + '.train')\n",
    "    # Preparing the validation dataset        \n",
    "    preprocess(df_dev['text'].tolist(),test_labels, 'tweets' + lbl + '.test')\n",
    "    \n",
    "    return test_labels\n",
    "    \n",
    "def train(lbl):\n",
    "    \n",
    "    Y_test = prepare_data(lbl)\n",
    "    hyper_params = {\"lr\": 0.01,\n",
    "                \"epoch\": 200,\n",
    "                \"wordNgrams\": 1,\n",
    "                \"dim\": 100,\n",
    "                \"loss\": \"softmax\",\n",
    "                \"ws\": 3}  \n",
    "    model = fasttext.train_supervised(input='tweets' + lbl + '.train',**hyper_params)\n",
    "    \n",
    "    result = model.test('tweets' + lbl + '.train')\n",
    "    validation = model.test('tweets' + lbl + '.test')\n",
    "    # DISPLAY ACCURACY OF TRAINED MODEL\n",
    "    text_line = str(hyper_params) + \"\\naccuracy:\" + str(result[1])  + \",validation:\" + str(validation[1]) + '\\n' \n",
    "    print(text_line)\n",
    "    \n",
    "    X_test = df_dev['text'].tolist()\n",
    "    Y_pred = model.predict(X_test)\n",
    "    Y_pred = [x[0].split('_')[-1] for x in Y_pred[0]]\n",
    "    print(classification_report(Y_test,Y_pred))\n",
    "    return model\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c1847d",
   "metadata": {},
   "source": [
    "## Train individual Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "e291b4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lr': 0.01, 'epoch': 200, 'wordNgrams': 1, 'dim': 100, 'loss': 'softmax', 'ws': 3}\n",
      "accuracy:0.9981780322748568,validation:0.9481268011527377\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         joy       0.89      0.89      0.89       158\n",
      "     not-joy       0.97      0.97      0.97       536\n",
      "\n",
      "    accuracy                           0.95       694\n",
      "   macro avg       0.93      0.93      0.93       694\n",
      "weighted avg       0.95      0.95      0.95       694\n",
      "\n",
      "{'lr': 0.01, 'epoch': 200, 'wordNgrams': 1, 'dim': 100, 'loss': 'softmax', 'ws': 3}\n",
      "accuracy:0.9888668975754576,validation:0.9250720461095101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.84      0.86      0.85       168\n",
      "   not-anger       0.95      0.95      0.95       526\n",
      "\n",
      "    accuracy                           0.93       694\n",
      "   macro avg       0.90      0.90      0.90       694\n",
      "weighted avg       0.93      0.93      0.93       694\n",
      "\n",
      "{'lr': 0.01, 'epoch': 200, 'wordNgrams': 1, 'dim': 100, 'loss': 'softmax', 'ws': 3}\n",
      "accuracy:0.9850606909430439,validation:0.9020172910662824\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fear       0.87      0.81      0.84       220\n",
      "    not-fear       0.91      0.95      0.93       474\n",
      "\n",
      "    accuracy                           0.90       694\n",
      "   macro avg       0.89      0.88      0.88       694\n",
      "weighted avg       0.90      0.90      0.90       694\n",
      "\n",
      "{'lr': 0.01, 'epoch': 200, 'wordNgrams': 1, 'dim': 100, 'loss': 'softmax', 'ws': 3}\n",
      "accuracy:0.9832739237729641,validation:0.9106628242074928\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " not-sadness       0.94      0.95      0.94       546\n",
      "     sadness       0.79      0.78      0.79       148\n",
      "\n",
      "    accuracy                           0.91       694\n",
      "   macro avg       0.87      0.86      0.87       694\n",
      "weighted avg       0.91      0.91      0.91       694\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_joy = train('joy')\n",
    "model_anger = train('anger')\n",
    "model_fear = train('fear')\n",
    "model_sadness = train('sadness')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47324491",
   "metadata": {},
   "source": [
    "## Train Multiclass Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "3dde33c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lr': 0.01, 'epoch': 100, 'wordNgrams': 1, 'dim': 100, 'loss': 'softmax', 'ws': 5}\n",
      "accuracy:0.9700962250185048,validation:0.8472622478386167\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.81      0.86      0.83       168\n",
      "        fear       0.82      0.88      0.85       220\n",
      "         joy       0.91      0.87      0.89       158\n",
      "     sadness       0.88      0.76      0.81       148\n",
      "\n",
      "    accuracy                           0.85       694\n",
      "   macro avg       0.85      0.84      0.85       694\n",
      "weighted avg       0.85      0.85      0.85       694\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_labels = df_train['label'].tolist()\n",
    "test_labels = df_dev['label'].tolist()\n",
    "# Preparing the training dataset   \n",
    "preprocess(df_train['text'].tolist(),train_labels, 'tweets.train')\n",
    "# Preparing the validation dataset        \n",
    "preprocess(df_dev['text'].tolist(),test_labels, 'tweets.test')\n",
    "\n",
    "hyper_params = {\"lr\": 0.01,\n",
    "                \"epoch\": 100,\n",
    "                \"wordNgrams\": 1,\n",
    "                \"dim\": 100,\n",
    "                \"loss\": \"softmax\",\n",
    "                \"ws\": 5}  \n",
    "\n",
    "model_all = fasttext.train_supervised(input='tweets.train',**hyper_params)\n",
    "\n",
    "result = model_all.test('tweets.train')\n",
    "validation = model_all.test('tweets.test')\n",
    "# DISPLAY ACCURACY OF TRAINED MODEL\n",
    "text_line = str(hyper_params) + \"\\naccuracy:\" + str(result[1])  + \",validation:\" + str(validation[1]) + '\\n' \n",
    "print(text_line)\n",
    "\n",
    "X_test = df_dev['text'].tolist()\n",
    "Y_pred = model_all.predict(X_test)\n",
    "Y_pred = [x[0].split('_')[-1] for x in Y_pred[0]]\n",
    "print(classification_report(test_labels,Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "91080404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sometimes get mad something minuscule try ruin somebody life like lose job like get federal prison\n",
      "pred :  fear\n",
      "actual :  anger\n",
      "\n",
      "\n",
      "\n",
      "please canadian player play player lag atrocious fixthisgame trash sfvrefund\n",
      "pred :  fear\n",
      "actual :  anger\n",
      "\n",
      "\n",
      "\n",
      "sorry guy absolutely idea time cam tomorrow keep posted\n",
      "pred :  fear\n",
      "actual :  anger\n",
      "\n",
      "\n",
      "\n",
      "ding wearing look man found arch enemy bed missus angryman\n",
      "pred :  sadness\n",
      "actual :  anger\n",
      "\n",
      "\n",
      "\n",
      "ding wearing look man found arch enemy bed missus angryman scowl\n",
      "pred :  sadness\n",
      "actual :  anger\n",
      "\n",
      "\n",
      "\n",
      "oh brian invite\n",
      "pred :  fear\n",
      "actual :  anger\n",
      "\n",
      "\n",
      "\n",
      "take k number madden low dropped people unhappy\n",
      "pred :  sadness\n",
      "actual :  anger\n",
      "\n",
      "\n",
      "\n",
      "opinion worst delhi govt acrid hypocrisy\n",
      "pred :  fear\n",
      "actual :  anger\n",
      "\n",
      "\n",
      "\n",
      "thirsty chance disagree left even realize something affront bigoted platform\n",
      "pred :  fear\n",
      "actual :  anger\n",
      "\n",
      "\n",
      "\n",
      "baby born soon lifechanging year feel like yesterday sad happy emotionalrollercoaster\n",
      "pred :  sadness\n",
      "actual :  anger\n",
      "\n",
      "\n",
      "\n",
      "coincidentally watched ulzanas raid last night brutally indignant filmmaking\n",
      "pred :  fear\n",
      "actual :  anger\n",
      "\n",
      "\n",
      "\n",
      "watched django unchained people may frown titter delight\n",
      "pred :  joy\n",
      "actual :  anger\n",
      "\n",
      "\n",
      "\n",
      "really jumanji rock jack black kevin ha kidding wtf thisisaterribleidea\n",
      "pred :  anger\n",
      "actual :  fear\n",
      "\n",
      "\n",
      "\n",
      "dreadful even met catfish still thought\n",
      "pred :  sadness\n",
      "actual :  fear\n",
      "\n",
      "\n",
      "\n",
      "easily forgive child afraid dark real tragedy life men afraid light plato\n",
      "pred :  sadness\n",
      "actual :  fear\n",
      "\n",
      "\n",
      "\n",
      "android app designed buggy work sporadically fire tv box\n",
      "pred :  anger\n",
      "actual :  fear\n",
      "\n",
      "\n",
      "\n",
      "step beating anxiety depression realising waiting take action\n",
      "pred :  sadness\n",
      "actual :  fear\n",
      "\n",
      "\n",
      "\n",
      "nothing worse uber driver cannot drive\n",
      "pred :  anger\n",
      "actual :  fear\n",
      "\n",
      "\n",
      "\n",
      "cannot wait hear say brilliant dr hawking rich poorest taste\n",
      "pred :  joy\n",
      "actual :  fear\n",
      "\n",
      "\n",
      "\n",
      "awareness seek shelter letgo old habit chasing desire resultin anger fear worry choose satisfaction within peace relax\n",
      "pred :  anger\n",
      "actual :  fear\n",
      "\n",
      "\n",
      "\n",
      "modern proverb let anyone intimidate single marriage end divorce\n",
      "pred :  sadness\n",
      "actual :  fear\n",
      "\n",
      "\n",
      "\n",
      "week till pick camera though think group cemetery shoot october make photography\n",
      "pred :  anger\n",
      "actual :  fear\n",
      "\n",
      "\n",
      "\n",
      "thanks ripping luthansa enough one way flight man frk bag free gate\n",
      "pred :  joy\n",
      "actual :  fear\n",
      "\n",
      "\n",
      "\n",
      "isnt obrien supposed offensive genius\n",
      "pred :  anger\n",
      "actual :  fear\n",
      "\n",
      "\n",
      "\n",
      "staff fr asked info told look online get pay ryanair compensation\n",
      "pred :  anger\n",
      "actual :  fear\n",
      "\n",
      "\n",
      "\n",
      "assure laughter increasing anger cost arrogance westminster\n",
      "pred :  anger\n",
      "actual :  joy\n",
      "\n",
      "\n",
      "\n",
      "somebody braved storm brewing\n",
      "pred :  anger\n",
      "actual :  joy\n",
      "\n",
      "\n",
      "\n",
      "mick nod would like went back food smiling finished\n",
      "pred :  fear\n",
      "actual :  joy\n",
      "\n",
      "\n",
      "\n",
      "spirit\n",
      "pred :  fear\n",
      "actual :  joy\n",
      "\n",
      "\n",
      "\n",
      "great mind think alike\n",
      "pred :  fear\n",
      "actual :  joy\n",
      "\n",
      "\n",
      "\n",
      "issue broadband bill charged month signed aiel\n",
      "pred :  fear\n",
      "actual :  joy\n",
      "\n",
      "\n",
      "\n",
      "tell body cheering death black people cop fear killing\n",
      "pred :  fear\n",
      "actual :  joy\n",
      "\n",
      "\n",
      "\n",
      "feel good get outside minute get fresh air hard stay cooped inside day\n",
      "pred :  fear\n",
      "actual :  joy\n",
      "\n",
      "\n",
      "\n",
      "ppl get triggered smiling irrelevant\n",
      "pred :  anger\n",
      "actual :  joy\n",
      "\n",
      "\n",
      "\n",
      "snapchat new would beg differ\n",
      "pred :  anger\n",
      "actual :  joy\n",
      "\n",
      "\n",
      "\n",
      "ball watching rojod header equally dreadful\n",
      "pred :  fear\n",
      "actual :  sadness\n",
      "\n",
      "\n",
      "\n",
      "public product high downheaed price tag consumer survey sum substance ovth\n",
      "pred :  fear\n",
      "actual :  sadness\n",
      "\n",
      "\n",
      "\n",
      "really offer sacrifice daily keep safe\n",
      "pred :  anger\n",
      "actual :  sadness\n",
      "\n",
      "\n",
      "\n",
      "would frown bit folding arms every time need assistance someone expects lil\n",
      "pred :  anger\n",
      "actual :  sadness\n",
      "\n",
      "\n",
      "\n",
      "feel like appendix purpose depressed alone lonely broken cry hu cry life\n",
      "pred :  fear\n",
      "actual :  sadness\n",
      "\n",
      "\n",
      "\n",
      "heard talk something miss look weary\n",
      "pred :  fear\n",
      "actual :  sadness\n",
      "\n",
      "\n",
      "\n",
      "frustration looking elphaba frown aggravation high pitched voice growing\n",
      "pred :  anger\n",
      "actual :  sadness\n",
      "\n",
      "\n",
      "\n",
      "hi dom saw notts county away looking mufc away ticket pay\n",
      "pred :  fear\n",
      "actual :  sadness\n",
      "\n",
      "\n",
      "\n",
      "folk band thistle replaced paul edward quaet laurel bank park sat pm due health jazz\n",
      "pred :  joy\n",
      "actual :  sadness\n",
      "\n",
      "\n",
      "\n",
      "mind know threaten job disagree put damper thing\n",
      "pred :  fear\n",
      "actual :  sadness\n",
      "\n",
      "\n",
      "\n",
      "thoroughly love zen jumin think would even patience either irl old weary\n",
      "pred :  anger\n",
      "actual :  sadness\n",
      "\n",
      "\n",
      "\n",
      "disheaened get card wanted end world e eh want cheer\n",
      "pred :  joy\n",
      "actual :  sadness\n",
      "\n",
      "\n",
      "\n",
      "yesterday finished watching penny dreadful beautiful thing saw one question remains writer hims fan\n",
      "pred :  fear\n",
      "actual :  sadness\n",
      "\n",
      "\n",
      "\n",
      "little hour away arrive soon fret\n",
      "pred :  anger\n",
      "actual :  sadness\n",
      "\n",
      "\n",
      "\n",
      "basically dead skin peel sound grim literally get rid much dead skin pore\n",
      "pred :  anger\n",
      "actual :  sadness\n",
      "\n",
      "\n",
      "\n",
      "aaahhhh little soothe soul music\n",
      "pred :  joy\n",
      "actual :  sadness\n",
      "\n",
      "\n",
      "\n",
      "invest new film stop asking invest new film concession crime despair shosightedness celebrity\n",
      "pred :  fear\n",
      "actual :  sadness\n",
      "\n",
      "\n",
      "\n",
      "watched django unchained people may frown titter delight\n",
      "pred :  joy\n",
      "actual :  sadness\n",
      "\n",
      "\n",
      "\n",
      "really jumanji rock jack black kevin ha kidding wtf thisisaterribleidea\n",
      "pred :  anger\n",
      "actual :  fear\n",
      "\n",
      "\n",
      "\n",
      "dreadful even met catfish still thought\n",
      "pred :  sadness\n",
      "actual :  fear\n",
      "\n",
      "\n",
      "\n",
      "easily forgive child afraid dark real tragedy life men afraid light plato\n",
      "pred :  sadness\n",
      "actual :  fear\n",
      "\n",
      "\n",
      "\n",
      "android app designed buggy work sporadically fire tv box\n",
      "pred :  anger\n",
      "actual :  fear\n",
      "\n",
      "\n",
      "\n",
      "step beating anxiety depression realising waiting take action\n",
      "pred :  sadness\n",
      "actual :  fear\n",
      "\n",
      "\n",
      "\n",
      "nothing worse uber driver cannot drive\n",
      "pred :  anger\n",
      "actual :  fear\n",
      "\n",
      "\n",
      "\n",
      "cannot wait hear say brilliant dr hawking rich poorest taste\n",
      "pred :  joy\n",
      "actual :  fear\n",
      "\n",
      "\n",
      "\n",
      "awareness seek shelter letgo old habit chasing desire resultin anger fear worry choose satisfaction within peace relax\n",
      "pred :  anger\n",
      "actual :  fear\n",
      "\n",
      "\n",
      "\n",
      "modern proverb let anyone intimidate single marriage end divorce\n",
      "pred :  sadness\n",
      "actual :  fear\n",
      "\n",
      "\n",
      "\n",
      "week till pick camera though think group cemetery shoot october make photography\n",
      "pred :  anger\n",
      "actual :  fear\n",
      "\n",
      "\n",
      "\n",
      "thanks ripping luthansa enough one way flight man frk bag free gate\n",
      "pred :  joy\n",
      "actual :  fear\n",
      "\n",
      "\n",
      "\n",
      "isnt obrien supposed offensive genius\n",
      "pred :  anger\n",
      "actual :  fear\n",
      "\n",
      "\n",
      "\n",
      "staff fr asked info told look online get pay ryanair compensation\n",
      "pred :  anger\n",
      "actual :  fear\n",
      "\n",
      "\n",
      "\n",
      "assure laughter increasing anger cost arrogance westminster\n",
      "pred :  anger\n",
      "actual :  joy\n",
      "\n",
      "\n",
      "\n",
      "somebody braved storm brewing\n",
      "pred :  anger\n",
      "actual :  joy\n",
      "\n",
      "\n",
      "\n",
      "mick nod would like went back food smiling finished\n",
      "pred :  fear\n",
      "actual :  joy\n",
      "\n",
      "\n",
      "\n",
      "spirit\n",
      "pred :  fear\n",
      "actual :  joy\n",
      "\n",
      "\n",
      "\n",
      "great mind think alike\n",
      "pred :  fear\n",
      "actual :  joy\n",
      "\n",
      "\n",
      "\n",
      "issue broadband bill charged month signed aiel\n",
      "pred :  fear\n",
      "actual :  joy\n",
      "\n",
      "\n",
      "\n",
      "tell body cheering death black people cop fear killing\n",
      "pred :  fear\n",
      "actual :  joy\n",
      "\n",
      "\n",
      "\n",
      "feel good get outside minute get fresh air hard stay cooped inside day\n",
      "pred :  fear\n",
      "actual :  joy\n",
      "\n",
      "\n",
      "\n",
      "ppl get triggered smiling irrelevant\n",
      "pred :  anger\n",
      "actual :  joy\n",
      "\n",
      "\n",
      "\n",
      "snapchat new would beg differ\n",
      "pred :  anger\n",
      "actual :  joy\n",
      "\n",
      "\n",
      "\n",
      "ball watching rojod header equally dreadful\n",
      "pred :  fear\n",
      "actual :  sadness\n",
      "\n",
      "\n",
      "\n",
      "public product high downheaed price tag consumer survey sum substance ovth\n",
      "pred :  fear\n",
      "actual :  sadness\n",
      "\n",
      "\n",
      "\n",
      "really offer sacrifice daily keep safe\n",
      "pred :  anger\n",
      "actual :  sadness\n",
      "\n",
      "\n",
      "\n",
      "would frown bit folding arms every time need assistance someone expects lil\n",
      "pred :  anger\n",
      "actual :  sadness\n",
      "\n",
      "\n",
      "\n",
      "feel like appendix purpose depressed alone lonely broken cry hu cry life\n",
      "pred :  fear\n",
      "actual :  sadness\n",
      "\n",
      "\n",
      "\n",
      "heard talk something miss look weary\n",
      "pred :  fear\n",
      "actual :  sadness\n",
      "\n",
      "\n",
      "\n",
      "frustration looking elphaba frown aggravation high pitched voice growing\n",
      "pred :  anger\n",
      "actual :  sadness\n",
      "\n",
      "\n",
      "\n",
      "hi dom saw notts county away looking mufc away ticket pay\n",
      "pred :  fear\n",
      "actual :  sadness\n",
      "\n",
      "\n",
      "\n",
      "folk band thistle replaced paul edward quaet laurel bank park sat pm due health jazz\n",
      "pred :  joy\n",
      "actual :  sadness\n",
      "\n",
      "\n",
      "\n",
      "mind know threaten job disagree put damper thing\n",
      "pred :  fear\n",
      "actual :  sadness\n",
      "\n",
      "\n",
      "\n",
      "thoroughly love zen jumin think would even patience either irl old weary\n",
      "pred :  anger\n",
      "actual :  sadness\n",
      "\n",
      "\n",
      "\n",
      "disheaened get card wanted end world e eh want cheer\n",
      "pred :  joy\n",
      "actual :  sadness\n",
      "\n",
      "\n",
      "\n",
      "yesterday finished watching penny dreadful beautiful thing saw one question remains writer hims fan\n",
      "pred :  fear\n",
      "actual :  sadness\n",
      "\n",
      "\n",
      "\n",
      "little hour away arrive soon fret\n",
      "pred :  anger\n",
      "actual :  sadness\n",
      "\n",
      "\n",
      "\n",
      "basically dead skin peel sound grim literally get rid much dead skin pore\n",
      "pred :  anger\n",
      "actual :  sadness\n",
      "\n",
      "\n",
      "\n",
      "aaahhhh little soothe soul music\n",
      "pred :  joy\n",
      "actual :  sadness\n",
      "\n",
      "\n",
      "\n",
      "invest new film stop asking invest new film concession crime despair shosightedness celebrity\n",
      "pred :  fear\n",
      "actual :  sadness\n",
      "\n",
      "\n",
      "\n",
      "watched django unchained people may frown titter delight\n",
      "pred :  joy\n",
      "actual :  sadness\n",
      "\n",
      "\n",
      "\n",
      "sometimes get mad something minuscule try ruin somebody life like lose job like get federal prison\n",
      "pred :  fear\n",
      "actual :  anger\n",
      "\n",
      "\n",
      "\n",
      "please canadian player play player lag atrocious fixthisgame trash sfvrefund\n",
      "pred :  fear\n",
      "actual :  anger\n",
      "\n",
      "\n",
      "\n",
      "sorry guy absolutely idea time cam tomorrow keep posted\n",
      "pred :  fear\n",
      "actual :  anger\n",
      "\n",
      "\n",
      "\n",
      "ding wearing look man found arch enemy bed missus angryman\n",
      "pred :  sadness\n",
      "actual :  anger\n",
      "\n",
      "\n",
      "\n",
      "ding wearing look man found arch enemy bed missus angryman scowl\n",
      "pred :  sadness\n",
      "actual :  anger\n",
      "\n",
      "\n",
      "\n",
      "oh brian invite\n",
      "pred :  fear\n",
      "actual :  anger\n",
      "\n",
      "\n",
      "\n",
      "take k number madden low dropped people unhappy\n",
      "pred :  sadness\n",
      "actual :  anger\n",
      "\n",
      "\n",
      "\n",
      "opinion worst delhi govt acrid hypocrisy\n",
      "pred :  fear\n",
      "actual :  anger\n",
      "\n",
      "\n",
      "\n",
      "thirsty chance disagree left even realize something affront bigoted platform\n",
      "pred :  fear\n",
      "actual :  anger\n",
      "\n",
      "\n",
      "\n",
      "baby born soon lifechanging year feel like yesterday sad happy emotionalrollercoaster\n",
      "pred :  sadness\n",
      "actual :  anger\n",
      "\n",
      "\n",
      "\n",
      "coincidentally watched ulzanas raid last night brutally indignant filmmaking\n",
      "pred :  fear\n",
      "actual :  anger\n",
      "\n",
      "\n",
      "\n",
      "watched django unchained people may frown titter delight\n",
      "pred :  joy\n",
      "actual :  anger\n",
      "\n",
      "\n",
      "\n",
      "106\n"
     ]
    }
   ],
   "source": [
    "num_wrong = 0\n",
    "for i in range(len(X_test)):\n",
    "    if (Y_pred[i]!=test_labels[i]):\n",
    "        print(X_test[i])\n",
    "        print('pred : ',Y_pred[i])\n",
    "        print('actual : ',test_labels[i])\n",
    "        print('\\n\\n')\n",
    "        num_wrong+=1\n",
    "print(num_wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "cb3b1c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_all.save_model('fasttext_all.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd980a4f",
   "metadata": {},
   "source": [
    "## One vs All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "a3d4a4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_all(model,X):\n",
    "    Y_pred = model.predict(df_dev['text'].tolist())\n",
    "    Y_preds = [x[0].split('_')[-1] for x in Y_pred[0]]\n",
    "    Y_pred_scores = [x[0] for x in Y_pred[1]]\n",
    "    return Y_preds, Y_pred_scores\n",
    "\n",
    "def predict(model_sadness,model_joy,model_anger,model_fear,model_all,X):\n",
    "    # predict sadness\n",
    "    sadness_preds , sadness_scores = predict_all(model_sadness,X)\n",
    "    \n",
    "    # predict joy\n",
    "    joy_preds , joy_scores = predict_all(model_joy,X)\n",
    "    \n",
    "    # predict anger\n",
    "    anger_preds , anger_scores = predict_all(model_anger,X)\n",
    "    \n",
    "    # predict fear\n",
    "    fear_preds , fear_scores = predict_all(model_fear,X)\n",
    "    \n",
    "    # predict all\n",
    "    all_preds , all_scores = predict_all(model_all,X)\n",
    "    \n",
    "    final_preds = []\n",
    "    num_not = 0\n",
    "    for i in range(len(X)):\n",
    "        predictions = []\n",
    "        predictions_scores = []\n",
    "        # check sadness\n",
    "        if not ('not' in sadness_preds[i]):\n",
    "            predictions.append(sadness_preds[i])\n",
    "            predictions_scores.append(sadness_scores[i])\n",
    "            \n",
    "        # check joy\n",
    "        if not ('not' in joy_preds[i]):\n",
    "            predictions.append(joy_preds[i])\n",
    "            predictions_scores.append(joy_scores[i])\n",
    "            \n",
    "        # check anger\n",
    "        if not ('not' in anger_preds[i]):\n",
    "            predictions.append(anger_preds[i])\n",
    "            predictions_scores.append(anger_scores[i])\n",
    "            \n",
    "        # check fear\n",
    "        if not ('not' in fear_preds[i]):\n",
    "            predictions.append(fear_preds[i])\n",
    "            predictions_scores.append(fear_scores[i])\n",
    "            \n",
    "        if len(predictions) == 0:\n",
    "            num_not+=1\n",
    "            final_preds.append(all_preds[i])\n",
    "#             prediction_scores = [sadness_scores[i],joy_scores[i],anger_scores[i],fear_scores[i]]\n",
    "#             predictions = ['sadness','joy','anger','fear']\n",
    "#             scores = [1 - t for t in prediction_scores]\n",
    "#             max_index = scores.index(max(scores))\n",
    "#             final_preds.append(predictions[max_index])\n",
    "        elif len(predictions) == 1:\n",
    "            final_preds.append(predictions[0])\n",
    "        else:\n",
    "            # choose the one with the biggest score\n",
    "            max_index = predictions_scores.index(max(predictions_scores))\n",
    "            final_preds.append(predictions[max_index])\n",
    "    print(num_not)\n",
    "    return final_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "3ca72321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.84      0.86      0.85       168\n",
      "        fear       0.85      0.85      0.85       220\n",
      "         joy       0.85      0.87      0.86       158\n",
      "     sadness       0.84      0.80      0.82       148\n",
      "\n",
      "    accuracy                           0.85       694\n",
      "   macro avg       0.85      0.85      0.85       694\n",
      "weighted avg       0.85      0.85      0.85       694\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_test = df_dev['text'].tolist()\n",
    "Y_pred = predict(model_sadness,model_joy,model_anger,model_fear,model_all,X_test)\n",
    "print(classification_report(test_labels,Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed3d83b",
   "metadata": {},
   "source": [
    "## One vs All Experiment with Sadness + joy & anger + fear groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "0d314b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(model,X):\n",
    "    \n",
    "    preds = model.predict(X,k=2)\n",
    "    scores_labels = preds[0]\n",
    "    scores = preds[1]\n",
    "    \n",
    "    out = []\n",
    "    for i in range(len(X)):\n",
    "        index = 0\n",
    "        if 'not' in scores_labels[i][0]:\n",
    "            index = 1\n",
    "        out.append(scores[i][index])\n",
    "            \n",
    "    return np.array(out)\n",
    "\n",
    "def predict_sum(model_sadness,model_joy,model_anger,model_fear,X):\n",
    "    \n",
    "    # predict sadness\n",
    "    sadness_scores = get_scores(model_sadness,X)\n",
    "    \n",
    "    # predict joy\n",
    "    joy_scores = get_scores(model_joy,X)\n",
    "\n",
    "    # predict anger\n",
    "    anger_scores = get_scores(model_anger,X)\n",
    "\n",
    "    # predict fear\n",
    "    fear_scores = get_scores(model_fear,X)\n",
    "    \n",
    "    # add groups\n",
    "    sad_joy_scores = sadness_scores + joy_scores\n",
    "    anger_fear_scores = anger_scores + fear_scores\n",
    "    \n",
    "    final_preds = []\n",
    "    for i in range(len(X)):\n",
    "        if sad_joy_scores[i] >= anger_fear_scores[i]:\n",
    "            if sadness_scores[i] >= joy_scores[i]:\n",
    "                final_preds.append('sadness')\n",
    "            else:\n",
    "                final_preds.append('joy')\n",
    "        else:\n",
    "            if anger_scores[i] >= fear_scores[i]:\n",
    "                final_preds.append('anger')\n",
    "            else:\n",
    "                final_preds.append('fear')\n",
    "                \n",
    "    return final_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "36512ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.82      0.88      0.85       168\n",
      "        fear       0.87      0.84      0.85       220\n",
      "         joy       0.87      0.87      0.87       158\n",
      "     sadness       0.81      0.78      0.79       148\n",
      "\n",
      "    accuracy                           0.84       694\n",
      "   macro avg       0.84      0.84      0.84       694\n",
      "weighted avg       0.84      0.84      0.84       694\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_test = df_dev['text'].tolist()\n",
    "Y_pred = predict_sum(model_sadness,model_joy,model_anger,model_fear,X_test)\n",
    "print(classification_report(test_labels,Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1fe67a",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8b4753",
   "metadata": {},
   "source": [
    "### Multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "52a31cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.88      0.85      0.86       168\n",
      "        fear       0.80      0.94      0.87       220\n",
      "         joy       0.96      0.86      0.91       158\n",
      "     sadness       0.88      0.80      0.84       148\n",
      "\n",
      "    accuracy                           0.87       694\n",
      "   macro avg       0.88      0.86      0.87       694\n",
      "weighted avg       0.87      0.87      0.87       694\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_features = 3000\n",
    "model_tfidf = TfidfVectorizer(max_features=num_features)\n",
    "model_tfidf.fit(df_train['text'])\n",
    "\n",
    "X_train = model_tfidf.transform(df_train['text']).toarray()\n",
    "X_test = model_tfidf.transform(df_dev['text']).toarray()\n",
    "\n",
    "Y_train = df_train['label_numeric']\n",
    "Y_test = df_dev['label_numeric']\n",
    "\n",
    "model_t = LogisticRegression(max_iter=200)\n",
    "model_t.fit(X_train,Y_train)\n",
    "Y_pred = model_t.predict(X_test)\n",
    "print(classification_report(Y_test,Y_pred,target_names = class_mapping))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb7c93e",
   "metadata": {},
   "source": [
    "### Individual Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "391848fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.92      0.82      0.87       168\n",
      "   not-anger       0.94      0.98      0.96       526\n",
      "\n",
      "    accuracy                           0.94       694\n",
      "   macro avg       0.93      0.90      0.91       694\n",
      "weighted avg       0.94      0.94      0.94       694\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fear       0.93      0.67      0.78       220\n",
      "    not-fear       0.87      0.97      0.92       474\n",
      "\n",
      "    accuracy                           0.88       694\n",
      "   macro avg       0.90      0.82      0.85       694\n",
      "weighted avg       0.88      0.88      0.87       694\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         joy       0.92      0.75      0.83       158\n",
      "     not-joy       0.93      0.98      0.95       536\n",
      "\n",
      "    accuracy                           0.93       694\n",
      "   macro avg       0.93      0.86      0.89       694\n",
      "weighted avg       0.93      0.93      0.93       694\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     sadness       0.84      0.73      0.78       148\n",
      " not-sadness       0.93      0.96      0.95       546\n",
      "\n",
      "    accuracy                           0.91       694\n",
      "   macro avg       0.89      0.85      0.86       694\n",
      "weighted avg       0.91      0.91      0.91       694\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for lbl in [0,1,2,3]:\n",
    "    \n",
    "    df_train_0 = df_train[df_train['label_numeric'] == lbl]\n",
    "    df_train_rest = df_train[df_train['label_numeric'] != lbl]\n",
    "    df_train_rest = df_train_rest.sample(int(len(df_train_0)*))\n",
    "                                         \n",
    "\n",
    "    df_train_sampled = df_train_0.append(df_train_rest)\n",
    "    df_train_sampled = df_train_sampled.sample(frac=1).reset_index()\n",
    "\n",
    "    Y_train = df_train_sampled['label_numeric'].apply(lambda x: 0 if x == lbl else 1 ).tolist()\n",
    "    Y_test = df_dev['label_numeric'].apply(lambda x: 0 if x == lbl else 1 ).tolist()\n",
    "\n",
    "    num_features = 3000\n",
    "    model_tfidf = TfidfVectorizer(max_features=num_features)\n",
    "    model_tfidf.fit(df_train['text'])\n",
    "\n",
    "    X_train = model_tfidf.transform(df_train_sampled['text']).toarray()\n",
    "    X_test = model_tfidf.transform(df_dev['text']).toarray()\n",
    "    \n",
    "\n",
    "    model_t = LogisticRegression()\n",
    "    model_t.fit(X_train,Y_train)\n",
    "    Y_pred = model_t.predict(X_test)\n",
    "    print(classification_report(Y_test,Y_pred,target_names = [class_mapping[lbl],'not-'+class_mapping[lbl]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0470400",
   "metadata": {},
   "source": [
    "### One vs All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "416a016d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.85      0.86       168\n",
      "           1       0.79      0.94      0.86       220\n",
      "           2       0.96      0.86      0.91       158\n",
      "           3       0.88      0.76      0.81       148\n",
      "\n",
      "    accuracy                           0.86       694\n",
      "   macro avg       0.87      0.85      0.86       694\n",
      "weighted avg       0.87      0.86      0.86       694\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_features = 3000\n",
    "model_tfidf = TfidfVectorizer(max_features=num_features,ngram_range=(1,1))\n",
    "model_tfidf.fit(df_train['text'])\n",
    "\n",
    "X_train = model_tfidf.transform(df_train['text']).toarray()\n",
    "X_test = model_tfidf.transform(df_dev['text']).toarray()\n",
    "\n",
    "Y_train = df_train['label_numeric'].tolist()\n",
    "Y_test = df_dev['label_numeric'].tolist()\n",
    "\n",
    "LR = OneVsRestClassifier(LogisticRegression(max_iter=200))\n",
    "LR.fit(X_train, Y_train)\n",
    "Y_pred = LR.predict(X_test)\n",
    "print(classification_report(Y_test,Y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
