{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS640_project_baseline_models.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXETdGUWcF6R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed7a4505-2950-4650-f058-91a06df3c1eb"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQkfKsoZrdit",
        "outputId": "265496f8-001d-4cc9-9715-133b346a2f0e"
      },
      "source": [
        "!pip install preprocessor\n",
        "!pip install tweet-preprocessor\n",
        "!pip install beautifulsoup4\n",
        "!pip install fasttext\n",
        "!pip install emoji\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting preprocessor\n",
            "  Downloading preprocessor-1.1.3.tar.gz (4.2 kB)\n",
            "Building wheels for collected packages: preprocessor\n",
            "  Building wheel for preprocessor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for preprocessor: filename=preprocessor-1.1.3-py3-none-any.whl size=4477 sha256=8ef891838e347d220b9d16a82357892875a0036c6d726f7c7de72adc86d20b70\n",
            "  Stored in directory: /root/.cache/pip/wheels/0e/b7/36/aa37256db62b4bfd35a6f1b5536e9ba843f257b79dcbf3d5f1\n",
            "Successfully built preprocessor\n",
            "Installing collected packages: preprocessor\n",
            "Successfully installed preprocessor-1.1.3\n",
            "Collecting tweet-preprocessor\n",
            "  Downloading tweet_preprocessor-0.6.0-py3-none-any.whl (27 kB)\n",
            "Installing collected packages: tweet-preprocessor\n",
            "Successfully installed tweet-preprocessor-0.6.0\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (4.6.3)\n",
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[K     |████████████████████████████████| 68 kB 5.7 MB/s \n",
            "\u001b[?25hCollecting pybind11>=2.2\n",
            "  Using cached pybind11-2.8.1-py2.py3-none-any.whl (208 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext) (57.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fasttext) (1.19.5)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp37-cp37m-linux_x86_64.whl size=3126223 sha256=39e5aaf7e5ff27e7b498bd14a511ce0efa9fc30651ca4690263bd07bcbc8a8f3\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/ca/bf/b020d2be95f7641801a6597a29c8f4f19e38f9c02a345bab9b\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.8.1\n",
            "Collecting emoji\n",
            "  Downloading emoji-1.6.1.tar.gz (170 kB)\n",
            "\u001b[K     |████████████████████████████████| 170 kB 13.1 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.6.1-py3-none-any.whl size=169314 sha256=236678e5e6bdbbf9991f77dfef06aefd457e3d2caaaa353cc07ec2f35e54fca7\n",
            "  Stored in directory: /root/.cache/pip/wheels/ea/5f/d3/03d313ddb3c2a1a427bb4690f1621eea60fe6f2a30cc95940f\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-1.6.1\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZPyZu9zcODP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7d38587-239e-412b-95fa-46dadbc56bbd"
      },
      "source": [
        "import os\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import preprocessor\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import fasttext\n",
        "import csv\n",
        "import datetime\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import itertools\n",
        "import emoji\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vk8YMKVTcSvq",
        "outputId": "9fd41cd1-5422-4e64-9018-00e16fede9cb"
      },
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():       \n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
        "    print('Device name:', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "Device name: Tesla P100-PCIE-16GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEnPKZ3XcXTF",
        "outputId": "2322b5cf-92a5-4801-ab86-e3e730d7f626"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRSeYgKQcrkE",
        "outputId": "b6d30ee9-d3dd-4f92-e8b0-46a8320499a0"
      },
      "source": [
        "import os\n",
        "import json\n",
        "import argparse\n",
        "import importlib\n",
        "from typing import Dict, List, Optional, Tuple, Union\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import metrics\n",
        "\n",
        "def getData(type):\n",
        "\n",
        "    folder_path = \"/content/drive/MyDrive/CS640-project/DataTwitter/\"+ type + \"/\"\n",
        "    dfs = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith('txt'):\n",
        "            path = os.path.join(folder_path, filename)\n",
        "            df = pd.read_csv(path, sep=\"\\\\t\", header=None)\n",
        "\n",
        "            df.rename(columns={1: \"tweet\", 2: \"sentiment\"}, inplace=True)\n",
        "            df = df.drop(columns=[df.columns[0], df.columns[3]])\n",
        "            df['sentiment'] = df['sentiment'].replace(['anger'],0)\n",
        "            df['sentiment'] = df['sentiment'].replace(['fear'],1)\n",
        "            df['sentiment'] = df['sentiment'].replace(['joy'],2)\n",
        "            df['sentiment'] = df['sentiment'].replace(['sadness'],3)\n",
        "\n",
        "            dfs.append(df)\n",
        "\n",
        "\n",
        "    df = pd.concat(dfs)\n",
        "    # df.columns = ['text', 'label']\n",
        "    # df['label_numeric'] = df['label'].astype('category').cat.codes\n",
        "    print('total ' + type +' samples : ', len(df))\n",
        "    print(df['sentiment'].value_counts())\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train = getData(\"Train\")\n",
        "dev = getData(\"Dev\")\n",
        "test = getData(\"Test\")\n",
        "# train = pd.concat([train, test])\n",
        "# train = train.sample(frac=0.01, replace=True, random_state=1)\n",
        "# # test = test.sample(frac=0.1, replace=True, random_state=1)\n",
        "X_train = train.filter(regex='tweet').tweet.values\n",
        "y_train = train.filter(regex='sentiment').sentiment.values\n",
        "X_dev = dev.filter(regex='tweet').tweet.values\n",
        "y_dev = dev.filter(regex='sentiment').sentiment.values\n",
        "X_test = test.filter(regex='tweet').tweet.values\n",
        "y_test = test.filter(regex='sentiment').sentiment.values\n",
        "\n",
        "\n",
        "\n",
        "class_mapping = ['anger', 'fear', 'joy', 'sadness']\n",
        "print(class_mapping)\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:18: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total Train samples :  3613\n",
            "1    1147\n",
            "0     857\n",
            "2     823\n",
            "3     786\n",
            "Name: sentiment, dtype: int64\n",
            "total Dev samples :  347\n",
            "1    110\n",
            "0     84\n",
            "2     79\n",
            "3     74\n",
            "Name: sentiment, dtype: int64\n",
            "total Test samples :  3142\n",
            "1    995\n",
            "0    760\n",
            "2    714\n",
            "3    673\n",
            "Name: sentiment, dtype: int64\n",
            "['anger', 'fear', 'joy', 'sadness']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVbzt4w3iH1x",
        "outputId": "35d33fc2-c3ae-4b91-c3cd-845ad3699412"
      },
      "source": [
        "import os\n",
        "import json\n",
        "import argparse\n",
        "import importlib\n",
        "from typing import Dict, List, Optional, Tuple, Union\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import metrics\n",
        "\n",
        "def getData():\n",
        "\n",
        "    folder_path = \"/content/drive/MyDrive/CS640-project/DataInstagram/\"\n",
        "    dfs = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith('csv'):\n",
        "            path = os.path.join(folder_path, filename)\n",
        "            df = pd.read_csv(path)\n",
        "\n",
        "            df.rename(columns={'imagename': \"image_id\", \"Contents\": \"tweet\", 'Q5_presence_asian': \"isEastAsian\"}, inplace=True)\n",
        "            df = df.drop(columns=[df.columns[1], df.columns[2], df.columns[3], df.columns[4], df.columns[6], df.columns[7], df.columns[8], df.columns[9],\n",
        "                                  df.columns[10], df.columns[11], df.columns[13], df.columns[14], df.columns[15], df.columns[16], df.columns[17],\n",
        "                                  df.columns[18], df.columns[19], df.columns[20], df.columns[21], df.columns[22], df.columns[23], df.columns[24], df.columns[25]])\n",
        "    \n",
        "            # df['sentiment'] = df['sentiment'].replace(['anger'],0)\n",
        "            # df['sentiment'] = df['sentiment'].replace(['fear'],1)\n",
        "            # df['sentiment'] = df['sentiment'].replace(['joy'],2)\n",
        "            # df['sentiment'] = df['sentiment'].replace(['sadness'],3)\n",
        "\n",
        "            dfs.append(df)\n",
        "\n",
        "\n",
        "    df = pd.concat(dfs)\n",
        "    # df.columns = ['text', 'label']\n",
        "    # df['label_numeric'] = df['label'].astype('category').cat.codes\n",
        "    print('total ' + \"insta\" +' samples : ', len(df))\n",
        "    # print(df['isEastAsian'].value_counts())\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Insta_data = getData()\n",
        "Insta_data = Insta_data.dropna(thresh=2)\n",
        "X_insta = Insta_data.tweet.values\n",
        "\n",
        "# print(Insta_train)\n",
        "print(Insta_data)\n",
        "\n",
        "# train = pd.concat([train, test])\n",
        "# train = train.sample(frac=0.01, replace=True, random_state=1)\n",
        "# # test = test.sample(frac=0.1, replace=True, random_state=1)\n",
        "# X_insta = train.filter(regex='tweet').tweet.values\n",
        "# y_train = train.filter(regex='sentiment').sentiment.values\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total insta samples :  9648\n",
            "         image_id  ...                                    translated_text\n",
            "0     B-A5FIIIEKJ  ...  #covid #covid2020 #covidvirus #virus #coronava...\n",
            "1     B-A74YQn_4a  ...  Well this is the final mural of my trip in Aus...\n",
            "2     B-AajnDp6GQ  ...  We have arrived!!! Keep following the movement...\n",
            "3     B-AarR4pUQA  ...                                              😻😻😻😻😻\n",
            "4     B-AE1y3HD-Z  ...  AT MY HOME 🏡_x000D__x000D__x000D__x000D_\\n# st...\n",
            "...           ...  ...                                                ...\n",
            "9643  CEZyheBgtMd  ...  #water #foryou #followforfollowback #photograp...\n",
            "9644  CEZYoyRluKG  ...  #like4likes #20likes #tagforlikes #instalikes ...\n",
            "9645  CEZz-solaUF  ...  🙈🥰😍👉🏽 @love_serie_karma #daancorona #daancoron...\n",
            "9646  CEZZ4P4j3rh  ...  💥🚨 𝗙𝗔𝗟𝗟 𝗜𝗦 𝗖𝗢𝗠𝗜𝗡𝗚 🚨💥⁣_x000D__x000D__x000D__x00...\n",
            "9647  CEZZyqshFLq  ...  Follow me @kestine_kylie _x000D__x000D__x000D_...\n",
            "\n",
            "[9648 rows x 5 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsC-g8QAcw8v"
      },
      "source": [
        "# def text_preprocessing(text):  \n",
        "#     text = re.sub(r'(@.*?)[\\s]', '', text)\n",
        "#     text = re.sub(r'&amp;', '&', text)\n",
        "#     text = re.sub(r\"<[\\w'/'\\s]*>\",'' ,text)\n",
        "#     text = re.sub(r\"[\\'\\\"\\-]+\", '',text)\n",
        "#     text = re.sub(r\"@[\\w]+\", '',text)\n",
        "#     text = re.sub(r'http\\S+', '',text)\n",
        "#     text = re.sub(r'\\s+', '', text).strip()\n",
        "#     return text\n",
        "preprocessor.set_options(preprocessor.OPT.URL,preprocessor.OPT.RESERVED)\n",
        "stop_words = nltk.corpus.stopwords.words('english')\n",
        "def text_preprocessing(text):   \n",
        "    text = preprocessor.tokenize(text)\n",
        "    text = ' '.join([word for word in text.split(' ') if word.lower() not in stop_words])\n",
        "    return text"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGzaMsZvdQx6"
      },
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def load_dict_smileys():\n",
        "    \n",
        "    return {\n",
        "        \":‑)\":\"smiley\",\n",
        "        \":-]\":\"smiley\",\n",
        "        \":-3\":\"smiley\",\n",
        "        \":->\":\"smiley\",\n",
        "        \"8-)\":\"smiley\",\n",
        "        \":-}\":\"smiley\",\n",
        "        \":)\":\"smiley\",\n",
        "        \":]\":\"smiley\",\n",
        "        \":3\":\"smiley\",\n",
        "        \":>\":\"smiley\",\n",
        "        \"8)\":\"smiley\",\n",
        "        \":}\":\"smiley\",\n",
        "        \":o)\":\"smiley\",\n",
        "        \":c)\":\"smiley\",\n",
        "        \":^)\":\"smiley\",\n",
        "        \"=]\":\"smiley\",\n",
        "        \"=)\":\"smiley\",\n",
        "        \":-))\":\"smiley\",\n",
        "        \":‑D\":\"smiley\",\n",
        "        \"8‑D\":\"smiley\",\n",
        "        \"x‑D\":\"smiley\",\n",
        "        \"X‑D\":\"smiley\",\n",
        "        \":D\":\"smiley\",\n",
        "        \"8D\":\"smiley\",\n",
        "        \"xD\":\"smiley\",\n",
        "        \"XD\":\"smiley\",\n",
        "        \":‑(\":\"sad\",\n",
        "        \":‑c\":\"sad\",\n",
        "        \":‑<\":\"sad\",\n",
        "        \":‑[\":\"sad\",\n",
        "        \":(\":\"sad\",\n",
        "        \":c\":\"sad\",\n",
        "        \":<\":\"sad\",\n",
        "        \":[\":\"sad\",\n",
        "        \":-||\":\"sad\",\n",
        "        \">:[\":\"sad\",\n",
        "        \":{\":\"sad\",\n",
        "        \":@\":\"sad\",\n",
        "        \">:(\":\"sad\",\n",
        "        \":'‑(\":\"sad\",\n",
        "        \":'(\":\"sad\",\n",
        "        \":‑P\":\"playful\",\n",
        "        \"X‑P\":\"playful\",\n",
        "        \"x‑p\":\"playful\",\n",
        "        \":‑p\":\"playful\",\n",
        "        \":‑Þ\":\"playful\",\n",
        "        \":‑þ\":\"playful\",\n",
        "        \":‑b\":\"playful\",\n",
        "        \":P\":\"playful\",\n",
        "        \"XP\":\"playful\",\n",
        "        \"xp\":\"playful\",\n",
        "        \":p\":\"playful\",\n",
        "        \":Þ\":\"playful\",\n",
        "        \":þ\":\"playful\",\n",
        "        \":b\":\"playful\",\n",
        "        \"<3\":\"love\"\n",
        "        }\n",
        "\n",
        "\n",
        "def load_dict_contractions_slangs():\n",
        "    \n",
        "    cont = {\n",
        "        \"ain't\":\"is not\",\n",
        "        \"amn't\":\"am not\",\n",
        "        \"aren't\":\"are not\",\n",
        "        \"can't\":\"cannot\",\n",
        "        \"'cause\":\"because\",\n",
        "        \"couldn't\":\"could not\",\n",
        "        \"couldn't've\":\"could not have\",\n",
        "        \"could've\":\"could have\",\n",
        "        \"daren't\":\"dare not\",\n",
        "        \"daresn't\":\"dare not\",\n",
        "        \"dasn't\":\"dare not\",\n",
        "        \"didn't\":\"did not\",\n",
        "        \"doesn't\":\"does not\",\n",
        "        \"don't\":\"do not\",\n",
        "        \"e'er\":\"ever\",\n",
        "        \"em\":\"them\",\n",
        "        \"everyone's\":\"everyone is\",\n",
        "        \"finna\":\"fixing to\",\n",
        "        \"gimme\":\"give me\",\n",
        "        \"gonna\":\"going to\",\n",
        "        \"gon't\":\"go not\",\n",
        "        \"gotta\":\"got to\",\n",
        "        \"hadn't\":\"had not\",\n",
        "        \"hasn't\":\"has not\",\n",
        "        \"haven't\":\"have not\",\n",
        "        \"he'd\":\"he would\",\n",
        "        \"he'll\":\"he will\",\n",
        "        \"he's\":\"he is\",\n",
        "        \"he've\":\"he have\",\n",
        "        \"how'd\":\"how would\",\n",
        "        \"how'll\":\"how will\",\n",
        "        \"how're\":\"how are\",\n",
        "        \"how's\":\"how is\",\n",
        "        \"i'd\":\"i would\",\n",
        "        \"i'll\":\"i will\",\n",
        "        \"i'm\":\"i am\",\n",
        "        \"i'm'a\":\"i am about to\",\n",
        "        \"i'm'o\":\"i am going to\",\n",
        "        \"isn't\":\"is not\",\n",
        "        \"it'd\":\"it would\",\n",
        "        \"it'll\":\"it will\",\n",
        "        \"it's\":\"it is\",\n",
        "        \"i've\":\"i have\",\n",
        "        \"kinda\":\"kind of\",\n",
        "        \"let's\":\"let us\",\n",
        "        \"mayn't\":\"may not\",\n",
        "        \"may've\":\"may have\",\n",
        "        \"mightn't\":\"might not\",\n",
        "        \"might've\":\"might have\",\n",
        "        \"mustn't\":\"must not\",\n",
        "        \"mustn't've\":\"must not have\",\n",
        "        \"must've\":\"must have\",\n",
        "        \"needn't\":\"need not\",\n",
        "        \"ne'er\":\"never\",\n",
        "        \"o'\":\"of\",\n",
        "        \"o'er\":\"over\",\n",
        "        \"ol'\":\"old\",\n",
        "        \"oughtn't\":\"ought not\",\n",
        "        \"shalln't\":\"shall not\",\n",
        "        \"shan't\":\"shall not\",\n",
        "        \"she'd\":\"she would\",\n",
        "        \"she'll\":\"she will\",\n",
        "        \"she's\":\"she is\",\n",
        "        \"shouldn't\":\"should not\",\n",
        "        \"shouldn't've\":\"should not have\",\n",
        "        \"should've\":\"should have\",\n",
        "        \"somebody's\":\"somebody is\",\n",
        "        \"someone's\":\"someone is\",\n",
        "        \"something's\":\"something is\",\n",
        "        \"that'd\":\"that would\",\n",
        "        \"that'll\":\"that will\",\n",
        "        \"that're\":\"that are\",\n",
        "        \"that's\":\"that is\",\n",
        "        \"there'd\":\"there would\",\n",
        "        \"there'll\":\"there will\",\n",
        "        \"there're\":\"there are\",\n",
        "        \"there's\":\"there is\",\n",
        "        \"these're\":\"these are\",\n",
        "        \"they'd\":\"they would\",\n",
        "        \"they'll\":\"they will\",\n",
        "        \"they're\":\"they are\",\n",
        "        \"they've\":\"they have\",\n",
        "        \"this's\":\"this is\",\n",
        "        \"those're\":\"those are\",\n",
        "        \"'tis\":\"it is\",\n",
        "        \"'twas\":\"it was\",\n",
        "        \"wanna\":\"want to\",\n",
        "        \"wasn't\":\"was not\",\n",
        "        \"we'd\":\"we would\",\n",
        "        \"we'd've\":\"we would have\",\n",
        "        \"we'll\":\"we will\",\n",
        "        \"we're\":\"we are\",\n",
        "        \"weren't\":\"were not\",\n",
        "        \"we've\":\"we have\",\n",
        "        \"what'd\":\"what did\",\n",
        "        \"what'll\":\"what will\",\n",
        "        \"what're\":\"what are\",\n",
        "        \"what's\":\"what is\",\n",
        "        \"what've\":\"what have\",\n",
        "        \"when's\":\"when is\",\n",
        "        \"where'd\":\"where did\",\n",
        "        \"where're\":\"where are\",\n",
        "        \"where's\":\"where is\",\n",
        "        \"where've\":\"where have\",\n",
        "        \"which's\":\"which is\",\n",
        "        \"who'd\":\"who would\",\n",
        "        \"who'd've\":\"who would have\",\n",
        "        \"who'll\":\"who will\",\n",
        "        \"who're\":\"who are\",\n",
        "        \"who's\":\"who is\",\n",
        "        \"who've\":\"who have\",\n",
        "        \"why'd\":\"why did\",\n",
        "        \"why're\":\"why are\",\n",
        "        \"why's\":\"why is\",\n",
        "        \"won't\":\"will not\",\n",
        "        \"wouldn't\":\"would not\",\n",
        "        \"would've\":\"would have\",\n",
        "        \"y'all\":\"you all\",\n",
        "        \"you'd\":\"you would\",\n",
        "        \"you'll\":\"you will\",\n",
        "        \"you're\":\"you are\",\n",
        "        \"you've\":\"you have\",\n",
        "        \"Whatcha\":\"What are you\",\n",
        "        \"luv\":\"love\",\n",
        "        \"sux\":\"sucks\",\n",
        "        \"rn\":\"right now\",\n",
        "        \"atm\": \"at the moment\",\n",
        "        \"idk\": \"i dont know\",\n",
        "        \"cuz\": \"because\",\n",
        "        \"bcuz\": \"because\",\n",
        "        \"ur\": \"you are\",\n",
        "        \"ly\": \"love you\",\n",
        "        \"lol\": \"laugh out loud\",\n",
        "        \"rofl\": \"rolling on floor laughing\",\n",
        "        \"lmao\": \"laughing my ass off\",\n",
        "        \"ok\": \"okay\",\n",
        "        \"ty\": \"thank you\",\n",
        "        \"fav\": \"favorite\",\n",
        "        \"omg\": \"oh my god\"\n",
        "        }\n",
        "    \n",
        "    to_ret = {}\n",
        "    for k,v in cont.items():\n",
        "        key = k.lower().replace(\"'\",'')\n",
        "        to_ret[key] = v.lower()\n",
        "        \n",
        "    return to_ret\n",
        "\n",
        "\n",
        "def strip_accents(text):\n",
        "    if 'ø' in text or  'Ø' in text:\n",
        "        #Do nothing when finding ø \n",
        "        return text   \n",
        "    text = text.encode('ascii', 'ignore')\n",
        "    text = text.decode(\"utf-8\")\n",
        "    return str(text)\n",
        "\n",
        "\n",
        "def tweet_cleaning_for_sentiment_analysis(tweet):    \n",
        "    \n",
        "    #Escaping HTML characters\n",
        "    tweet = BeautifulSoup(tweet).get_text()\n",
        "    #Special case not handled previously.\n",
        "    tweet = tweet.replace('\\x92',\"'\")\n",
        "    #Removal of account\n",
        "    tweet = ' '.join(re.sub(\"(@[A-Za-z0-9_]+)\", \" \", tweet).split())\n",
        "    # removal of hashtag\n",
        "    tweet = tweet.replace('#','')\n",
        "    #Removal of address\n",
        "    tweet = ' '.join(re.sub(\"(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
        "    #Lower case\n",
        "    tweet = tweet.lower()\n",
        "    # rempval of 'RT'\n",
        "    tweet = tweet.replace('rt','')\n",
        "    # Removal of Punctuation\n",
        "    tweet = ' '.join(re.sub(\"[\\.\\,\\!\\?\\:\\;\\-\\=]\", \" \", tweet).split())\n",
        "    # removal of stop words\n",
        "    tweet = ' '.join([word for word in tweet.split(' ') if word not in stop_words])\n",
        "    # CONTRACTIONS source: https://en.wikipedia.org/wiki/Contraction_%28grammar%29\n",
        "    CONTRACTIONS_SLANGS = load_dict_contractions_slangs()\n",
        "    tweet = tweet.replace(\"’\",\"'\").replace(\"'\",\"\")\n",
        "    words = tweet.split()\n",
        "    reformed = [CONTRACTIONS_SLANGS[word] if word in CONTRACTIONS_SLANGS else word for word in words]\n",
        "    tweet = \" \".join(reformed)\n",
        "\n",
        "    # Standardizing words - lemmatization\n",
        "    tweet = ' '.join([lemmatizer.lemmatize(word) for word in tweet.split(' ')]).lower()\n",
        "    \n",
        "    #Deal with smileys\n",
        "    #source: https://en.wikipedia.org/wiki/List_of_emoticons\n",
        "    SMILEY = load_dict_smileys()  \n",
        "    words = tweet.split()\n",
        "    reformed = [SMILEY[word] if word in SMILEY else word for word in words]\n",
        "    tweet = \" \".join(reformed)\n",
        "    # replace emojis with desctiption - remove accents - remove underscores\n",
        "    tweet = emoji.demojize(tweet).lower()\n",
        "    tweet = strip_accents(tweet).lower()\n",
        "    tweet = tweet.replace(\":\",\" \")\n",
        "    tweet = ' '.join(tweet.split()).replace('_',' ')\n",
        "    \n",
        "    # remove new line characters\n",
        "    tweet = tweet.strip().replace('\\\\n',' ')\n",
        "    \n",
        "    # only keep alphabets\n",
        "    tweet = ''.join([c for c in tweet if (c.isalpha() or c ==' ')]).strip()\n",
        "    \n",
        "    # rempove double spaces and triple spaces\n",
        "    tweet = tweet.replace('   ',' ').replace('  ',' ')\n",
        "    \n",
        "    # one word replacements\n",
        "    replacement_dict = {\n",
        "        'u': 'you',\n",
        "        'v': 'we',\n",
        "        'r': 'are',\n",
        "        'w': 'we',\n",
        "        'n': 'and',\n",
        "        'nd': 'and',\n",
        "        '&': 'and'\n",
        "    }\n",
        "    \n",
        "    tweet = ' '.join([replacement_dict[word] if word in replacement_dict else word for word in tweet.split(' ')])\n",
        "    \n",
        "    # removal of stop words\n",
        "    tweet = ' '.join([word for word in tweet.split(' ') if word not in stop_words])\n",
        "\n",
        "    return tweet\n",
        "    "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ihe7Ni8sc7_s"
      },
      "source": [
        "def preprocessAllData(X):\n",
        "  count = []\n",
        "  res = []\n",
        "  for data in X:\n",
        "    if data != \"\":\n",
        "      try:\n",
        "        res.append(tweet_cleaning_for_sentiment_analysis(data))\n",
        "        count.append(len(res))\n",
        "      except:\n",
        "        #nan values are filtered here \n",
        "        pass\n",
        "  print(count)\n",
        "  return np.array(res)\n",
        "\n",
        "# X_train = tweet_cleaning_for_sentiment_analysis(X_train)\n",
        "# X_dev = preprocessAllData(X_dev)\n",
        "# X_test = preprocessAllData(X_test)\n",
        "# X_insta = preprocessAllData(Insta_train)\n",
        "\n",
        "train['tweet'] = train['tweet'].apply(tweet_cleaning_for_sentiment_analysis)\n",
        "dev['tweet'] = dev['tweet'].apply(tweet_cleaning_for_sentiment_analysis)\n",
        "test['tweet'] = test['tweet'].apply(tweet_cleaning_for_sentiment_analysis)\n",
        "X_train = train.filter(regex='tweet').tweet.values\n",
        "y_train = train.filter(regex='sentiment').sentiment.values\n",
        "X_dev = dev.filter(regex='tweet').tweet.values\n",
        "y_dev = dev.filter(regex='sentiment').sentiment.values\n",
        "X_test = test.filter(regex='tweet').tweet.values\n",
        "y_test = test.filter(regex='sentiment').sentiment.values\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vi8hLvNi8VRb"
      },
      "source": [
        "Naive Bayes "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjQRe9WpdDc4"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from gensim.models import Word2Vec\n",
        "num_features = 3000\n",
        "#count vectorizer \n",
        "cv = CountVectorizer(max_features=num_features)\n",
        "cv.fit_transform(X_train)\n",
        "X_train_cv = cv.transform(X_train)\n",
        "X_dev_cv = cv.transform(X_dev)\n",
        "X_test_cv = cv.transform(X_test)\n",
        "\n",
        "#tfidf vectorizer \n",
        "\n",
        "tfidf = TfidfVectorizer(max_features=num_features)\n",
        "tfidf.fit_transform(X_train)\n",
        "X_train_tfidf = tfidf.transform(X_train)\n",
        "X_dev_tfidf = tfidf.transform(X_dev)\n",
        "X_test_tfidf = tfidf.transform(X_test)\n",
        "\n",
        "\n",
        "#word2vec\n",
        "\n",
        "# word2vecModel = Word2Vec(X_train, min_count=1,size= 50,workers=3, window =3, sg = 1)\n",
        "# X_train_word2vec = []\n",
        "# for i in X_train:\n",
        "#   X_train_word2vec.append(word2vecModel[i])\n",
        "# print(X_train_word2vec)\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eNCH4mfGZW3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbPD00cIfD0D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31158bec-cf9c-4e26-9cab-b9552a095ef2"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report\n",
        "print(\"Naive Bayes:\")\n",
        "model_cv = MultinomialNB()\n",
        "model_cv.fit(X_train_cv, y_train)\n",
        "cv_nb_score = model_cv.score(X_dev_cv, y_dev)\n",
        "Y_pred = model_cv.predict(X_dev_cv)\n",
        "print(classification_report(y_dev,Y_pred, target_names = class_mapping))\n",
        "print(\"count vectorizer score: \", cv_nb_score)\n",
        "print(\"instagram data results: \")\n",
        "InstaPredictions = []\n",
        "\n",
        "naiveBayesInstaPredictions = []\n",
        "Insta_ea = Insta_data.isEastAsian.values\n",
        "eastAsian = []\n",
        "index = 0\n",
        "for row in X_insta:\n",
        "  try: \n",
        "    emotion_pred = class_mapping[model_cv.predict(cv.transform([row]))[0]]\n",
        "    naiveBayesInstaPredictions.append(emotion_pred)\n",
        "    eastAsian.append(Insta_ea[index])\n",
        "  except:\n",
        "    pass\n",
        "  index += 1\n",
        "print(pd.DataFrame(naiveBayesInstaPredictions).value_counts())\n",
        "with open('Insta_predictions_cv_nb.csv', 'w') as csvfile:\n",
        "  writer=csv.writer(csvfile, delimiter=',')\n",
        "  writer.writerows(zip(X_insta, naiveBayesInstaPredictions))\n",
        "# print(pd.DataFrame(eastAsian).value_counts())\n",
        "eastAsianFear = [0, 0]\n",
        "eastAsianAnger = [0, 0]\n",
        "\n",
        "for person in range(len(eastAsian)):\n",
        "  if eastAsian[person] == 1:\n",
        "    if naiveBayesInstaPredictions[person] == 'fear':\n",
        "      eastAsianFear[0] += 1\n",
        "    if naiveBayesInstaPredictions[person] == 'anger':\n",
        "      eastAsianAnger[0] += 1\n",
        "    else:\n",
        "      eastAsianFear[1] += 1\n",
        "      eastAsianAnger[1] += 1\n",
        "# print(eastAsianFear, eastAsianAnger)\n",
        "from scipy.stats import chi2_contingency \n",
        "# using Pearson’s chi-squared statistic\n",
        "# corrected for the Yates’ continuity\n",
        "observed = np.array([eastAsianFear, eastAsianAnger])\n",
        "chi_val, p_val, dof, expected =  chi2_contingency(observed)\n",
        "print(\"correlation vals\")\n",
        "print(\"Pearson chi square value: \", chi_val, \"p-value:\" , p_val)\n",
        "\n",
        "model_tfidf = MultinomialNB()\n",
        "model_tfidf.fit(X_train_tfidf, y_train)\n",
        "tfidf_nb_score = model_tfidf.score(X_dev_tfidf, y_dev)\n",
        "Y_pred = model_tfidf.predict(X_dev_cv)\n",
        "print(classification_report(y_dev,Y_pred, target_names = class_mapping))\n",
        "print(\"tfidf vectorizer score: \", tfidf_nb_score)\n",
        "print(\"instagram data results: \")\n",
        "naiveBayesInstaPredictions = []\n",
        "Insta_ea = Insta_data.isEastAsian.values\n",
        "eastAsian = []\n",
        "index = 0\n",
        "for row in X_insta:\n",
        "  try: \n",
        "    emotion_pred = class_mapping[model_tfidf.predict(tfidf.transform([row]))[0]]\n",
        "    naiveBayesInstaPredictions.append(emotion_pred)\n",
        "    eastAsian.append(Insta_ea[index])\n",
        "  except:\n",
        "    pass\n",
        "  index += 1\n",
        "print(pd.DataFrame(naiveBayesInstaPredictions).value_counts())\n",
        "\n",
        "with open('Insta_predictions_tfidf_nb.csv', 'w') as csvfile:\n",
        "  writer=csv.writer(csvfile, delimiter=',')\n",
        "  writer.writerows(zip(X_insta, naiveBayesInstaPredictions))\n",
        "\n",
        "eastAsianFear = [0, 0]\n",
        "eastAsianAnger = [0, 0]\n",
        "for person in range(len(eastAsian)):\n",
        "  if eastAsian[person] == 1:\n",
        "    if naiveBayesInstaPredictions[person] == 'fear':\n",
        "      eastAsianFear[0] += 1\n",
        "    if naiveBayesInstaPredictions[person] == 'anger':\n",
        "      eastAsianAnger[0] += 1\n",
        "    else:\n",
        "      eastAsianFear[1] += 1\n",
        "      eastAsianAnger[1] += 1\n",
        "# print(eastAsianFear, eastAsianAnger)\n",
        "observed = np.array([eastAsianFear, eastAsianAnger])\n",
        "chi_val, p_val, dof, expected =  chi2_contingency(observed)\n",
        "print(\"correlation vals\")\n",
        "print(\"Pearson chi square value: \", chi_val, \"p-value:\" , p_val)\n"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.78      0.75      0.76        84\n",
            "        fear       0.81      0.72      0.76       110\n",
            "         joy       0.77      0.82      0.80        79\n",
            "     sadness       0.75      0.85      0.80        74\n",
            "\n",
            "    accuracy                           0.78       347\n",
            "   macro avg       0.78      0.79      0.78       347\n",
            "weighted avg       0.78      0.78      0.78       347\n",
            "\n",
            "count vectorizer score:  0.7780979827089337\n",
            "instagram data results: \n",
            "fear       4058\n",
            "joy        2111\n",
            "anger      1712\n",
            "sadness    1708\n",
            "dtype: int64\n",
            "correlation vals\n",
            "Pearson chi square value:  50.98779180434007 p-value: 9.29422273346263e-13\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.83      0.77      0.80        84\n",
            "        fear       0.77      0.78      0.77       110\n",
            "         joy       0.77      0.80      0.78        79\n",
            "     sadness       0.80      0.81      0.81        74\n",
            "\n",
            "    accuracy                           0.79       347\n",
            "   macro avg       0.79      0.79      0.79       347\n",
            "weighted avg       0.79      0.79      0.79       347\n",
            "\n",
            "tfidf vectorizer score:  0.8097982708933718\n",
            "instagram data results: \n",
            "fear       5481\n",
            "joy        1715\n",
            "anger      1294\n",
            "sadness    1099\n",
            "dtype: int64\n",
            "correlation vals\n",
            "Pearson chi square value:  142.9801173745654 p-value: 5.937151822668577e-33\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9LW8WSmZ3E8",
        "outputId": "d498f8dc-4a60-4690-fcf1-0a33761bc584"
      },
      "source": [
        ""
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9648 9589\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltaMRLECeWkV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43fbb16d-7d7b-4d6f-c229-9252feceb07e"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "print(\"Logistic Regression:\")\n",
        "model_cv = LogisticRegression()\n",
        "model_cv.fit(X_train_cv, y_train)\n",
        "cv_nb_score = model_cv.score(X_dev_cv, y_dev)\n",
        "Y_pred = model_cv.predict(X_dev_cv)\n",
        "print(classification_report(y_dev,Y_pred, target_names = class_mapping))\n",
        "print(\"count vectorizer score: \", cv_nb_score)\n",
        "print(\"instagram data results: \")\n",
        "InstaPredictions = []\n",
        "\n",
        "naiveBayesInstaPredictions = []\n",
        "Insta_ea = Insta_data.isEastAsian.values\n",
        "eastAsian = []\n",
        "index = 0\n",
        "for row in X_insta:\n",
        "  try: \n",
        "    emotion_pred = class_mapping[model_cv.predict(cv.transform([row]))[0]]\n",
        "    naiveBayesInstaPredictions.append(emotion_pred)\n",
        "    eastAsian.append(Insta_ea[index])\n",
        "  except:\n",
        "    pass\n",
        "  index += 1\n",
        "print(pd.DataFrame(naiveBayesInstaPredictions).value_counts())\n",
        "with open('Insta_predictions_cv_logistic_regression.csv', 'w') as csvfile:\n",
        "  writer=csv.writer(csvfile, delimiter=',')\n",
        "  writer.writerows(zip(X_insta, naiveBayesInstaPredictions))\n",
        "# print(pd.DataFrame(eastAsian).value_counts())\n",
        "eastAsianFear = [0, 0]\n",
        "eastAsianAnger = [0, 0]\n",
        "\n",
        "for person in range(len(eastAsian)):\n",
        "  if eastAsian[person] == 1:\n",
        "    if naiveBayesInstaPredictions[person] == 'fear':\n",
        "      eastAsianFear[0] += 1\n",
        "    if naiveBayesInstaPredictions[person] == 'anger':\n",
        "      eastAsianAnger[0] += 1\n",
        "    else:\n",
        "      eastAsianFear[1] += 1\n",
        "      eastAsianAnger[1] += 1\n",
        "# print(eastAsianFear, eastAsianAnger)\n",
        "from scipy.stats import chi2_contingency \n",
        "# using Pearson’s chi-squared statistic\n",
        "# corrected for the Yates’ continuity\n",
        "observed = np.array([eastAsianFear, eastAsianAnger])\n",
        "chi_val, p_val, dof, expected =  chi2_contingency(observed)\n",
        "print(\"correlation vals\")\n",
        "print(\"Pearson chi square value: \", chi_val, \"p-value:\" , p_val)\n",
        "\n",
        "model_tfidf = LogisticRegression()\n",
        "model_tfidf.fit(X_train_tfidf, y_train)\n",
        "tfidf_nb_score = model_tfidf.score(X_dev_tfidf, y_dev)\n",
        "Y_pred = model_tfidf.predict(X_dev_cv)\n",
        "print(classification_report(y_dev,Y_pred, target_names = class_mapping))\n",
        "print(\"tfidf vectorizer score: \", tfidf_nb_score)\n",
        "print(\"instagram data results: \")\n",
        "naiveBayesInstaPredictions = []\n",
        "Insta_ea = Insta_data.isEastAsian.values\n",
        "eastAsian = []\n",
        "index = 0\n",
        "for row in X_insta:\n",
        "  try: \n",
        "    emotion_pred = class_mapping[model_tfidf.predict(tfidf.transform([row]))[0]]\n",
        "    naiveBayesInstaPredictions.append(emotion_pred)\n",
        "    eastAsian.append(Insta_ea[index])\n",
        "  except:\n",
        "    pass\n",
        "  index += 1\n",
        "print(pd.DataFrame(naiveBayesInstaPredictions).value_counts())\n",
        "\n",
        "with open('Insta_predictions_tfidf_logistic_regression.csv', 'w') as csvfile:\n",
        "  writer=csv.writer(csvfile, delimiter=',')\n",
        "  writer.writerows(zip(X_insta, naiveBayesInstaPredictions))\n",
        "\n",
        "eastAsianFear = [0, 0]\n",
        "eastAsianAnger = [0, 0]\n",
        "for person in range(len(eastAsian)):\n",
        "  if eastAsian[person] == 1:\n",
        "    if naiveBayesInstaPredictions[person] == 'fear':\n",
        "      eastAsianFear[0] += 1\n",
        "    if naiveBayesInstaPredictions[person] == 'anger':\n",
        "      eastAsianAnger[0] += 1\n",
        "    else:\n",
        "      eastAsianFear[1] += 1\n",
        "      eastAsianAnger[1] += 1\n",
        "# print(eastAsianFear, eastAsianAnger)\n",
        "observed = np.array([eastAsianFear, eastAsianAnger])\n",
        "chi_val, p_val, dof, expected =  chi2_contingency(observed)\n",
        "print(\"correlation vals\")\n",
        "print(\"Pearson chi square value: \", chi_val, \"p-value:\" , p_val)\n"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.84      0.83      0.84        84\n",
            "        fear       0.81      0.85      0.83       110\n",
            "         joy       0.87      0.85      0.86        79\n",
            "     sadness       0.83      0.81      0.82        74\n",
            "\n",
            "    accuracy                           0.84       347\n",
            "   macro avg       0.84      0.83      0.84       347\n",
            "weighted avg       0.84      0.84      0.84       347\n",
            "\n",
            "count vectorizer score:  0.8357348703170029\n",
            "instagram data results: \n",
            "fear       5479\n",
            "anger      1663\n",
            "joy        1578\n",
            "sadness     869\n",
            "dtype: int64\n",
            "correlation vals\n",
            "Pearson chi square value:  77.66328091123472 p-value: 1.2219192901169517e-18\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.86      0.80      0.83        84\n",
            "        fear       0.82      0.80      0.81       110\n",
            "         joy       0.84      0.84      0.84        79\n",
            "     sadness       0.80      0.89      0.84        74\n",
            "\n",
            "    accuracy                           0.83       347\n",
            "   macro avg       0.83      0.83      0.83       347\n",
            "weighted avg       0.83      0.83      0.83       347\n",
            "\n",
            "tfidf vectorizer score:  0.8386167146974063\n",
            "instagram data results: \n",
            "fear       5989\n",
            "joy        1462\n",
            "anger      1350\n",
            "sadness     788\n",
            "dtype: int64\n",
            "correlation vals\n",
            "Pearson chi square value:  124.80952434492461 p-value: 5.602215036566652e-29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l99bwIuTffwh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ba94e84-9000-4a01-e4f0-586438997876"
      },
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import classification_report\n",
        "print(\"Linear SVC:\")\n",
        "model_cv = LinearSVC()\n",
        "model_cv.fit(X_train_cv, y_train)\n",
        "cv_nb_score = model_cv.score(X_dev_cv, y_dev)\n",
        "Y_pred = model_cv.predict(X_dev_cv)\n",
        "print(classification_report(y_dev,Y_pred, target_names = class_mapping))\n",
        "print(\"count vectorizer score: \", cv_nb_score)\n",
        "print(\"instagram data results: \")\n",
        "InstaPredictions = []\n",
        "\n",
        "naiveBayesInstaPredictions = []\n",
        "Insta_ea = Insta_data.isEastAsian.values\n",
        "eastAsian = []\n",
        "index = 0\n",
        "for row in X_insta:\n",
        "  try: \n",
        "    emotion_pred = class_mapping[model_cv.predict(cv.transform([row]))[0]]\n",
        "    naiveBayesInstaPredictions.append(emotion_pred)\n",
        "    eastAsian.append(Insta_ea[index])\n",
        "  except:\n",
        "    pass\n",
        "  index += 1\n",
        "print(pd.DataFrame(naiveBayesInstaPredictions).value_counts())\n",
        "with open('Insta_predictions_cv_LinearSVC.csv', 'w') as csvfile:\n",
        "  writer=csv.writer(csvfile, delimiter=',')\n",
        "  writer.writerows(zip(X_insta, naiveBayesInstaPredictions))\n",
        "# print(pd.DataFrame(eastAsian).value_counts())\n",
        "eastAsianFear = [0, 0]\n",
        "eastAsianAnger = [0, 0]\n",
        "\n",
        "for person in range(len(eastAsian)):\n",
        "  if eastAsian[person] == 1:\n",
        "    if naiveBayesInstaPredictions[person] == 'fear':\n",
        "      eastAsianFear[0] += 1\n",
        "    if naiveBayesInstaPredictions[person] == 'anger':\n",
        "      eastAsianAnger[0] += 1\n",
        "    else:\n",
        "      eastAsianFear[1] += 1\n",
        "      eastAsianAnger[1] += 1\n",
        "# print(eastAsianFear, eastAsianAnger)\n",
        "from scipy.stats import chi2_contingency \n",
        "# using Pearson’s chi-squared statistic\n",
        "# corrected for the Yates’ continuity\n",
        "observed = np.array([eastAsianFear, eastAsianAnger])\n",
        "chi_val, p_val, dof, expected =  chi2_contingency(observed)\n",
        "print(\"correlation vals\")\n",
        "print(\"Pearson chi square value: \", chi_val, \"p-value:\" , p_val)\n",
        "\n",
        "model_tfidf = LinearSVC()\n",
        "model_tfidf.fit(X_train_tfidf, y_train)\n",
        "tfidf_nb_score = model_tfidf.score(X_dev_tfidf, y_dev)\n",
        "Y_pred = model_tfidf.predict(X_dev_cv)\n",
        "print(classification_report(y_dev,Y_pred, target_names = class_mapping))\n",
        "print(\"tfidf vectorizer score: \", tfidf_nb_score)\n",
        "print(\"instagram data results: \")\n",
        "naiveBayesInstaPredictions = []\n",
        "Insta_ea = Insta_data.isEastAsian.values\n",
        "eastAsian = []\n",
        "index = 0\n",
        "for row in X_insta:\n",
        "  try: \n",
        "    emotion_pred = class_mapping[model_tfidf.predict(tfidf.transform([row]))[0]]\n",
        "    naiveBayesInstaPredictions.append(emotion_pred)\n",
        "    eastAsian.append(Insta_ea[index])\n",
        "  except:\n",
        "    pass\n",
        "  index += 1\n",
        "print(pd.DataFrame(naiveBayesInstaPredictions).value_counts())\n",
        "\n",
        "with open('Insta_predictions_tfidf_linearSVC.csv', 'w') as csvfile:\n",
        "  writer=csv.writer(csvfile, delimiter=',')\n",
        "  writer.writerows(zip(X_insta, naiveBayesInstaPredictions))\n",
        "\n",
        "eastAsianFear = [0, 0]\n",
        "eastAsianAnger = [0, 0]\n",
        "for person in range(len(eastAsian)):\n",
        "  if eastAsian[person] == 1:\n",
        "    if naiveBayesInstaPredictions[person] == 'fear':\n",
        "      eastAsianFear[0] += 1\n",
        "    if naiveBayesInstaPredictions[person] == 'anger':\n",
        "      eastAsianAnger[0] += 1\n",
        "    else:\n",
        "      eastAsianFear[1] += 1\n",
        "      eastAsianAnger[1] += 1\n",
        "# print(eastAsianFear, eastAsianAnger)\n",
        "observed = np.array([eastAsianFear, eastAsianAnger])\n",
        "chi_val, p_val, dof, expected =  chi2_contingency(observed)\n",
        "print(\"correlation vals\")\n",
        "print(\"Pearson chi square value: \", chi_val, \"p-value:\" , p_val)\n"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear SVC:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.85      0.80      0.82        84\n",
            "        fear       0.79      0.85      0.82       110\n",
            "         joy       0.84      0.81      0.83        79\n",
            "     sadness       0.81      0.80      0.80        74\n",
            "\n",
            "    accuracy                           0.82       347\n",
            "   macro avg       0.82      0.81      0.82       347\n",
            "weighted avg       0.82      0.82      0.82       347\n",
            "\n",
            "count vectorizer score:  0.8184438040345822\n",
            "instagram data results: \n",
            "fear       5222\n",
            "joy        1665\n",
            "anger      1663\n",
            "sadness    1039\n",
            "dtype: int64\n",
            "correlation vals\n",
            "Pearson chi square value:  72.40528852928797 p-value: 1.7524369800453984e-17\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.88      0.81      0.84        84\n",
            "        fear       0.84      0.79      0.82       110\n",
            "         joy       0.82      0.86      0.84        79\n",
            "     sadness       0.76      0.86      0.81        74\n",
            "\n",
            "    accuracy                           0.83       347\n",
            "   macro avg       0.83      0.83      0.83       347\n",
            "weighted avg       0.83      0.83      0.83       347\n",
            "\n",
            "tfidf vectorizer score:  0.8530259365994236\n",
            "instagram data results: \n",
            "fear       4659\n",
            "anger      1927\n",
            "joy        1765\n",
            "sadness    1238\n",
            "dtype: int64\n",
            "correlation vals\n",
            "Pearson chi square value:  17.37819738829533 p-value: 3.0631969090193514e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ir1OCRgkobnf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c5ffac5-4146-45af-a5dc-ca94230f8b05"
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "print(\"MLP Classifier:\")\n",
        "model_cv = MLPClassifier()\n",
        "model_cv.fit(X_train_cv, y_train)\n",
        "cv_nb_score = model_cv.score(X_dev_cv, y_dev)\n",
        "Y_pred = model_cv.predict(X_dev_cv)\n",
        "print(classification_report(y_dev,Y_pred, target_names = class_mapping))\n",
        "print(\"count vectorizer score: \", cv_nb_score)\n",
        "print(\"instagram data results: \")\n",
        "InstaPredictions = []\n",
        "\n",
        "naiveBayesInstaPredictions = []\n",
        "Insta_ea = Insta_data.isEastAsian.values\n",
        "eastAsian = []\n",
        "index = 0\n",
        "for row in X_insta:\n",
        "  try: \n",
        "    emotion_pred = class_mapping[model_cv.predict(cv.transform([row]))[0]]\n",
        "    naiveBayesInstaPredictions.append(emotion_pred)\n",
        "    eastAsian.append(Insta_ea[index])\n",
        "  except:\n",
        "    pass\n",
        "  index += 1\n",
        "print(pd.DataFrame(naiveBayesInstaPredictions).value_counts())\n",
        "with open('Insta_predictions_cv_MLP_classifier.csv', 'w') as csvfile:\n",
        "  writer=csv.writer(csvfile, delimiter=',')\n",
        "  writer.writerows(zip(X_insta, naiveBayesInstaPredictions))\n",
        "# print(pd.DataFrame(eastAsian).value_counts())\n",
        "eastAsianFear = [0, 0]\n",
        "eastAsianAnger = [0, 0]\n",
        "\n",
        "for person in range(len(eastAsian)):\n",
        "  if eastAsian[person] == 1:\n",
        "    if naiveBayesInstaPredictions[person] == 'fear':\n",
        "      eastAsianFear[0] += 1\n",
        "    if naiveBayesInstaPredictions[person] == 'anger':\n",
        "      eastAsianAnger[0] += 1\n",
        "    else:\n",
        "      eastAsianFear[1] += 1\n",
        "      eastAsianAnger[1] += 1\n",
        "# print(eastAsianFear, eastAsianAnger)\n",
        "from scipy.stats import chi2_contingency \n",
        "# using Pearson’s chi-squared statistic\n",
        "# corrected for the Yates’ continuity\n",
        "observed = np.array([eastAsianFear, eastAsianAnger])\n",
        "chi_val, p_val, dof, expected =  chi2_contingency(observed)\n",
        "print(\"correlation vals\")\n",
        "print(\"Pearson chi square value: \", chi_val, \"p-value:\" , p_val)\n",
        "\n",
        "model_tfidf = MLPClassifier()\n",
        "model_tfidf.fit(X_train_tfidf, y_train)\n",
        "tfidf_nb_score = model_tfidf.score(X_dev_tfidf, y_dev)\n",
        "Y_pred = model_tfidf.predict(X_dev_cv)\n",
        "print(classification_report(y_dev,Y_pred, target_names = class_mapping))\n",
        "print(\"tfidf vectorizer score: \", tfidf_nb_score)\n",
        "print(\"instagram data results: \")\n",
        "naiveBayesInstaPredictions = []\n",
        "Insta_ea = Insta_data.isEastAsian.values\n",
        "eastAsian = []\n",
        "index = 0\n",
        "for row in X_insta:\n",
        "  try: \n",
        "    emotion_pred = class_mapping[model_tfidf.predict(tfidf.transform([row]))[0]]\n",
        "    naiveBayesInstaPredictions.append(emotion_pred)\n",
        "    eastAsian.append(Insta_ea[index])\n",
        "  except:\n",
        "    pass\n",
        "  index += 1\n",
        "print(pd.DataFrame(naiveBayesInstaPredictions).value_counts())\n",
        "\n",
        "with open('Insta_predictions_tfidf_MLP_classifier.csv', 'w') as csvfile:\n",
        "  writer=csv.writer(csvfile, delimiter=',')\n",
        "  writer.writerows(zip(X_insta, naiveBayesInstaPredictions))\n",
        "\n",
        "eastAsianFear = [0, 0]\n",
        "eastAsianAnger = [0, 0]\n",
        "for person in range(len(eastAsian)):\n",
        "  if eastAsian[person] == 1:\n",
        "    if naiveBayesInstaPredictions[person] == 'fear':\n",
        "      eastAsianFear[0] += 1\n",
        "    if naiveBayesInstaPredictions[person] == 'anger':\n",
        "      eastAsianAnger[0] += 1\n",
        "    else:\n",
        "      eastAsianFear[1] += 1\n",
        "      eastAsianAnger[1] += 1\n",
        "# print(eastAsianFear, eastAsianAnger)\n",
        "observed = np.array([eastAsianFear, eastAsianAnger])\n",
        "chi_val, p_val, dof, expected =  chi2_contingency(observed)\n",
        "print(\"correlation vals\")\n",
        "print(\"Pearson chi square value: \", chi_val, \"p-value:\" , p_val)\n"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP Classifier:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.84      0.82      0.83        84\n",
            "        fear       0.82      0.83      0.82       110\n",
            "         joy       0.82      0.86      0.84        79\n",
            "     sadness       0.79      0.76      0.77        74\n",
            "\n",
            "    accuracy                           0.82       347\n",
            "   macro avg       0.82      0.82      0.82       347\n",
            "weighted avg       0.82      0.82      0.82       347\n",
            "\n",
            "count vectorizer score:  0.8184438040345822\n",
            "instagram data results: \n",
            "fear       4564\n",
            "anger      1882\n",
            "joy        1650\n",
            "sadness    1493\n",
            "dtype: int64\n",
            "correlation vals\n",
            "Pearson chi square value:  20.896849124797836 p-value: 4.846912120902017e-06\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.84      0.81      0.82        84\n",
            "        fear       0.84      0.77      0.81       110\n",
            "         joy       0.79      0.89      0.83        79\n",
            "     sadness       0.76      0.78      0.77        74\n",
            "\n",
            "    accuracy                           0.81       347\n",
            "   macro avg       0.81      0.81      0.81       347\n",
            "weighted avg       0.81      0.81      0.81       347\n",
            "\n",
            "tfidf vectorizer score:  0.7953890489913544\n",
            "instagram data results: \n",
            "fear       4251\n",
            "anger      1928\n",
            "sadness    1795\n",
            "joy        1615\n",
            "dtype: int64\n",
            "correlation vals\n",
            "Pearson chi square value:  11.188067124672362 p-value: 0.0008232505878525054\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBnmxeDGsVSX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a652f1e9-da85-4ff7-e498-d42b8f7eda2f"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "print(\"Random Forest Classifier:\")\n",
        "model_cv = RandomForestClassifier()\n",
        "model_cv.fit(X_train_cv, y_train)\n",
        "cv_nb_score = model_cv.score(X_dev_cv, y_dev)\n",
        "Y_pred = model_cv.predict(X_dev_cv)\n",
        "print(classification_report(y_dev,Y_pred, target_names = class_mapping))\n",
        "print(\"count vectorizer score: \", cv_nb_score)\n",
        "print(\"instagram data results: \")\n",
        "InstaPredictions = []\n",
        "\n",
        "naiveBayesInstaPredictions = []\n",
        "Insta_ea = Insta_data.isEastAsian.values\n",
        "eastAsian = []\n",
        "index = 0\n",
        "for row in X_insta:\n",
        "  try: \n",
        "    emotion_pred = class_mapping[model_cv.predict(cv.transform([row]))[0]]\n",
        "    naiveBayesInstaPredictions.append(emotion_pred)\n",
        "    eastAsian.append(Insta_ea[index])\n",
        "  except:\n",
        "    pass\n",
        "  index += 1\n",
        "print(pd.DataFrame(naiveBayesInstaPredictions).value_counts())\n",
        "with open('Insta_predictions_cv_RandomForest.csv', 'w') as csvfile:\n",
        "  writer=csv.writer(csvfile, delimiter=',')\n",
        "  writer.writerows(zip(X_insta, naiveBayesInstaPredictions))\n",
        "# print(pd.DataFrame(eastAsian).value_counts())\n",
        "eastAsianFear = [0, 0]\n",
        "eastAsianAnger = [0, 0]\n",
        "\n",
        "for person in range(len(eastAsian)):\n",
        "  if eastAsian[person] == 1:\n",
        "    if naiveBayesInstaPredictions[person] == 'fear':\n",
        "      eastAsianFear[0] += 1\n",
        "    if naiveBayesInstaPredictions[person] == 'anger':\n",
        "      eastAsianAnger[0] += 1\n",
        "    else:\n",
        "      eastAsianFear[1] += 1\n",
        "      eastAsianAnger[1] += 1\n",
        "# print(eastAsianFear, eastAsianAnger)\n",
        "from scipy.stats import chi2_contingency \n",
        "# using Pearson’s chi-squared statistic\n",
        "# corrected for the Yates’ continuity\n",
        "observed = np.array([eastAsianFear, eastAsianAnger])\n",
        "chi_val, p_val, dof, expected =  chi2_contingency(observed)\n",
        "print(\"correlation vals\")\n",
        "print(\"Pearson chi square value: \", chi_val, \"p-value:\" , p_val)\n",
        "\n",
        "model_tfidf = RandomForestClassifier()\n",
        "model_tfidf.fit(X_train_tfidf, y_train)\n",
        "tfidf_nb_score = model_tfidf.score(X_dev_tfidf, y_dev)\n",
        "Y_pred = model_tfidf.predict(X_dev_cv)\n",
        "print(classification_report(y_dev,Y_pred, target_names = class_mapping))\n",
        "print(\"tfidf vectorizer score: \", tfidf_nb_score)\n",
        "print(\"instagram data results: \")\n",
        "naiveBayesInstaPredictions = []\n",
        "Insta_ea = Insta_data.isEastAsian.values\n",
        "eastAsian = []\n",
        "index = 0\n",
        "for row in X_insta:\n",
        "  try: \n",
        "    emotion_pred = class_mapping[model_tfidf.predict(tfidf.transform([row]))[0]]\n",
        "    naiveBayesInstaPredictions.append(emotion_pred)\n",
        "    eastAsian.append(Insta_ea[index])\n",
        "  except:\n",
        "    pass\n",
        "  index += 1\n",
        "print(pd.DataFrame(naiveBayesInstaPredictions).value_counts())\n",
        "\n",
        "with open('Insta_predictions_tfidf_RandomForest.csv', 'w') as csvfile:\n",
        "  writer=csv.writer(csvfile, delimiter=',')\n",
        "  writer.writerows(zip(X_insta, naiveBayesInstaPredictions))\n",
        "\n",
        "eastAsianFear = [0, 0]\n",
        "eastAsianAnger = [0, 0]\n",
        "for person in range(len(eastAsian)):\n",
        "  if eastAsian[person] == 1:\n",
        "    if naiveBayesInstaPredictions[person] == 'fear':\n",
        "      eastAsianFear[0] += 1\n",
        "    if naiveBayesInstaPredictions[person] == 'anger':\n",
        "      eastAsianAnger[0] += 1\n",
        "    else:\n",
        "      eastAsianFear[1] += 1\n",
        "      eastAsianAnger[1] += 1\n",
        "# print(eastAsianFear, eastAsianAnger)\n",
        "observed = np.array([eastAsianFear, eastAsianAnger])\n",
        "chi_val, p_val, dof, expected =  chi2_contingency(observed)\n",
        "print(\"correlation vals\")\n",
        "print(\"Pearson chi square value: \", chi_val, \"p-value:\" , p_val)\n"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Classifier:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.91      0.87      0.89        84\n",
            "        fear       0.88      0.84      0.86       110\n",
            "         joy       0.87      0.86      0.87        79\n",
            "     sadness       0.71      0.81      0.76        74\n",
            "\n",
            "    accuracy                           0.84       347\n",
            "   macro avg       0.84      0.84      0.84       347\n",
            "weighted avg       0.85      0.84      0.85       347\n",
            "\n",
            "count vectorizer score:  0.8443804034582133\n",
            "instagram data results: \n",
            "sadness    5386\n",
            "fear       2077\n",
            "anger      1122\n",
            "joy        1004\n",
            "dtype: int64\n",
            "correlation vals\n",
            "Pearson chi square value:  17.52638454458093 p-value: 2.8334819510252926e-05\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.84      0.85      0.84        84\n",
            "        fear       0.90      0.82      0.86       110\n",
            "         joy       0.87      0.86      0.87        79\n",
            "     sadness       0.74      0.84      0.78        74\n",
            "\n",
            "    accuracy                           0.84       347\n",
            "   macro avg       0.84      0.84      0.84       347\n",
            "weighted avg       0.84      0.84      0.84       347\n",
            "\n",
            "tfidf vectorizer score:  0.8501440922190202\n",
            "instagram data results: \n",
            "sadness    5567\n",
            "fear       2415\n",
            "anger       818\n",
            "joy         789\n",
            "dtype: int64\n",
            "correlation vals\n",
            "Pearson chi square value:  71.98349732726525 p-value: 2.1700460227619168e-17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woB6eaeDZxWE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTF9kJYRZnrT",
        "outputId": "5b16e4cb-2b4d-42ca-ff4e-6fdf7058ff8e"
      },
      "source": [
        "import numpy as np\n",
        "from scipy.stats import chi2_contingency \n",
        "# using Pearson’s chi-squared statistic\n",
        "# corrected for the Yates’ continuity\n",
        "observed = np.array([[60, 10], [30, 25]])\n",
        "chi_val, p_val, dof, expected =  chi2_contingency(observed)\n",
        "chi_val, p_val, dof, expected"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13.3364898989899, 0.0002602911116400899, 1, array([[50.4, 19.6],\n",
              "        [39.6, 15.4]]))"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZRCq9GMGrpE",
        "outputId": "5ac910b4-344f-4cdd-e700-556cc57df0e0"
      },
      "source": [
        "for lbl in [0,1,2,3]:\n",
        "    \n",
        "    train_0 = train[train['sentiment'] == lbl]\n",
        "    train_rest = train[train['sentiment'] != lbl]\n",
        "    train_rest = train_rest.sample(int(len(train_0)))\n",
        "                                         \n",
        "\n",
        "    train_sampled = train_0.append(train_rest)\n",
        "    train_sampled = train_sampled.sample(frac=1).reset_index()\n",
        "\n",
        "    Y_train = train_sampled['sentiment'].apply(lambda x: 0 if x == lbl else 1 ).tolist()\n",
        "    Y_test = dev['sentiment'].apply(lambda x: 0 if x == lbl else 1 ).tolist()\n",
        "\n",
        "    num_features = 3000\n",
        "    model_tfidf = TfidfVectorizer(max_features=num_features)\n",
        "    model_tfidf.fit(train['tweet'])\n",
        "\n",
        "    model_cv = CountVectorizer(max_features=num_features)\n",
        "    model_cv.fit(train['tweet'])\n",
        "\n",
        "    X_train_tfidf = model_tfidf.transform(train_sampled['tweet']).toarray()\n",
        "    X_test_tfidf = model_tfidf.transform(dev['tweet']).toarray()\n",
        "    X_train_cv = model_cv.transform(train_sampled['tweet']).toarray()\n",
        "    X_test_cv = model_cv.transform(dev['tweet']).toarray()\n",
        "    \n",
        "\n",
        "    model_t = LogisticRegression()\n",
        "    model_t.fit(X_train_tfidf,Y_train)\n",
        "    Y_pred_t = model_t.predict(X_test_tfidf)\n",
        "    model_c = LogisticRegression()\n",
        "    model_c.fit(X_train_cv,Y_train)\n",
        "    Y_pred_cv = model_c.predict(X_test_cv)\n",
        "    print(\"Individual class accuracies for logistic regression  (tfidf vectiorizer)\")\n",
        "    print(classification_report(Y_test,Y_pred_t,target_names = [class_mapping[lbl],'not-'+class_mapping[lbl]]))\n",
        "    print(\"Individual class accuracies for logistic regression  (count vectiorizer)\")\n",
        "    print(classification_report(Y_test,Y_pred_cv,target_names = [class_mapping[lbl],'not-'+class_mapping[lbl]]))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Individual class accuracies for logistic regression  (tfidf vectiorizer)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.74      0.86      0.80        84\n",
            "   not-anger       0.95      0.90      0.93       263\n",
            "\n",
            "    accuracy                           0.89       347\n",
            "   macro avg       0.85      0.88      0.86       347\n",
            "weighted avg       0.90      0.89      0.90       347\n",
            "\n",
            "Individual class accuracies for logistic regression  (count vectiorizer)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.76      0.88      0.81        84\n",
            "   not-anger       0.96      0.91      0.93       263\n",
            "\n",
            "    accuracy                           0.90       347\n",
            "   macro avg       0.86      0.89      0.87       347\n",
            "weighted avg       0.91      0.90      0.90       347\n",
            "\n",
            "Individual class accuracies for logistic regression  (tfidf vectiorizer)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        fear       0.76      0.79      0.78       110\n",
            "    not-fear       0.90      0.89      0.89       237\n",
            "\n",
            "    accuracy                           0.86       347\n",
            "   macro avg       0.83      0.84      0.84       347\n",
            "weighted avg       0.86      0.86      0.86       347\n",
            "\n",
            "Individual class accuracies for logistic regression  (count vectiorizer)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        fear       0.80      0.79      0.79       110\n",
            "    not-fear       0.90      0.91      0.91       237\n",
            "\n",
            "    accuracy                           0.87       347\n",
            "   macro avg       0.85      0.85      0.85       347\n",
            "weighted avg       0.87      0.87      0.87       347\n",
            "\n",
            "Individual class accuracies for logistic regression  (tfidf vectiorizer)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         joy       0.75      0.78      0.77        79\n",
            "     not-joy       0.94      0.92      0.93       268\n",
            "\n",
            "    accuracy                           0.89       347\n",
            "   macro avg       0.84      0.85      0.85       347\n",
            "weighted avg       0.89      0.89      0.89       347\n",
            "\n",
            "Individual class accuracies for logistic regression  (count vectiorizer)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         joy       0.76      0.82      0.79        79\n",
            "     not-joy       0.95      0.93      0.94       268\n",
            "\n",
            "    accuracy                           0.90       347\n",
            "   macro avg       0.86      0.87      0.86       347\n",
            "weighted avg       0.91      0.90      0.90       347\n",
            "\n",
            "Individual class accuracies for logistic regression  (tfidf vectiorizer)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     sadness       0.70      0.82      0.76        74\n",
            " not-sadness       0.95      0.90      0.93       273\n",
            "\n",
            "    accuracy                           0.89       347\n",
            "   macro avg       0.83      0.86      0.84       347\n",
            "weighted avg       0.90      0.89      0.89       347\n",
            "\n",
            "Individual class accuracies for logistic regression  (count vectiorizer)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     sadness       0.74      0.86      0.80        74\n",
            " not-sadness       0.96      0.92      0.94       273\n",
            "\n",
            "    accuracy                           0.91       347\n",
            "   macro avg       0.85      0.89      0.87       347\n",
            "weighted avg       0.92      0.91      0.91       347\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tVZ0zpYJv5H",
        "outputId": "30648f86-9dd7-4437-cbe5-652eebae197a"
      },
      "source": [
        "for lbl in [0,1,2,3]:\n",
        "    \n",
        "    train_0 = train[train['sentiment'] == lbl]\n",
        "    train_rest = train[train['sentiment'] != lbl]\n",
        "    train_rest = train_rest.sample(int(len(train_0)))\n",
        "                                         \n",
        "\n",
        "    train_sampled = train_0.append(train_rest)\n",
        "    train_sampled = train_sampled.sample(frac=1).reset_index()\n",
        "\n",
        "    Y_train = train_sampled['sentiment'].apply(lambda x: 0 if x == lbl else 1 ).tolist()\n",
        "    Y_test = dev['sentiment'].apply(lambda x: 0 if x == lbl else 1 ).tolist()\n",
        "\n",
        "    num_features = 3000\n",
        "    model_tfidf = TfidfVectorizer(max_features=num_features)\n",
        "    model_tfidf.fit(train['tweet'])\n",
        "\n",
        "    model_cv = CountVectorizer(max_features=num_features)\n",
        "    model_cv.fit(train['tweet'])\n",
        "\n",
        "    X_train_tfidf = model_tfidf.transform(train_sampled['tweet']).toarray()\n",
        "    X_test_tfidf = model_tfidf.transform(dev['tweet']).toarray()\n",
        "    X_train_cv = model_cv.transform(train_sampled['tweet']).toarray()\n",
        "    X_test_cv = model_cv.transform(dev['tweet']).toarray()\n",
        "    \n",
        "\n",
        "    model_t = MultinomialNB()\n",
        "    model_t.fit(X_train_tfidf,Y_train)\n",
        "    Y_pred_t = model_t.predict(X_test_tfidf)\n",
        "    model_c = MultinomialNB()\n",
        "    model_c.fit(X_train_cv,Y_train)\n",
        "    Y_pred_cv = model_c.predict(X_test_cv)\n",
        "    print(\"Individual class accuracies for Naive Bayes  (tfidf vectiorizer)\")\n",
        "    print(classification_report(Y_test,Y_pred_t,target_names = [class_mapping[lbl],'not-'+class_mapping[lbl]]))\n",
        "    print(\"Individual class accuracies for Naive Bayes  (count vectiorizer)\")\n",
        "    print(classification_report(Y_test,Y_pred_cv,target_names = [class_mapping[lbl],'not-'+class_mapping[lbl]]))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Individual class accuracies for Naive Bayes  (tfidf vectiorizer)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.57      0.88      0.69        84\n",
            "   not-anger       0.95      0.79      0.86       263\n",
            "\n",
            "    accuracy                           0.81       347\n",
            "   macro avg       0.76      0.83      0.78       347\n",
            "weighted avg       0.86      0.81      0.82       347\n",
            "\n",
            "Individual class accuracies for Naive Bayes  (count vectiorizer)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.58      0.88      0.70        84\n",
            "   not-anger       0.95      0.80      0.87       263\n",
            "\n",
            "    accuracy                           0.82       347\n",
            "   macro avg       0.77      0.84      0.79       347\n",
            "weighted avg       0.86      0.82      0.83       347\n",
            "\n",
            "Individual class accuracies for Naive Bayes  (tfidf vectiorizer)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        fear       0.72      0.81      0.76       110\n",
            "    not-fear       0.91      0.85      0.88       237\n",
            "\n",
            "    accuracy                           0.84       347\n",
            "   macro avg       0.81      0.83      0.82       347\n",
            "weighted avg       0.85      0.84      0.84       347\n",
            "\n",
            "Individual class accuracies for Naive Bayes  (count vectiorizer)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        fear       0.71      0.81      0.75       110\n",
            "    not-fear       0.90      0.84      0.87       237\n",
            "\n",
            "    accuracy                           0.83       347\n",
            "   macro avg       0.81      0.83      0.81       347\n",
            "weighted avg       0.84      0.83      0.84       347\n",
            "\n",
            "Individual class accuracies for Naive Bayes  (tfidf vectiorizer)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         joy       0.61      0.95      0.74        79\n",
            "     not-joy       0.98      0.82      0.89       268\n",
            "\n",
            "    accuracy                           0.85       347\n",
            "   macro avg       0.80      0.89      0.82       347\n",
            "weighted avg       0.90      0.85      0.86       347\n",
            "\n",
            "Individual class accuracies for Naive Bayes  (count vectiorizer)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         joy       0.64      0.95      0.76        79\n",
            "     not-joy       0.98      0.84      0.91       268\n",
            "\n",
            "    accuracy                           0.86       347\n",
            "   macro avg       0.81      0.89      0.83       347\n",
            "weighted avg       0.90      0.86      0.87       347\n",
            "\n",
            "Individual class accuracies for Naive Bayes  (tfidf vectiorizer)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     sadness       0.51      0.95      0.66        74\n",
            " not-sadness       0.98      0.75      0.85       273\n",
            "\n",
            "    accuracy                           0.80       347\n",
            "   macro avg       0.75      0.85      0.76       347\n",
            "weighted avg       0.88      0.80      0.81       347\n",
            "\n",
            "Individual class accuracies for Naive Bayes  (count vectiorizer)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     sadness       0.51      0.93      0.66        74\n",
            " not-sadness       0.98      0.76      0.85       273\n",
            "\n",
            "    accuracy                           0.80       347\n",
            "   macro avg       0.74      0.85      0.76       347\n",
            "weighted avg       0.88      0.80      0.81       347\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLS51qxfJ7KY",
        "outputId": "278055c1-481c-4211-f21b-7dbb232fc097"
      },
      "source": [
        "for lbl in [0,1,2,3]:\n",
        "    \n",
        "    train_0 = train[train['sentiment'] == lbl]\n",
        "    train_rest = train[train['sentiment'] != lbl]\n",
        "    train_rest = train_rest.sample(int(len(train_0)))\n",
        "                                         \n",
        "\n",
        "    train_sampled = train_0.append(train_rest)\n",
        "    train_sampled = train_sampled.sample(frac=1).reset_index()\n",
        "\n",
        "    Y_train = train_sampled['sentiment'].apply(lambda x: 0 if x == lbl else 1 ).tolist()\n",
        "    Y_test = dev['sentiment'].apply(lambda x: 0 if x == lbl else 1 ).tolist()\n",
        "\n",
        "    num_features = 3000\n",
        "    model_tfidf = TfidfVectorizer(max_features=num_features)\n",
        "    model_tfidf.fit(train['tweet'])\n",
        "\n",
        "    model_cv = CountVectorizer(max_features=num_features)\n",
        "    model_cv.fit(train['tweet'])\n",
        "\n",
        "    X_train_tfidf = model_tfidf.transform(train_sampled['tweet']).toarray()\n",
        "    X_test_tfidf = model_tfidf.transform(dev['tweet']).toarray()\n",
        "    X_train_cv = model_cv.transform(train_sampled['tweet']).toarray()\n",
        "    X_test_cv = model_cv.transform(dev['tweet']).toarray()\n",
        "    \n",
        "\n",
        "    model_t = LinearSVC()\n",
        "    model_t.fit(X_train_tfidf,Y_train)\n",
        "    Y_pred_t = model_t.predict(X_test_tfidf)\n",
        "    model_c = LinearSVC()\n",
        "    model_c.fit(X_train_cv,Y_train)\n",
        "    Y_pred_cv = model_c.predict(X_test_cv)\n",
        "    print(\"Individual class accuracies for Linear SVC  (tfidf vectiorizer)\")\n",
        "    print(classification_report(Y_test,Y_pred_t,target_names = [class_mapping[lbl],'not-'+class_mapping[lbl]]))\n",
        "    print(\"Individual class accuracies for Linear SVC  (count vectiorizer)\")\n",
        "    print(classification_report(Y_test,Y_pred_cv,target_names = [class_mapping[lbl],'not-'+class_mapping[lbl]]))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Individual class accuracies for Linear SVC  (tfidf vectiorizer)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.73      0.90      0.81        84\n",
            "   not-anger       0.97      0.89      0.93       263\n",
            "\n",
            "    accuracy                           0.90       347\n",
            "   macro avg       0.85      0.90      0.87       347\n",
            "weighted avg       0.91      0.90      0.90       347\n",
            "\n",
            "Individual class accuracies for Linear SVC  (count vectiorizer)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.72      0.90      0.80        84\n",
            "   not-anger       0.97      0.89      0.93       263\n",
            "\n",
            "    accuracy                           0.89       347\n",
            "   macro avg       0.85      0.90      0.87       347\n",
            "weighted avg       0.91      0.89      0.90       347\n",
            "\n",
            "Individual class accuracies for Linear SVC  (tfidf vectiorizer)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        fear       0.81      0.80      0.81       110\n",
            "    not-fear       0.91      0.92      0.91       237\n",
            "\n",
            "    accuracy                           0.88       347\n",
            "   macro avg       0.86      0.86      0.86       347\n",
            "weighted avg       0.88      0.88      0.88       347\n",
            "\n",
            "Individual class accuracies for Linear SVC  (count vectiorizer)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        fear       0.83      0.82      0.83       110\n",
            "    not-fear       0.92      0.92      0.92       237\n",
            "\n",
            "    accuracy                           0.89       347\n",
            "   macro avg       0.87      0.87      0.87       347\n",
            "weighted avg       0.89      0.89      0.89       347\n",
            "\n",
            "Individual class accuracies for Linear SVC  (tfidf vectiorizer)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         joy       0.76      0.85      0.80        79\n",
            "     not-joy       0.95      0.92      0.94       268\n",
            "\n",
            "    accuracy                           0.90       347\n",
            "   macro avg       0.86      0.88      0.87       347\n",
            "weighted avg       0.91      0.90      0.91       347\n",
            "\n",
            "Individual class accuracies for Linear SVC  (count vectiorizer)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         joy       0.76      0.84      0.80        79\n",
            "     not-joy       0.95      0.92      0.94       268\n",
            "\n",
            "    accuracy                           0.90       347\n",
            "   macro avg       0.85      0.88      0.87       347\n",
            "weighted avg       0.91      0.90      0.90       347\n",
            "\n",
            "Individual class accuracies for Linear SVC  (tfidf vectiorizer)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     sadness       0.69      0.89      0.78        74\n",
            " not-sadness       0.97      0.89      0.93       273\n",
            "\n",
            "    accuracy                           0.89       347\n",
            "   macro avg       0.83      0.89      0.85       347\n",
            "weighted avg       0.91      0.89      0.90       347\n",
            "\n",
            "Individual class accuracies for Linear SVC  (count vectiorizer)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     sadness       0.69      0.89      0.78        74\n",
            " not-sadness       0.97      0.89      0.93       273\n",
            "\n",
            "    accuracy                           0.89       347\n",
            "   macro avg       0.83      0.89      0.86       347\n",
            "weighted avg       0.91      0.89      0.90       347\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UBgSLXYyKtkr",
        "outputId": "b2e97b8f-c737-4524-f706-054672c5fbfd"
      },
      "source": [
        "for lbl in [0,1,2,3]:\n",
        "    \n",
        "    train_0 = train[train['sentiment'] == lbl]\n",
        "    train_rest = train[train['sentiment'] != lbl]\n",
        "    train_rest = train_rest.sample(int(len(train_0)))\n",
        "                                         \n",
        "\n",
        "    train_sampled = train_0.append(train_rest)\n",
        "    train_sampled = train_sampled.sample(frac=1).reset_index()\n",
        "\n",
        "    Y_train = train_sampled['sentiment'].apply(lambda x: 0 if x == lbl else 1 ).tolist()\n",
        "    Y_test = dev['sentiment'].apply(lambda x: 0 if x == lbl else 1 ).tolist()\n",
        "\n",
        "    num_features = 3000\n",
        "    model_tfidf = TfidfVectorizer(max_features=num_features)\n",
        "    model_tfidf.fit(train['tweet'])\n",
        "\n",
        "    model_cv = CountVectorizer(max_features=num_features)\n",
        "    model_cv.fit(train['tweet'])\n",
        "\n",
        "    X_train_tfidf = model_tfidf.transform(train_sampled['tweet']).toarray()\n",
        "    X_test_tfidf = model_tfidf.transform(dev['tweet']).toarray()\n",
        "    X_train_cv = model_cv.transform(train_sampled['tweet']).toarray()\n",
        "    X_test_cv = model_cv.transform(dev['tweet']).toarray()\n",
        "    \n",
        "\n",
        "    model_t = RandomForestClassifier()\n",
        "    model_t.fit(X_train_tfidf,Y_train)\n",
        "    Y_pred_t = model_t.predict(X_test_tfidf)\n",
        "    model_c = RandomForestClassifier()\n",
        "    model_c.fit(X_train_cv,Y_train)\n",
        "    Y_pred_cv = model_c.predict(X_test_cv)\n",
        "    print(\"Individual class accuracies for Random Forest  (tfidf vectiorizer)\")\n",
        "    print(classification_report(Y_test,Y_pred_t,target_names = [class_mapping[lbl],'not-'+class_mapping[lbl]]))\n",
        "    print(\"Individual class accuracies for Random Forest  (count vectiorizer)\")\n",
        "    print(classification_report(Y_test,Y_pred_cv,target_names = [class_mapping[lbl],'not-'+class_mapping[lbl]]))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Individual class accuracies for Random Forest  (tfidf vectiorizer)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.76      0.85      0.80        84\n",
            "   not-anger       0.95      0.91      0.93       263\n",
            "\n",
            "    accuracy                           0.90       347\n",
            "   macro avg       0.85      0.88      0.86       347\n",
            "weighted avg       0.90      0.90      0.90       347\n",
            "\n",
            "Individual class accuracies for Random Forest  (count vectiorizer)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.85      0.85      0.85        84\n",
            "   not-anger       0.95      0.95      0.95       263\n",
            "\n",
            "    accuracy                           0.93       347\n",
            "   macro avg       0.90      0.90      0.90       347\n",
            "weighted avg       0.93      0.93      0.93       347\n",
            "\n",
            "Individual class accuracies for Random Forest  (tfidf vectiorizer)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        fear       0.87      0.72      0.79       110\n",
            "    not-fear       0.88      0.95      0.91       237\n",
            "\n",
            "    accuracy                           0.88       347\n",
            "   macro avg       0.87      0.83      0.85       347\n",
            "weighted avg       0.88      0.88      0.87       347\n",
            "\n",
            "Individual class accuracies for Random Forest  (count vectiorizer)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        fear       0.86      0.75      0.80       110\n",
            "    not-fear       0.89      0.94      0.92       237\n",
            "\n",
            "    accuracy                           0.88       347\n",
            "   macro avg       0.87      0.85      0.86       347\n",
            "weighted avg       0.88      0.88      0.88       347\n",
            "\n",
            "Individual class accuracies for Random Forest  (tfidf vectiorizer)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         joy       0.79      0.89      0.83        79\n",
            "     not-joy       0.97      0.93      0.95       268\n",
            "\n",
            "    accuracy                           0.92       347\n",
            "   macro avg       0.88      0.91      0.89       347\n",
            "weighted avg       0.92      0.92      0.92       347\n",
            "\n",
            "Individual class accuracies for Random Forest  (count vectiorizer)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         joy       0.81      0.90      0.85        79\n",
            "     not-joy       0.97      0.94      0.95       268\n",
            "\n",
            "    accuracy                           0.93       347\n",
            "   macro avg       0.89      0.92      0.90       347\n",
            "weighted avg       0.93      0.93      0.93       347\n",
            "\n",
            "Individual class accuracies for Random Forest  (tfidf vectiorizer)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     sadness       0.54      0.92      0.68        74\n",
            " not-sadness       0.97      0.79      0.87       273\n",
            "\n",
            "    accuracy                           0.82       347\n",
            "   macro avg       0.76      0.85      0.78       347\n",
            "weighted avg       0.88      0.82      0.83       347\n",
            "\n",
            "Individual class accuracies for Random Forest  (count vectiorizer)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     sadness       0.67      0.88      0.76        74\n",
            " not-sadness       0.96      0.88      0.92       273\n",
            "\n",
            "    accuracy                           0.88       347\n",
            "   macro avg       0.82      0.88      0.84       347\n",
            "weighted avg       0.90      0.88      0.89       347\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKPpSObPK9lE",
        "outputId": "f233bca0-efd0-4958-e485-d3f655096c24"
      },
      "source": [
        "for lbl in [0,1,2,3]:\n",
        "    \n",
        "    train_0 = train[train['sentiment'] == lbl]\n",
        "    train_rest = train[train['sentiment'] != lbl]\n",
        "    train_rest = train_rest.sample(int(len(train_0)))\n",
        "                                         \n",
        "\n",
        "    train_sampled = train_0.append(train_rest)\n",
        "    train_sampled = train_sampled.sample(frac=1).reset_index()\n",
        "\n",
        "    Y_train = train_sampled['sentiment'].apply(lambda x: 0 if x == lbl else 1 ).tolist()\n",
        "    Y_test = dev['sentiment'].apply(lambda x: 0 if x == lbl else 1 ).tolist()\n",
        "\n",
        "    num_features = 3000\n",
        "    model_tfidf = TfidfVectorizer(max_features=num_features)\n",
        "    model_tfidf.fit(train['tweet'])\n",
        "\n",
        "    model_cv = CountVectorizer(max_features=num_features)\n",
        "    model_cv.fit(train['tweet'])\n",
        "\n",
        "    X_train_tfidf = model_tfidf.transform(train_sampled['tweet']).toarray()\n",
        "    X_test_tfidf = model_tfidf.transform(dev['tweet']).toarray()\n",
        "    X_train_cv = model_cv.transform(train_sampled['tweet']).toarray()\n",
        "    X_test_cv = model_cv.transform(dev['tweet']).toarray()\n",
        "    \n",
        "\n",
        "    model_t = MLPClassifier()\n",
        "    model_t.fit(X_train_tfidf,Y_train)\n",
        "    Y_pred_t = model_t.predict(X_test_tfidf)\n",
        "    model_c = MLPClassifier()\n",
        "    model_c.fit(X_train_cv,Y_train)\n",
        "    Y_pred_cv = model_c.predict(X_test_cv)\n",
        "    print(\"Individual class accuracies for MLP Classifier  (tfidf vectiorizer)\")\n",
        "    print(classification_report(Y_test,Y_pred_t,target_names = [class_mapping[lbl],'not-'+class_mapping[lbl]]))\n",
        "    print(\"Individual class accuracies for MLP Classifier  (count vectiorizer)\")\n",
        "    print(classification_report(Y_test,Y_pred_cv,target_names = [class_mapping[lbl],'not-'+class_mapping[lbl]]))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Individual class accuracies for MLP Classifier  (tfidf vectiorizer)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.64      0.88      0.74        84\n",
            "   not-anger       0.96      0.84      0.89       263\n",
            "\n",
            "    accuracy                           0.85       347\n",
            "   macro avg       0.80      0.86      0.82       347\n",
            "weighted avg       0.88      0.85      0.86       347\n",
            "\n",
            "Individual class accuracies for MLP Classifier  (count vectiorizer)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.68      0.90      0.78        84\n",
            "   not-anger       0.97      0.86      0.91       263\n",
            "\n",
            "    accuracy                           0.87       347\n",
            "   macro avg       0.82      0.88      0.84       347\n",
            "weighted avg       0.90      0.87      0.88       347\n",
            "\n",
            "Individual class accuracies for MLP Classifier  (tfidf vectiorizer)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        fear       0.69      0.75      0.72       110\n",
            "    not-fear       0.88      0.85      0.86       237\n",
            "\n",
            "    accuracy                           0.82       347\n",
            "   macro avg       0.79      0.80      0.79       347\n",
            "weighted avg       0.82      0.82      0.82       347\n",
            "\n",
            "Individual class accuracies for MLP Classifier  (count vectiorizer)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        fear       0.69      0.80      0.74       110\n",
            "    not-fear       0.90      0.84      0.87       237\n",
            "\n",
            "    accuracy                           0.82       347\n",
            "   macro avg       0.80      0.82      0.80       347\n",
            "weighted avg       0.83      0.82      0.83       347\n",
            "\n",
            "Individual class accuracies for MLP Classifier  (tfidf vectiorizer)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         joy       0.62      0.85      0.72        79\n",
            "     not-joy       0.95      0.85      0.90       268\n",
            "\n",
            "    accuracy                           0.85       347\n",
            "   macro avg       0.79      0.85      0.81       347\n",
            "weighted avg       0.87      0.85      0.85       347\n",
            "\n",
            "Individual class accuracies for MLP Classifier  (count vectiorizer)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         joy       0.62      0.84      0.71        79\n",
            "     not-joy       0.95      0.85      0.89       268\n",
            "\n",
            "    accuracy                           0.84       347\n",
            "   macro avg       0.78      0.84      0.80       347\n",
            "weighted avg       0.87      0.84      0.85       347\n",
            "\n",
            "Individual class accuracies for MLP Classifier  (tfidf vectiorizer)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     sadness       0.54      0.89      0.67        74\n",
            " not-sadness       0.96      0.79      0.87       273\n",
            "\n",
            "    accuracy                           0.81       347\n",
            "   macro avg       0.75      0.84      0.77       347\n",
            "weighted avg       0.87      0.81      0.83       347\n",
            "\n",
            "Individual class accuracies for MLP Classifier  (count vectiorizer)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     sadness       0.58      0.91      0.71        74\n",
            " not-sadness       0.97      0.82      0.89       273\n",
            "\n",
            "    accuracy                           0.84       347\n",
            "   macro avg       0.77      0.86      0.80       347\n",
            "weighted avg       0.89      0.84      0.85       347\n",
            "\n"
          ]
        }
      ]
    }
  ]
}