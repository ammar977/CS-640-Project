{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS640-project-baseline_models.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXETdGUWcF6R"
      },
      "source": [
        "# %tensorflow_version 2.x\n",
        "# import tensorflow as tf\n",
        "# device_name = tf.test.gpu_device_name()\n",
        "# if device_name != '/device:GPU:0':\n",
        "#   raise SystemError('GPU device not found')\n",
        "# print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQkfKsoZrdit",
        "outputId": "294c120d-c99f-46d2-bcad-ea72f8e9029c"
      },
      "source": [
        "!pip install preprocessor\n",
        "!pip install tweet-preprocessor\n",
        "!pip install beautifulsoup4\n",
        "!pip install fasttext\n",
        "!pip install emoji\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: preprocessor in /usr/local/lib/python3.7/dist-packages (1.1.3)\n",
            "Requirement already satisfied: tweet-preprocessor in /usr/local/lib/python3.7/dist-packages (0.6.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (4.6.3)\n",
            "Requirement already satisfied: fasttext in /usr/local/lib/python3.7/dist-packages (0.9.2)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.7/dist-packages (from fasttext) (2.8.1)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext) (57.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fasttext) (1.19.5)\n",
            "Collecting emoji\n",
            "  Downloading emoji-1.6.1.tar.gz (170 kB)\n",
            "\u001b[K     |████████████████████████████████| 170 kB 31.8 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.6.1-py3-none-any.whl size=169314 sha256=7b6d31e6f87a9014e577e76f45f02e12e5f71a6804a44cb0ecc3b299010f81bf\n",
            "  Stored in directory: /root/.cache/pip/wheels/ea/5f/d3/03d313ddb3c2a1a427bb4690f1621eea60fe6f2a30cc95940f\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-1.6.1\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZPyZu9zcODP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54cb1d4f-c1ea-44a6-8b39-6bb19a3d2c42"
      },
      "source": [
        "import os\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import preprocessor\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import fasttext\n",
        "import csv\n",
        "import datetime\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import itertools\n",
        "import emoji\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vk8YMKVTcSvq",
        "outputId": "0dcca088-1df7-4bc4-f0c8-bccc51aa177d"
      },
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():       \n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
        "    print('Device name:', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No GPU available, using the CPU instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEnPKZ3XcXTF",
        "outputId": "e0aec635-2feb-4782-bc4d-454f70aa1c47"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRSeYgKQcrkE",
        "outputId": "9258dcc0-6348-4da4-fba1-02c67c3f15ec"
      },
      "source": [
        "import os\n",
        "import json\n",
        "import argparse\n",
        "import importlib\n",
        "from typing import Dict, List, Optional, Tuple, Union\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import metrics\n",
        "\n",
        "def getData(type):\n",
        "\n",
        "    folder_path = \"/content/drive/MyDrive/CS640-project/DataTwitter/\"+ type + \"/\"\n",
        "    dfs = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith('txt'):\n",
        "            path = os.path.join(folder_path, filename)\n",
        "            df = pd.read_csv(path, sep=\"\\\\t\", header=None)\n",
        "\n",
        "            df.rename(columns={1: \"tweet\", 2: \"sentiment\"}, inplace=True)\n",
        "            df = df.drop(columns=[df.columns[0], df.columns[3]])\n",
        "            df['sentiment'] = df['sentiment'].replace(['anger'],0)\n",
        "            df['sentiment'] = df['sentiment'].replace(['fear'],1)\n",
        "            df['sentiment'] = df['sentiment'].replace(['joy'],2)\n",
        "            df['sentiment'] = df['sentiment'].replace(['sadness'],3)\n",
        "\n",
        "            dfs.append(df)\n",
        "\n",
        "\n",
        "    df = pd.concat(dfs)\n",
        "    # df.columns = ['text', 'label']\n",
        "    # df['label_numeric'] = df['label'].astype('category').cat.codes\n",
        "    print('total ' + type +' samples : ', len(df))\n",
        "    print(df['sentiment'].value_counts())\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train = getData(\"Train\")\n",
        "dev = getData(\"Dev\")\n",
        "test = getData(\"Test\")\n",
        "# train = pd.concat([train, test])\n",
        "# train = train.sample(frac=0.01, replace=True, random_state=1)\n",
        "# # test = test.sample(frac=0.1, replace=True, random_state=1)\n",
        "X_train = train.filter(regex='tweet').tweet.values\n",
        "y_train = train.filter(regex='sentiment').sentiment.values\n",
        "X_dev = dev.filter(regex='tweet').tweet.values\n",
        "y_dev = dev.filter(regex='sentiment').sentiment.values\n",
        "X_test = test.filter(regex='tweet').tweet.values\n",
        "y_test = test.filter(regex='sentiment').sentiment.values\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total Train samples :  3613\n",
            "1    1147\n",
            "0     857\n",
            "2     823\n",
            "3     786\n",
            "Name: sentiment, dtype: int64\n",
            "total Dev samples :  347\n",
            "1    110\n",
            "0     84\n",
            "2     79\n",
            "3     74\n",
            "Name: sentiment, dtype: int64\n",
            "total Test samples :  3142\n",
            "1    995\n",
            "0    760\n",
            "2    714\n",
            "3    673\n",
            "Name: sentiment, dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:18: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVbzt4w3iH1x",
        "outputId": "15e2cc2f-c152-40ba-e4c5-2006d2d84811"
      },
      "source": [
        "import os\n",
        "import json\n",
        "import argparse\n",
        "import importlib\n",
        "from typing import Dict, List, Optional, Tuple, Union\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import metrics\n",
        "\n",
        "def getData():\n",
        "\n",
        "    folder_path = \"/content/drive/MyDrive/CS640-project/DataInstagram/\"\n",
        "    dfs = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith('csv'):\n",
        "            path = os.path.join(folder_path, filename)\n",
        "            df = pd.read_csv(path)\n",
        "\n",
        "            df.rename(columns={'imagename': \"image_id\", \"Contents\": \"tweet\", 'Q5_presence_asian': \"isEastAsian\"}, inplace=True)\n",
        "            df = df.drop(columns=[df.columns[1], df.columns[2], df.columns[3], df.columns[4], df.columns[6], df.columns[7], df.columns[8], df.columns[9],\n",
        "                                  df.columns[10], df.columns[11], df.columns[13], df.columns[14], df.columns[15], df.columns[16], df.columns[17],\n",
        "                                  df.columns[18], df.columns[19], df.columns[20], df.columns[21], df.columns[22], df.columns[23], df.columns[24], df.columns[25]])\n",
        "            # df['sentiment'] = df['sentiment'].replace(['anger'],0)\n",
        "            # df['sentiment'] = df['sentiment'].replace(['fear'],1)\n",
        "            # df['sentiment'] = df['sentiment'].replace(['joy'],2)\n",
        "            # df['sentiment'] = df['sentiment'].replace(['sadness'],3)\n",
        "\n",
        "            dfs.append(df)\n",
        "\n",
        "\n",
        "    df = pd.concat(dfs)\n",
        "    # df.columns = ['text', 'label']\n",
        "    # df['label_numeric'] = df['label'].astype('category').cat.codes\n",
        "    print('total ' + \"insta\" +' samples : ', len(df))\n",
        "    # print(df['isEastAsian'].value_counts())\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Insta_train = getData().tweet.values\n",
        "print(Insta_train)\n",
        "\n",
        "# train = pd.concat([train, test])\n",
        "# train = train.sample(frac=0.01, replace=True, random_state=1)\n",
        "# # test = test.sample(frac=0.1, replace=True, random_state=1)\n",
        "# Insta_train = train.filter(regex='tweet').tweet.values\n",
        "# y_train = train.filter(regex='sentiment').sentiment.values\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total insta samples :  9648\n",
            "['#covid #covid2020 #covidvirus #virus #coronavairus #coronavirus #coronavírus #coronavirüs #blackandwhite #blackandwhiteportrait #blackandwhitephoto #blackandwhite_photos #lockdown #lockdown2020 #lockdownlife #lockdownitaly #italylockdown #lockdowndiaries #lockdownactivities #stayathome #staysafe #stayhome #iorestoacasa #myhome #covid19 #covıd19 #covi̇d_19 #coviditalia #🖤 #🖤🖤🖤'\n",
            " 'Well this is the final mural of my trip in Australia, a very weird trip, to be honest I couldn’t connect with my painting, at the beginning it was a popular psicosis which looked unreal, then the airline call me with news that my flights was rebooked for 4 month later, I still had a lot to do, people to meet and paint to make, it was super sad when I had to buy another thicket a week before of planed and run out of the country, the same day they closed the border, I like to think that everything happens for reason, everything is meant to be ... if this is my way to start the painting tour this year I don’t really know what to expect. When I painted the mural I meant to make something for the woman, for warriors who don’t want anybody to tell them what to do or say, now it feels empty cos the world is thinking on a different thing. Thank you to my new friend @r_o_n_e who really helped and connect with me, what a beautiful city melbourne, what a beautiful country Australia, I hope to be back some day for more projects. Be safe and do what you’re told, this is not a joke. \\r\\r\\r\\r\\n#graff#graffiti#mural#muralart#muralgraffiti#streetart#artecallejero#portrait#retraro#painting#pintura#realismo#realism#hiperrealismo#hyperrealism#portrait#retrato#onlyspraypaint#onlyspray#noproyector#sinproyector#cobreart#melbourne#australia#graffitiaustralia#corona#coronavirus'\n",
            " 'Chegamos !!! Vão seguindo o movimento... Tem muita coisa que estamos preparando pra vocês!!!'\n",
            " ...\n",
            " '🙈🥰😍👉🏽 @love_serie_karma #daancorona #daancoronavirus #anddaancorona #corona #coronavirus #covid19 #restezchezvous #restezchezvouschallenge #restezalamaison #coronaout #senegal #dieyeart #senegal🇸🇳 #senegalaise #senegalcelebrities #galsen #fisha_senegal #kebetu #dudu #origineartsn #dudufaitdesvideos #dakar #dakarbuzz #saly #thies #marodi @fisha_senegal @fisha_senegal2 @jaaw_ketchup @abbanostress @mahfousse_comedien @origineartsn @seneweb_officiel @les_meilleurs_comediens @10mil_problemes @rakaaju @humour_galsen_221 @zapping_senegal @senegal_nexna @affichedakar @singom_comedien @galsen_reposts @amiraabed0 #amiraabed00 #amiraabed00 #karmalaserie #marodiproduction'\n",
            " '💥🚨 𝗙𝗔𝗟𝗟 𝗜𝗦 𝗖𝗢𝗠𝗜𝗡𝗚 🚨💥\\u2063\\r\\r\\r\\r\\r\\n\\u2063\\r\\r\\r\\r\\r\\n𝘼𝙧𝙚 𝙮𝙤𝙪 𝙧𝙚𝙖𝙙𝙮 𝙛𝙤𝙧 𝙖 𝙣𝙚𝙬 𝙛𝙖𝙡𝙡 𝙡𝙤𝙤𝙠?! \\u2063\\r\\r\\r\\r\\r\\n𝘽𝙤𝙤𝙠 𝙩𝙤𝙙𝙖𝙮 @𝙥𝙡𝙖𝙣𝙚𝙩𝙨𝙖𝙡𝙤𝙣𝙖𝙣𝙙𝙨𝙥𝙖.𝙘𝙤𝙢 \\u2063\\r\\r\\r\\r\\r\\n𝙊𝙧 ☎️ 𝟴𝟱𝟵-𝟮𝟲𝟯-𝟬𝟬𝟬𝟭\\u2063 ask for Ashley Bickers!\\r\\r\\r\\r\\r\\n\\u2063\\r\\r\\r\\r\\r\\n#sharethelex #coronavirus #lexingtonky #lexingtonhairstylist #kentuckyderby #universitylife  #universityofkentucky #olaplex #oribeobsessed #pumpkinspice'\n",
            " 'Follow me @kestine_kylie \\r\\r\\r\\r\\n.\\r\\r\\r\\r\\n.\\r\\r\\r\\r\\n#aimankhan #minalkhan #aimabaig\\xa0 #SanaJaved #imranabbas #mahirakhan #sajalaly #mehwishhayat #ushnashah\\xa0 #farhansaeed #mawrahocane #urwahocane #ayezakhan #ferozekhan\\r\\r\\r\\r\\n#hiramani\\xa0 #atifaslam #sumbuliqbal\\xa0 #hinaaltaf\\xa0 #haniaamir #mayaali\\xa0 #sarahkhan #sabaqamar #neelammuneer #sanamcha6uhdry\\xa0 #armeenakhan #hareemfarooq #iqraaziz\\xa0 #noorkhan\\xa0 #lollywood\\xa0 #coronavirus']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsC-g8QAcw8v"
      },
      "source": [
        "# def text_preprocessing(text):  \n",
        "#     text = re.sub(r'(@.*?)[\\s]', '', text)\n",
        "#     text = re.sub(r'&amp;', '&', text)\n",
        "#     text = re.sub(r\"<[\\w'/'\\s]*>\",'' ,text)\n",
        "#     text = re.sub(r\"[\\'\\\"\\-]+\", '',text)\n",
        "#     text = re.sub(r\"@[\\w]+\", '',text)\n",
        "#     text = re.sub(r'http\\S+', '',text)\n",
        "#     text = re.sub(r'\\s+', '', text).strip()\n",
        "#     return text\n",
        "preprocessor.set_options(preprocessor.OPT.URL,preprocessor.OPT.RESERVED)\n",
        "stop_words = nltk.corpus.stopwords.words('english')\n",
        "def text_preprocessing(text):   \n",
        "    text = preprocessor.tokenize(text)\n",
        "    text = ' '.join([word for word in text.split(' ') if word.lower() not in stop_words])\n",
        "    return text"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGzaMsZvdQx6"
      },
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def load_dict_smileys():\n",
        "    \n",
        "    return {\n",
        "        \":‑)\":\"smiley\",\n",
        "        \":-]\":\"smiley\",\n",
        "        \":-3\":\"smiley\",\n",
        "        \":->\":\"smiley\",\n",
        "        \"8-)\":\"smiley\",\n",
        "        \":-}\":\"smiley\",\n",
        "        \":)\":\"smiley\",\n",
        "        \":]\":\"smiley\",\n",
        "        \":3\":\"smiley\",\n",
        "        \":>\":\"smiley\",\n",
        "        \"8)\":\"smiley\",\n",
        "        \":}\":\"smiley\",\n",
        "        \":o)\":\"smiley\",\n",
        "        \":c)\":\"smiley\",\n",
        "        \":^)\":\"smiley\",\n",
        "        \"=]\":\"smiley\",\n",
        "        \"=)\":\"smiley\",\n",
        "        \":-))\":\"smiley\",\n",
        "        \":‑D\":\"smiley\",\n",
        "        \"8‑D\":\"smiley\",\n",
        "        \"x‑D\":\"smiley\",\n",
        "        \"X‑D\":\"smiley\",\n",
        "        \":D\":\"smiley\",\n",
        "        \"8D\":\"smiley\",\n",
        "        \"xD\":\"smiley\",\n",
        "        \"XD\":\"smiley\",\n",
        "        \":‑(\":\"sad\",\n",
        "        \":‑c\":\"sad\",\n",
        "        \":‑<\":\"sad\",\n",
        "        \":‑[\":\"sad\",\n",
        "        \":(\":\"sad\",\n",
        "        \":c\":\"sad\",\n",
        "        \":<\":\"sad\",\n",
        "        \":[\":\"sad\",\n",
        "        \":-||\":\"sad\",\n",
        "        \">:[\":\"sad\",\n",
        "        \":{\":\"sad\",\n",
        "        \":@\":\"sad\",\n",
        "        \">:(\":\"sad\",\n",
        "        \":'‑(\":\"sad\",\n",
        "        \":'(\":\"sad\",\n",
        "        \":‑P\":\"playful\",\n",
        "        \"X‑P\":\"playful\",\n",
        "        \"x‑p\":\"playful\",\n",
        "        \":‑p\":\"playful\",\n",
        "        \":‑Þ\":\"playful\",\n",
        "        \":‑þ\":\"playful\",\n",
        "        \":‑b\":\"playful\",\n",
        "        \":P\":\"playful\",\n",
        "        \"XP\":\"playful\",\n",
        "        \"xp\":\"playful\",\n",
        "        \":p\":\"playful\",\n",
        "        \":Þ\":\"playful\",\n",
        "        \":þ\":\"playful\",\n",
        "        \":b\":\"playful\",\n",
        "        \"<3\":\"love\"\n",
        "        }\n",
        "\n",
        "\n",
        "def load_dict_contractions_slangs():\n",
        "    \n",
        "    cont = {\n",
        "        \"ain't\":\"is not\",\n",
        "        \"amn't\":\"am not\",\n",
        "        \"aren't\":\"are not\",\n",
        "        \"can't\":\"cannot\",\n",
        "        \"'cause\":\"because\",\n",
        "        \"couldn't\":\"could not\",\n",
        "        \"couldn't've\":\"could not have\",\n",
        "        \"could've\":\"could have\",\n",
        "        \"daren't\":\"dare not\",\n",
        "        \"daresn't\":\"dare not\",\n",
        "        \"dasn't\":\"dare not\",\n",
        "        \"didn't\":\"did not\",\n",
        "        \"doesn't\":\"does not\",\n",
        "        \"don't\":\"do not\",\n",
        "        \"e'er\":\"ever\",\n",
        "        \"em\":\"them\",\n",
        "        \"everyone's\":\"everyone is\",\n",
        "        \"finna\":\"fixing to\",\n",
        "        \"gimme\":\"give me\",\n",
        "        \"gonna\":\"going to\",\n",
        "        \"gon't\":\"go not\",\n",
        "        \"gotta\":\"got to\",\n",
        "        \"hadn't\":\"had not\",\n",
        "        \"hasn't\":\"has not\",\n",
        "        \"haven't\":\"have not\",\n",
        "        \"he'd\":\"he would\",\n",
        "        \"he'll\":\"he will\",\n",
        "        \"he's\":\"he is\",\n",
        "        \"he've\":\"he have\",\n",
        "        \"how'd\":\"how would\",\n",
        "        \"how'll\":\"how will\",\n",
        "        \"how're\":\"how are\",\n",
        "        \"how's\":\"how is\",\n",
        "        \"i'd\":\"i would\",\n",
        "        \"i'll\":\"i will\",\n",
        "        \"i'm\":\"i am\",\n",
        "        \"i'm'a\":\"i am about to\",\n",
        "        \"i'm'o\":\"i am going to\",\n",
        "        \"isn't\":\"is not\",\n",
        "        \"it'd\":\"it would\",\n",
        "        \"it'll\":\"it will\",\n",
        "        \"it's\":\"it is\",\n",
        "        \"i've\":\"i have\",\n",
        "        \"kinda\":\"kind of\",\n",
        "        \"let's\":\"let us\",\n",
        "        \"mayn't\":\"may not\",\n",
        "        \"may've\":\"may have\",\n",
        "        \"mightn't\":\"might not\",\n",
        "        \"might've\":\"might have\",\n",
        "        \"mustn't\":\"must not\",\n",
        "        \"mustn't've\":\"must not have\",\n",
        "        \"must've\":\"must have\",\n",
        "        \"needn't\":\"need not\",\n",
        "        \"ne'er\":\"never\",\n",
        "        \"o'\":\"of\",\n",
        "        \"o'er\":\"over\",\n",
        "        \"ol'\":\"old\",\n",
        "        \"oughtn't\":\"ought not\",\n",
        "        \"shalln't\":\"shall not\",\n",
        "        \"shan't\":\"shall not\",\n",
        "        \"she'd\":\"she would\",\n",
        "        \"she'll\":\"she will\",\n",
        "        \"she's\":\"she is\",\n",
        "        \"shouldn't\":\"should not\",\n",
        "        \"shouldn't've\":\"should not have\",\n",
        "        \"should've\":\"should have\",\n",
        "        \"somebody's\":\"somebody is\",\n",
        "        \"someone's\":\"someone is\",\n",
        "        \"something's\":\"something is\",\n",
        "        \"that'd\":\"that would\",\n",
        "        \"that'll\":\"that will\",\n",
        "        \"that're\":\"that are\",\n",
        "        \"that's\":\"that is\",\n",
        "        \"there'd\":\"there would\",\n",
        "        \"there'll\":\"there will\",\n",
        "        \"there're\":\"there are\",\n",
        "        \"there's\":\"there is\",\n",
        "        \"these're\":\"these are\",\n",
        "        \"they'd\":\"they would\",\n",
        "        \"they'll\":\"they will\",\n",
        "        \"they're\":\"they are\",\n",
        "        \"they've\":\"they have\",\n",
        "        \"this's\":\"this is\",\n",
        "        \"those're\":\"those are\",\n",
        "        \"'tis\":\"it is\",\n",
        "        \"'twas\":\"it was\",\n",
        "        \"wanna\":\"want to\",\n",
        "        \"wasn't\":\"was not\",\n",
        "        \"we'd\":\"we would\",\n",
        "        \"we'd've\":\"we would have\",\n",
        "        \"we'll\":\"we will\",\n",
        "        \"we're\":\"we are\",\n",
        "        \"weren't\":\"were not\",\n",
        "        \"we've\":\"we have\",\n",
        "        \"what'd\":\"what did\",\n",
        "        \"what'll\":\"what will\",\n",
        "        \"what're\":\"what are\",\n",
        "        \"what's\":\"what is\",\n",
        "        \"what've\":\"what have\",\n",
        "        \"when's\":\"when is\",\n",
        "        \"where'd\":\"where did\",\n",
        "        \"where're\":\"where are\",\n",
        "        \"where's\":\"where is\",\n",
        "        \"where've\":\"where have\",\n",
        "        \"which's\":\"which is\",\n",
        "        \"who'd\":\"who would\",\n",
        "        \"who'd've\":\"who would have\",\n",
        "        \"who'll\":\"who will\",\n",
        "        \"who're\":\"who are\",\n",
        "        \"who's\":\"who is\",\n",
        "        \"who've\":\"who have\",\n",
        "        \"why'd\":\"why did\",\n",
        "        \"why're\":\"why are\",\n",
        "        \"why's\":\"why is\",\n",
        "        \"won't\":\"will not\",\n",
        "        \"wouldn't\":\"would not\",\n",
        "        \"would've\":\"would have\",\n",
        "        \"y'all\":\"you all\",\n",
        "        \"you'd\":\"you would\",\n",
        "        \"you'll\":\"you will\",\n",
        "        \"you're\":\"you are\",\n",
        "        \"you've\":\"you have\",\n",
        "        \"Whatcha\":\"What are you\",\n",
        "        \"luv\":\"love\",\n",
        "        \"sux\":\"sucks\",\n",
        "        \"rn\":\"right now\",\n",
        "        \"atm\": \"at the moment\",\n",
        "        \"idk\": \"i dont know\",\n",
        "        \"cuz\": \"because\",\n",
        "        \"bcuz\": \"because\",\n",
        "        \"ur\": \"you are\",\n",
        "        \"ly\": \"love you\",\n",
        "        \"lol\": \"laugh out loud\",\n",
        "        \"rofl\": \"rolling on floor laughing\",\n",
        "        \"lmao\": \"laughing my ass off\",\n",
        "        \"ok\": \"okay\",\n",
        "        \"ty\": \"thank you\",\n",
        "        \"fav\": \"favorite\",\n",
        "        \"omg\": \"oh my god\"\n",
        "        }\n",
        "    \n",
        "    to_ret = {}\n",
        "    for k,v in cont.items():\n",
        "        key = k.lower().replace(\"'\",'')\n",
        "        to_ret[key] = v.lower()\n",
        "        \n",
        "    return to_ret\n",
        "\n",
        "\n",
        "def strip_accents(text):\n",
        "    if 'ø' in text or  'Ø' in text:\n",
        "        #Do nothing when finding ø \n",
        "        return text   \n",
        "    text = text.encode('ascii', 'ignore')\n",
        "    text = text.decode(\"utf-8\")\n",
        "    return str(text)\n",
        "\n",
        "\n",
        "def tweet_cleaning_for_sentiment_analysis(tweet):    \n",
        "    \n",
        "    #Escaping HTML characters\n",
        "    tweet = BeautifulSoup(tweet).get_text()\n",
        "    #Special case not handled previously.\n",
        "    tweet = tweet.replace('\\x92',\"'\")\n",
        "    #Removal of account\n",
        "    tweet = ' '.join(re.sub(\"(@[A-Za-z0-9_]+)\", \" \", tweet).split())\n",
        "    # removal of hashtag\n",
        "    tweet = tweet.replace('#','')\n",
        "    #Removal of address\n",
        "    tweet = ' '.join(re.sub(\"(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
        "    #Lower case\n",
        "    tweet = tweet.lower()\n",
        "    # rempval of 'RT'\n",
        "    tweet = tweet.replace('rt','')\n",
        "    # Removal of Punctuation\n",
        "    tweet = ' '.join(re.sub(\"[\\.\\,\\!\\?\\:\\;\\-\\=]\", \" \", tweet).split())\n",
        "    # removal of stop words\n",
        "    tweet = ' '.join([word for word in tweet.split(' ') if word not in stop_words])\n",
        "    # CONTRACTIONS source: https://en.wikipedia.org/wiki/Contraction_%28grammar%29\n",
        "    CONTRACTIONS_SLANGS = load_dict_contractions_slangs()\n",
        "    tweet = tweet.replace(\"’\",\"'\").replace(\"'\",\"\")\n",
        "    words = tweet.split()\n",
        "    reformed = [CONTRACTIONS_SLANGS[word] if word in CONTRACTIONS_SLANGS else word for word in words]\n",
        "    tweet = \" \".join(reformed)\n",
        "\n",
        "    # Standardizing words - lemmatization\n",
        "    tweet = ' '.join([lemmatizer.lemmatize(word) for word in tweet.split(' ')]).lower()\n",
        "    \n",
        "    #Deal with smileys\n",
        "    #source: https://en.wikipedia.org/wiki/List_of_emoticons\n",
        "    SMILEY = load_dict_smileys()  \n",
        "    words = tweet.split()\n",
        "    reformed = [SMILEY[word] if word in SMILEY else word for word in words]\n",
        "    tweet = \" \".join(reformed)\n",
        "    # replace emojis with desctiption - remove accents - remove underscores\n",
        "    tweet = emoji.demojize(tweet).lower()\n",
        "    tweet = strip_accents(tweet).lower()\n",
        "    tweet = tweet.replace(\":\",\" \")\n",
        "    tweet = ' '.join(tweet.split()).replace('_',' ')\n",
        "    \n",
        "    # remove new line characters\n",
        "    tweet = tweet.strip().replace('\\\\n',' ')\n",
        "    \n",
        "    # only keep alphabets\n",
        "    tweet = ''.join([c for c in tweet if (c.isalpha() or c ==' ')]).strip()\n",
        "    \n",
        "    # rempove double spaces and triple spaces\n",
        "    tweet = tweet.replace('   ',' ').replace('  ',' ')\n",
        "    \n",
        "    # one word replacements\n",
        "    replacement_dict = {\n",
        "        'u': 'you',\n",
        "        'v': 'we',\n",
        "        'r': 'are',\n",
        "        'w': 'we',\n",
        "        'n': 'and',\n",
        "        'nd': 'and',\n",
        "        '&': 'and'\n",
        "    }\n",
        "    \n",
        "    tweet = ' '.join([replacement_dict[word] if word in replacement_dict else word for word in tweet.split(' ')])\n",
        "    \n",
        "    # removal of stop words\n",
        "    tweet = ' '.join([word for word in tweet.split(' ') if word not in stop_words])\n",
        "\n",
        "    return tweet\n",
        "    "
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ihe7Ni8sc7_s"
      },
      "source": [
        "def preprocessAllData(X):\n",
        "  count = []\n",
        "  res = []\n",
        "  for data in X:\n",
        "    if data != \"\":\n",
        "      try:\n",
        "        res.append(tweet_cleaning_for_sentiment_analysis(data))\n",
        "        count.append(len(res))\n",
        "      except:\n",
        "        #nan values are filtered here \n",
        "        pass\n",
        "  print(count)\n",
        "  return np.array(res)\n",
        "\n",
        "# X_train = tweet_cleaning_for_sentiment_analysis(X_train)\n",
        "# X_dev = preprocessAllData(X_dev)\n",
        "# X_test = preprocessAllData(X_test)\n",
        "# X_insta = preprocessAllData(Insta_train)\n",
        "\n",
        "train['tweet'] = train['tweet'].apply(tweet_cleaning_for_sentiment_analysis)\n",
        "dev['tweet'] = dev['tweet'].apply(tweet_cleaning_for_sentiment_analysis)\n",
        "test['tweet'] = test['tweet'].apply(tweet_cleaning_for_sentiment_analysis)\n",
        "X_train = train.filter(regex='tweet').tweet.values\n",
        "y_train = train.filter(regex='sentiment').sentiment.values\n",
        "X_dev = dev.filter(regex='tweet').tweet.values\n",
        "y_dev = dev.filter(regex='sentiment').sentiment.values\n",
        "X_test = test.filter(regex='tweet').tweet.values\n",
        "y_test = test.filter(regex='sentiment').sentiment.values\n",
        "\n"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vi8hLvNi8VRb"
      },
      "source": [
        "Naive Bayes "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjQRe9WpdDc4"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from gensim.models import Word2Vec\n",
        "num_features = 3000\n",
        "#count vectorizer \n",
        "cv = CountVectorizer(max_features=num_features)\n",
        "cv.fit_transform(X_train)\n",
        "X_train_cv = cv.transform(X_train)\n",
        "X_dev_cv = cv.transform(X_dev)\n",
        "X_test_cv = cv.transform(X_test)\n",
        "\n",
        "#tfidf vectorizer \n",
        "\n",
        "tfidf = TfidfVectorizer(max_features=num_features)\n",
        "tfidf.fit_transform(X_train)\n",
        "X_train_tfidf = tfidf.transform(X_train)\n",
        "X_dev_tfidf = tfidf.transform(X_dev)\n",
        "X_test_tfidf = tfidf.transform(X_test)\n",
        "\n",
        "\n",
        "#word2vec\n",
        "\n",
        "# word2vecModel = Word2Vec(X_train, min_count=1,size= 50,workers=3, window =3, sg = 1)\n",
        "# X_train_word2vec = []\n",
        "# for i in X_train:\n",
        "#   X_train_word2vec.append(word2vecModel[i])\n",
        "# print(X_train_word2vec)\n",
        "\n"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbPD00cIfD0D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62b4d301-de09-4266-d977-5cb3b4590675"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "model_cv = MultinomialNB()\n",
        "model_cv.fit(X_train_cv, y_train)\n",
        "cv_nb_score = model_cv.score(X_dev_cv, y_dev)\n",
        "Y_pred = model_cv.predict(X_dev_cv)\n",
        "print(classification_report(y_dev,Y_pred))\n",
        "print(\"count vectorizer score: \", cv_nb_score)\n",
        "print(\"instagram data results: \")\n",
        "naiveBayesInstaPredictions = []\n",
        "for row in X_insta:\n",
        "  naiveBayesInstaPredictions.append(model_cv.predict(cv.transform([row]))[0])\n",
        "print(pd.DataFrame(naiveBayesInstaPredictions).value_counts())\n",
        "\n",
        "\n",
        "model_tfidf = MultinomialNB()\n",
        "model_tfidf.fit(X_train_tfidf, y_train)\n",
        "tfidf_nb_score = model_tfidf.score(X_dev_tfidf, y_dev)\n",
        "Y_pred = model_tfidf.predict(X_dev_cv)\n",
        "print(classification_report(y_dev,Y_pred))\n",
        "print(\"tfidf vectorizer score: \", tfidf_nb_score)\n",
        "print(\"instagram data results: \")\n",
        "naiveBayesInstaPredictions = []\n",
        "for row in X_insta:\n",
        "  naiveBayesInstaPredictions.append(model_tfidf.predict(tfidf.transform([row]))[0])\n",
        "print(pd.DataFrame(naiveBayesInstaPredictions).value_counts())"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.75      0.77        84\n",
            "           1       0.80      0.69      0.74       110\n",
            "           2       0.77      0.84      0.80        79\n",
            "           3       0.72      0.84      0.77        74\n",
            "\n",
            "    accuracy                           0.77       347\n",
            "   macro avg       0.77      0.78      0.77       347\n",
            "weighted avg       0.77      0.77      0.77       347\n",
            "\n",
            "count vectorizer score:  0.7694524495677233\n",
            "instagram data results: \n",
            "Series([], dtype: int64)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.75      0.80        84\n",
            "           1       0.77      0.78      0.77       110\n",
            "           2       0.78      0.80      0.79        79\n",
            "           3       0.79      0.85      0.82        74\n",
            "\n",
            "    accuracy                           0.79       347\n",
            "   macro avg       0.80      0.80      0.79       347\n",
            "weighted avg       0.79      0.79      0.79       347\n",
            "\n",
            "tfidf vectorizer score:  0.8069164265129684\n",
            "instagram data results: \n",
            "Series([], dtype: int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltaMRLECeWkV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6044c440-06ee-463e-e319-8f383d003545"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "model_cv = LogisticRegression()\n",
        "model_cv.fit(X_train_cv, y_train)\n",
        "cv_nb_score = model_cv.score(X_dev_cv, y_dev)\n",
        "Y_pred = model_cv.predict(X_dev_cv)\n",
        "print(classification_report(y_dev,Y_pred))\n",
        "print(\"count vectorizer score: \", cv_nb_score)\n",
        "print(\"instagram data results: \")\n",
        "naiveBayesInstaPredictions = []\n",
        "for row in X_insta:\n",
        "  naiveBayesInstaPredictions.append(model_cv.predict(cv.transform([row]))[0])\n",
        "print(pd.DataFrame(naiveBayesInstaPredictions).value_counts())\n",
        "\n",
        "\n",
        "model_tfidf = LogisticRegression()\n",
        "model_tfidf.fit(X_train_tfidf, y_train)\n",
        "tfidf_nb_score = model_tfidf.score(X_dev_tfidf, y_dev)\n",
        "Y_pred = model_tfidf.predict(X_dev_cv)\n",
        "print(classification_report(y_dev,Y_pred))\n",
        "print(\"tfidf vectorizer score: \", tfidf_nb_score)\n",
        "print(\"instagram data results: \")\n",
        "naiveBayesInstaPredictions = []\n",
        "for row in X_insta:\n",
        "  naiveBayesInstaPredictions.append(model_tfidf.predict(tfidf.transform([row]))[0])\n",
        "print(pd.DataFrame(naiveBayesInstaPredictions).value_counts())\n"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.83      0.84        84\n",
            "           1       0.81      0.84      0.82       110\n",
            "           2       0.86      0.85      0.85        79\n",
            "           3       0.83      0.81      0.82        74\n",
            "\n",
            "    accuracy                           0.83       347\n",
            "   macro avg       0.84      0.83      0.83       347\n",
            "weighted avg       0.83      0.83      0.83       347\n",
            "\n",
            "count vectorizer score:  0.8328530259365994\n",
            "instagram data results: \n",
            "Series([], dtype: int64)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.80      0.83        84\n",
            "           1       0.83      0.79      0.81       110\n",
            "           2       0.83      0.85      0.84        79\n",
            "           3       0.80      0.89      0.84        74\n",
            "\n",
            "    accuracy                           0.83       347\n",
            "   macro avg       0.83      0.83      0.83       347\n",
            "weighted avg       0.83      0.83      0.83       347\n",
            "\n",
            "tfidf vectorizer score:  0.8386167146974063\n",
            "instagram data results: \n",
            "Series([], dtype: int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l99bwIuTffwh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8060d01-199f-49eb-8896-62070cb7f02b"
      },
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "model_cv = LinearSVC()\n",
        "model_cv.fit(X_train_cv, y_train)\n",
        "cv_nb_score = model_cv.score(X_dev_cv, y_dev)\n",
        "Y_pred = model_cv.predict(X_dev_cv)\n",
        "print(classification_report(y_dev,Y_pred))\n",
        "print(\"count vectorizer score: \", cv_nb_score)\n",
        "print(\"instagram data results: \")\n",
        "naiveBayesInstaPredictions = []\n",
        "for row in X_insta:\n",
        "  naiveBayesInstaPredictions.append(model_cv.predict(cv.transform([row]))[0])\n",
        "print(pd.DataFrame(naiveBayesInstaPredictions).value_counts())\n",
        "\n",
        "\n",
        "model_tfidf = LinearSVC()\n",
        "model_tfidf.fit(X_train_tfidf, y_train)\n",
        "tfidf_nb_score = model_tfidf.score(X_dev_tfidf, y_dev)\n",
        "Y_pred = model_tfidf.predict(X_dev_cv)\n",
        "print(classification_report(y_dev,Y_pred))\n",
        "print(\"tfidf vectorizer score: \", tfidf_nb_score)\n",
        "print(\"instagram data results: \")\n",
        "naiveBayesInstaPredictions = []\n",
        "for row in X_insta:\n",
        "  naiveBayesInstaPredictions.append(model_tfidf.predict(tfidf.transform([row]))[0])\n",
        "print(pd.DataFrame(naiveBayesInstaPredictions).value_counts())"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.82      0.84        84\n",
            "           1       0.81      0.86      0.84       110\n",
            "           2       0.82      0.82      0.82        79\n",
            "           3       0.81      0.77      0.79        74\n",
            "\n",
            "    accuracy                           0.82       347\n",
            "   macro avg       0.83      0.82      0.82       347\n",
            "weighted avg       0.82      0.82      0.82       347\n",
            "\n",
            "count vectorizer score:  0.8242074927953891\n",
            "instagram data results: \n",
            "Series([], dtype: int64)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.82      0.86        84\n",
            "           1       0.85      0.80      0.83       110\n",
            "           2       0.81      0.87      0.84        79\n",
            "           3       0.80      0.89      0.84        74\n",
            "\n",
            "    accuracy                           0.84       347\n",
            "   macro avg       0.84      0.85      0.84       347\n",
            "weighted avg       0.85      0.84      0.84       347\n",
            "\n",
            "tfidf vectorizer score:  0.8645533141210374\n",
            "instagram data results: \n",
            "Series([], dtype: int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ir1OCRgkobnf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fb6ae21-ef2d-4dea-b43a-d4d8490a718d"
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "model_cv = MLPClassifier()\n",
        "model_cv.fit(X_train_cv, y_train)\n",
        "cv_nb_score = model_cv.score(X_dev_cv, y_dev)\n",
        "Y_pred = model_cv.predict(X_dev_cv)\n",
        "print(classification_report(y_dev,Y_pred))\n",
        "print(\"count vectorizer score: \", cv_nb_score)\n",
        "print(\"instagram data results: \")\n",
        "naiveBayesInstaPredictions = []\n",
        "for row in X_insta:\n",
        "  naiveBayesInstaPredictions.append(model_cv.predict(cv.transform([row]))[0])\n",
        "print(pd.DataFrame(naiveBayesInstaPredictions).value_counts())\n",
        "\n",
        "\n",
        "model_tfidf = MLPClassifier()\n",
        "model_tfidf.fit(X_train_tfidf, y_train)\n",
        "tfidf_nb_score = model_tfidf.score(X_dev_tfidf, y_dev)\n",
        "Y_pred = model_tfidf.predict(X_dev_cv)\n",
        "print(classification_report(y_dev,Y_pred))\n",
        "print(\"tfidf vectorizer score: \", tfidf_nb_score)\n",
        "print(\"instagram data results: \")\n",
        "naiveBayesInstaPredictions = []\n",
        "for row in X_insta:\n",
        "  naiveBayesInstaPredictions.append(model_tfidf.predict(tfidf.transform([row]))[0])\n",
        "print(pd.DataFrame(naiveBayesInstaPredictions).value_counts())"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.85      0.85        84\n",
            "           1       0.82      0.79      0.81       110\n",
            "           2       0.81      0.85      0.83        79\n",
            "           3       0.79      0.80      0.79        74\n",
            "\n",
            "    accuracy                           0.82       347\n",
            "   macro avg       0.82      0.82      0.82       347\n",
            "weighted avg       0.82      0.82      0.82       347\n",
            "\n",
            "count vectorizer score:  0.8184438040345822\n",
            "instagram data results: \n",
            "Series([], dtype: int64)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.85      0.84        84\n",
            "           1       0.84      0.74      0.79       110\n",
            "           2       0.80      0.89      0.84        79\n",
            "           3       0.76      0.80      0.78        74\n",
            "\n",
            "    accuracy                           0.81       347\n",
            "   macro avg       0.81      0.82      0.81       347\n",
            "weighted avg       0.81      0.81      0.81       347\n",
            "\n",
            "tfidf vectorizer score:  0.7723342939481268\n",
            "instagram data results: \n",
            "Series([], dtype: int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APlzih5VsVsN"
      },
      "source": [
        "OPEN AI GPT-3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBnmxeDGsVSX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d4ba824-d75c-4b05-93f9-c4af116e2427"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "model_cv = RandomForestClassifier()\n",
        "model_cv.fit(X_train_cv, y_train)\n",
        "cv_nb_score = model_cv.score(X_dev_cv, y_dev)\n",
        "Y_pred = model_cv.predict(X_dev_cv)\n",
        "print(classification_report(y_dev,Y_pred))\n",
        "print(\"count vectorizer score: \", cv_nb_score)\n",
        "print(\"instagram data results: \")\n",
        "naiveBayesInstaPredictions = []\n",
        "for row in X_insta:\n",
        "  naiveBayesInstaPredictions.append(model_cv.predict(cv.transform([row]))[0])\n",
        "print(pd.DataFrame(naiveBayesInstaPredictions).value_counts())\n",
        "\n",
        "\n",
        "model_tfidf = RandomForestClassifier()\n",
        "model_tfidf.fit(X_train_tfidf, y_train)\n",
        "tfidf_nb_score = model_tfidf.score(X_dev_tfidf, y_dev)\n",
        "Y_pred = model_tfidf.predict(X_dev_cv)\n",
        "print(classification_report(y_dev,Y_pred))\n",
        "print(\"tfidf vectorizer score: \", tfidf_nb_score)\n",
        "print(\"instagram data results: \")\n",
        "naiveBayesInstaPredictions = []\n",
        "for row in X_insta:\n",
        "  naiveBayesInstaPredictions.append(model_tfidf.predict(tfidf.transform([row]))[0])\n",
        "print(pd.DataFrame(naiveBayesInstaPredictions).value_counts())"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.81      0.87        84\n",
            "           1       0.80      0.93      0.86       110\n",
            "           2       0.87      0.86      0.87        79\n",
            "           3       0.87      0.80      0.83        74\n",
            "\n",
            "    accuracy                           0.86       347\n",
            "   macro avg       0.87      0.85      0.86       347\n",
            "weighted avg       0.86      0.86      0.86       347\n",
            "\n",
            "count vectorizer score:  0.8559077809798271\n",
            "instagram data results: \n",
            "Series([], dtype: int64)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.80      0.81        84\n",
            "           1       0.82      0.85      0.83       110\n",
            "           2       0.86      0.85      0.85        79\n",
            "           3       0.80      0.81      0.81        74\n",
            "\n",
            "    accuracy                           0.83       347\n",
            "   macro avg       0.83      0.83      0.83       347\n",
            "weighted avg       0.83      0.83      0.83       347\n",
            "\n",
            "tfidf vectorizer score:  0.8386167146974063\n",
            "instagram data results: \n",
            "Series([], dtype: int64)\n"
          ]
        }
      ]
    }
  ]
}